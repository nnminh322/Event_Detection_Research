{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "args_perm_id = 0\n",
    "args_task_num = 5\n",
    "args_class_num = 10\n",
    "args_shot_num = 5\n",
    "def collect_from_json(dataset, root, split):\n",
    "    if split == \"train\":\n",
    "        pth = os.path.join(\n",
    "            root,\n",
    "            dataset,\n",
    "            \"perm\" + str(args_perm_id),\n",
    "            f\"{dataset}_{args_task_num}task_{args_class_num // args_task_num}way_{args_shot_num}shot.{split}.jsonl\",\n",
    "        )\n",
    "    elif split in [\"dev\", \"test\"]:\n",
    "        pth = os.path.join(root, dataset, f\"{dataset}.{split}.jsonl\")\n",
    "    elif split == \"stream\":\n",
    "        pth = os.path.join(\n",
    "            root,\n",
    "            dataset,\n",
    "            f\"stream_label_{args_task_num}task_{args_class_num // args_task_num}way.json\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Split \"{split}\" value wrong!')\n",
    "    if not os.path.exists(pth):\n",
    "        raise FileNotFoundError(f\"Path {pth} do not exist!\")\n",
    "    else:\n",
    "        with open(pth) as f:\n",
    "            if pth.endswith(\".jsonl\"):\n",
    "                data = [json.loads(line) for line in f]\n",
    "                if split == \"train\":\n",
    "                    data = [list(i.values()) for i in data]\n",
    "            else:\n",
    "                data = json.load(f)\n",
    "    # if split == \"train\":\n",
    "    #     data = extract_single_dict(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ACE'\n",
    "root = './data_incremental'\n",
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [instance for t in collect_from_json(dataset, root, split)[1] for instance in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    trigger_word = []\n",
    "    seq_len = len(data_instance[\"piece_ids\"]) + 1 # because start_index of piece_ids is 1 instead of 0\n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            trigger_word.append(data_instance[\"span\"][i])\n",
    "\n",
    "    set_label_in_one_sentence = set(true_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(num_label)\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(num_label)[i]\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(seq_len)\n",
    "    trigger = []\n",
    "    for i in trigger_word:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig = set(trigger)\n",
    "    for i in set_trig:\n",
    "        true_one_hot_label_vector += torch.eye(seq_len)[i]\n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_one_hot_true_label_and_true_trigger(data[0],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "47\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(data[0]['piece_ids']))\n",
    "print(len(data[0]['label']))\n",
    "print(len(data[0]['span']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in data:\n",
    "    true_one_hot_trigger_vector, true_one_hot_label_vector = get_one_hot_true_label_and_true_trigger(instance, 10)\n",
    "    instance['true_one_hot_trigger_vector'] = true_one_hot_trigger_vector.tolist()\n",
    "    instance['true_one_hot_label_vector'] = true_one_hot_label_vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'piece_ids': [101,\n",
       "  3960,\n",
       "  1010,\n",
       "  1045,\n",
       "  2228,\n",
       "  2008,\n",
       "  1996,\n",
       "  3114,\n",
       "  7955,\n",
       "  1999,\n",
       "  1996,\n",
       "  2148,\n",
       "  1011,\n",
       "  1011,\n",
       "  2017,\n",
       "  2113,\n",
       "  1010,\n",
       "  2034,\n",
       "  1997,\n",
       "  2035,\n",
       "  1010,\n",
       "  2057,\n",
       "  2020,\n",
       "  1011,\n",
       "  1011,\n",
       "  2043,\n",
       "  5951,\n",
       "  8573,\n",
       "  2001,\n",
       "  2700,\n",
       "  2343,\n",
       "  1010,\n",
       "  2057,\n",
       "  2018,\n",
       "  2042,\n",
       "  2542,\n",
       "  2054,\n",
       "  2057,\n",
       "  2245,\n",
       "  2001,\n",
       "  2145,\n",
       "  1037,\n",
       "  11438,\n",
       "  3842,\n",
       "  2044,\n",
       "  1996,\n",
       "  2942,\n",
       "  2162,\n",
       "  1012,\n",
       "  102],\n",
       " 'label': [6,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'span': [[29, 29],\n",
       "  [46, 47],\n",
       "  [1, 1],\n",
       "  [2, 2],\n",
       "  [3, 3],\n",
       "  [4, 4],\n",
       "  [5, 5],\n",
       "  [6, 6],\n",
       "  [7, 7],\n",
       "  [8, 8],\n",
       "  [9, 9],\n",
       "  [10, 10],\n",
       "  [11, 11],\n",
       "  [12, 12],\n",
       "  [13, 13],\n",
       "  [14, 14],\n",
       "  [15, 15],\n",
       "  [16, 16],\n",
       "  [17, 17],\n",
       "  [18, 18],\n",
       "  [19, 19],\n",
       "  [20, 20],\n",
       "  [21, 21],\n",
       "  [22, 22],\n",
       "  [23, 23],\n",
       "  [24, 24],\n",
       "  [25, 25],\n",
       "  [26, 26],\n",
       "  [27, 27],\n",
       "  [28, 28],\n",
       "  [30, 30],\n",
       "  [31, 31],\n",
       "  [32, 32],\n",
       "  [33, 33],\n",
       "  [34, 34],\n",
       "  [35, 35],\n",
       "  [36, 36],\n",
       "  [37, 37],\n",
       "  [38, 38],\n",
       "  [39, 39],\n",
       "  [40, 40],\n",
       "  [41, 41],\n",
       "  [42, 42],\n",
       "  [43, 43],\n",
       "  [44, 44],\n",
       "  [45, 45],\n",
       "  [48, 48]],\n",
       " 'true_one_hot_trigger_vector': [0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'true_one_hot_label_vector': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    true_trigger = []\n",
    "    seq_len = len(data_instance[\"piece_ids\"]) # because start_index of piece_ids is 1 instead of 0\n",
    "    \n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            true_trigger.append(data_instance[\"span\"][i])\n",
    "\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(num_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(seq_len)\n",
    "\n",
    "    set_label_in_one_sentence = set([label.item() for label in true_label])\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_label_vector += torch.eye(num_label)[i]\n",
    "\n",
    "\n",
    "    list_trigger = [trigger.tolist() for trigger in true_trigger]\n",
    "    trigger = []\n",
    "    for i in list_trigger:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig_in_one_sentence = set(trigger)\n",
    "\n",
    "    for i in set_trig_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(seq_len)[i]\n",
    "    \n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    true_trigger = []\n",
    "    seq_len = len(data_instance[\"piece_ids\"]) # because start_index of piece_ids is 1 instead of 0\n",
    "    matrix_word_is_label = torch.zeros(seq_len, num_label,dtype=int)\n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            true_trigger.append(data_instance[\"span\"][i])\n",
    "            for word_is_trigger in data_instance['span'][i]:\n",
    "                matrix_word_is_label[word_is_trigger,data_instance['label'][i]] = 1\n",
    "\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(num_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(seq_len)\n",
    "\n",
    "    set_label_in_one_sentence = set([label.item() for label in true_label])\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_label_vector += torch.eye(num_label)[i]\n",
    "\n",
    "\n",
    "    list_trigger = [trigger.tolist() for trigger in true_trigger]\n",
    "    trigger = []\n",
    "    for i in list_trigger:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig_in_one_sentence = set(trigger)\n",
    "\n",
    "    for i in set_trig_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(seq_len)[i]\n",
    "    true_one_hot_trigger_vector = true_one_hot_trigger_vector.to(device)\n",
    "    true_one_hot_label_vector = true_one_hot_label_vector.to(device)\n",
    "    matrix_word_is_label = matrix_word_is_label.to(device)\n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label\n",
    "\n",
    "def true_label_and_trigger(train_x,train_y,train_masks, train_span, class_num):\n",
    "    num_instance = len(train_x)\n",
    "    true_one_hot_label_vectors = []\n",
    "    true_one_hot_trigger_vectors = []\n",
    "    golden_matrix = []\n",
    "    for i in range(num_instance):\n",
    "        data_instace={\n",
    "            'piece_ids': train_x[i],\n",
    "            'label': train_y[i],\n",
    "            'span': train_span[i],\n",
    "            'mask': train_masks[i]\n",
    "        }\n",
    "\n",
    "        true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label= get_one_hot_true_label_and_true_trigger(data_instance=data_instace,num_label=class_num)\n",
    "        true_one_hot_trigger_vectors.append(true_one_hot_trigger_vector)\n",
    "        true_one_hot_label_vectors.append(true_one_hot_label_vector)\n",
    "        golden_matrix.append(matrix_word_is_label)\n",
    "    true_one_hot_trigger_vectors = torch.stack([x.to(device) for x in true_one_hot_trigger_vectors])\n",
    "    true_one_hot_label_vectors = torch.stack([x.to(device) for x in true_one_hot_label_vectors])\n",
    "    pi_golden_matrix = torch.stack([x.to(device) for x in golden_matrix])\n",
    "    return true_one_hot_trigger_vectors, true_one_hot_label_vectors, pi_golden_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_x = torch.tensor([[  101,  6398,  1024,  6175,  2003,  2025,  1996,  2069,  2510,  2564,\n",
    "          2040,  2363,  1037,  6302,  1998,  3661,  1012,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101, 21524,  1998,  2037,  2079, 24968,  5611,  6956, 19974,  2224,\n",
    "          1996,  2773,  1036,  1036,  6139,  1005,  1005,  2043,  9694,  2008,\n",
    "          3956,  2681,  1996,  2225,  2924,  1998, 14474,  1998,  4487, 11512,\n",
    "          9286,  3644,  7617,  1012,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  1045,  2079,  1050,  1005,  1056,  2228,  2008,  1005,  1055,\n",
    "          3243,  8321,  2004,  2000,  2054,  2002,  2626,  2033,  2055,  1012,\n",
    "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2358, 23111,  6582,  1010,  2040,  2001,  2809,  2706,  6875,\n",
    "          1010,  2018,  3041,  2042,  3331,  2007,  2014,  2388,  2006,  1996,\n",
    "          3042,  1010,  1998,  5112,  2039,  3038,  1037,  2450,  2016,  2018,\n",
    "         11834,  3064,  2007,  3784,  2018,  2074,  3369,  2012,  2014,  2341,\n",
    "          1010,  4614,  2056,  1012,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]])\n",
    "\n",
    "train_y = [torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), torch.tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0]), torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), torch.tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
    "train_masks = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0]])\n",
    "train_span = [torch.tensor([[15, 15],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [16, 16]]), torch.tensor([[21, 21],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [15, 15],\n",
    "        [16, 16],\n",
    "        [17, 17],\n",
    "        [18, 18],\n",
    "        [19, 19],\n",
    "        [20, 20],\n",
    "        [22, 22],\n",
    "        [23, 23],\n",
    "        [24, 24],\n",
    "        [25, 25],\n",
    "        [26, 26],\n",
    "        [27, 27],\n",
    "        [28, 28],\n",
    "        [29, 29],\n",
    "        [30, 30],\n",
    "        [31, 31],\n",
    "        [32, 32],\n",
    "        [33, 33]]), torch.tensor([[16, 16],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [15, 15],\n",
    "        [17, 17],\n",
    "        [18, 18],\n",
    "        [19, 19]]), torch.tensor([[20, 20],\n",
    "        [30, 31],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [15, 15],\n",
    "        [16, 16],\n",
    "        [17, 17],\n",
    "        [18, 18],\n",
    "        [19, 19],\n",
    "        [21, 21],\n",
    "        [22, 22],\n",
    "        [23, 23],\n",
    "        [24, 24],\n",
    "        [25, 25],\n",
    "        [26, 26],\n",
    "        [27, 27],\n",
    "        [28, 28],\n",
    "        [29, 29],\n",
    "        [32, 32],\n",
    "        [33, 33],\n",
    "        [34, 34],\n",
    "        [35, 35],\n",
    "        [36, 36],\n",
    "        [37, 37],\n",
    "        [38, 38],\n",
    "        [39, 39],\n",
    "        [40, 40],\n",
    "        [41, 41],\n",
    "        [42, 42],\n",
    "        [43, 43]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trigger, true_label, pi_golden = true_label_and_trigger(train_x=train_x,train_y=train_y,train_masks=train_masks,train_span=train_span,class_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_golden[3][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi_star weighted by golden matrix (sum along axis 1):\n",
      "[0.7 0.5 0.3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Giả sử bạn có các ma trận sau:\n",
    "# pi_star: Ma trận alignment (sau khi tính toán thông qua OT)\n",
    "pi_star = np.array([[0.3, 0.7, 0.1],\n",
    "                    [0.5, 0.4, 0.1],\n",
    "                    [0.2, 0.6, 0.3]])\n",
    "\n",
    "# pi_g: Ma trận golden (true trigger labels)\n",
    "pi_g = np.array([[0, 1, 0],  # w1 có nhãn đúng là t2\n",
    "                 [1, 0, 0],  # w2 có nhãn đúng là t1\n",
    "                 [0, 0, 1]]) # w3 có nhãn đúng là t3\n",
    "\n",
    "# Nhân ma trận pi_star với pi_g (theo từng phần tử)\n",
    "pi_star_weighted = pi_star * pi_g\n",
    "\n",
    "# Tính tổng theo chiều ngang (axis=1)\n",
    "pi_star_sum = pi_star_weighted.sum(axis=1)\n",
    "\n",
    "print(\"Pi_star weighted by golden matrix (sum along axis 1):\")\n",
    "print(pi_star_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_star_1 = torch.tensor([\n",
    "    [[0.3, 0.7, 0.1,-1.0], [0.5, 0.4, 0.1,-1.0], [0.2, 0.6, 0.3,-1.0]],\n",
    "    [[0.5, 0.4, 0.1,-2.0], [0.3, 0.7, 0.1,-2.0], [0.2, 0.6, 0.3,-2.0]]]\n",
    ")\n",
    "pi_star_2 = torch.tensor([\n",
    "    [[0.3, 0.7, 0.1,-1.0], [0.5, 0.4, 0.1,-1.0], [0.2, 0.6, 0.3,-1.0]],\n",
    "    [[0.5, 0.4, 0.1,-2.0], [0.3, 0.7, 0.1,-2.0], [0.2, 0.6, 0.3,-2.0]]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6357)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def L_task(pi_star, y_true):\n",
    "    \"\"\"\n",
    "    Tính Loss Task (Negative Log-Likelihood Loss) cho mỗi batch dữ liệu.\n",
    "\n",
    "    Arguments:\n",
    "    - pi_star (Tensor): Tensor có kích thước (batch_size, seq_len), chứa xác suất dự đoán cho từng từ và nhãn.\n",
    "    - y_true (Tensor): Tensor có kích thước (batch_size, seq_len), chứa nhãn thực tế (labels) cho từng từ.\n",
    "\n",
    "    Return:\n",
    "    - loss (Tensor): giá trị loss trung bình cho cả batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Chỉ tính log của pi_star mà không có phần (1 - pi_star)\n",
    "    loss = -torch.log((torch.sum(pi_star*y_true,dim=-1))).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Ví dụ sử dụng:\n",
    "batch_size = 2  # Số câu trong batch\n",
    "seq_len = 4  # Số từ trong mỗi câu\n",
    "\n",
    "# Giả sử chúng ta có xác suất pi_star và nhãn thực tế y_true cho mỗi câu trong batch\n",
    "pi_star = torch.tensor(\n",
    "    [\n",
    "        [[0.3, 0.7, 0.1, -1.0], [0.5, 0.4, 0.1, -1.0], [0.2, 0.6, 0.3, -1.0]],\n",
    "        [[0.5, 0.4, 0.1, -2.0], [0.3, 0.7, 0.1, -2.0], [0.2, 0.6, 0.3, -2.0]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_true = torch.tensor(\n",
    "    [\n",
    "        [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0]],\n",
    "        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tính L_task\n",
    "loss_task = L_task(pi_star, y_true)\n",
    "print(loss_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6357)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = torch.sum(pi_star * y_true, dim=-1)\n",
    "log_probs = torch.log(log_probs + 1e-10)\n",
    "loss = -log_probs.mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6357)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log((torch.sum(pi_star*y_true,dim=-1))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.3567, -0.0000,     nan],\n",
       "         [-0.6931, -0.0000, -0.0000,     nan],\n",
       "         [-0.0000, -0.0000, -1.2040,     nan]],\n",
       "\n",
       "        [[-0.6931, -0.0000, -0.0000,     nan],\n",
       "         [-0.0000, -0.3567, -0.0000,     nan],\n",
       "         [-0.0000, -0.5108, -0.0000,     nan]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(pi_star)*y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul = -torch.log(pi_star_1*pi_star_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4079,  0.7133,  4.6052, -0.0000],\n",
       "         [ 1.3863,  1.8326,  4.6052, -0.0000],\n",
       "         [ 3.2189,  1.0217,  2.4079, -0.0000]],\n",
       "\n",
       "        [[ 1.3863,  1.8326,  4.6052, -1.3863],\n",
       "         [ 2.4079,  0.7133,  4.6052, -1.3863],\n",
       "         [ 3.2189,  1.0217,  2.4079, -1.3863]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8499, 1.5033])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul.sum(dim=[1,2])/(mul.size(1)*mul.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star.sum(dim=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instance = {\n",
    "    \"piece_ids\": train_x[3],\n",
    "    \"label\": train_y[3],\n",
    "    \"span\": train_span[3],\n",
    "    \"mask\": train_masks[3],\n",
    "}\n",
    "# a, b = get_one_hot_true_label_and_true_trigger(data_instance=data_instance,num_label=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_word_is_label = torch.zeros(len(data_instance['piece_ids']),10)\n",
    "for i in range(len(data_instance[\"label\"])):\n",
    "    if data_instance[\"label\"][i] != 0:\n",
    "        # print(data_instance[\"label\"][i])\n",
    "\n",
    "    #     true_label.append(data_instance[\"label\"][i])\n",
    "    #     true_trigger.append(data_instance[\"span\"][i])\n",
    "        for word_is_trigger in data_instance['span'][i]:\n",
    "            # print(word_is_trigger)\n",
    "            matrix_word_is_label[word_is_trigger,data_instance['label'][i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_word_is_label[20:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_span)):\n",
    "    print(train_span[i].size(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = [torch.tensor([[1,2,3],[4,5,6]]),torch.tensor([[7,8,9],[10,11,12]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Giả sử bạn có các tensor sau:\n",
    "# `last_hidden_state` có kích thước [batch_size, seqlen, hidden_dim]\n",
    "last_hidden_state = torch.randn(4, 122, 768)  # Kích thước giả định\n",
    "\n",
    "# `mask` có kích thước [batch_size, seqlen], giá trị 1 cho token thực sự và 0 cho padding\n",
    "mask = torch.randint(0, 2, (4, 122))  # Ví dụ: 1 cho token thực sự, 0 cho padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_hidden_state_without_padding(last_hidden_state, masks):\n",
    "    masks = masks.unsqueeze(-1)\n",
    "    mask_hidden_state = last_hidden_state * masks\n",
    "    true_hidden_state_without_padding = mask_hidden_state.view(-1, 768)[\n",
    "        masks.view(-1) == 1\n",
    "    ]\n",
    "    return true_hidden_state_without_padding  # [sum_true_token, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_embedding = get_true_hidden_state_without_padding(last_hidden_state,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_hidden_state = last_hidden_state * mask.unsqueeze(-1)\n",
    "masked_hidden_state.size()\n",
    "masked_hidden_state = masked_hidden_state.view(-1, 768)[mask.view(-1) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249, 768])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249, 768])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giả sử bạn có các tensor như sau:\n",
    "batch_size = 4\n",
    "seq_len = 6\n",
    "\n",
    "# Các xác suất trigger cho từng token trong câu (p_wi)\n",
    "p_wi = torch.sigmoid(torch.randn(batch_size, seq_len))  # [4, 6]\n",
    "\n",
    "# Các nhãn thực tế (true_trig)\n",
    "true_trig = torch.randint(0, 2, (batch_size, seq_len))  # [4, 6] với giá trị 0 hoặc 1\n",
    "\n",
    "# Attention mask (masks), 1 cho token thực, 0 cho token padding\n",
    "masks = torch.tensor([[1, 1, 1, 1, 0, 0],  # Câu 1 có 4 token thực\n",
    "                      [1, 1, 1, 0, 0, 0],  # Câu 2 có 3 token thực\n",
    "                      [1, 1, 1, 1, 1, 0],  # Câu 3 có 5 token thực\n",
    "                      [1, 1, 1, 1, 1, 1]]) # Câu 4 có 6 token thực (không có padding)\n",
    "\n",
    "# Tính loss TI\n",
    "# loss = compute_loss_TI(p_wi, true_trig, masks)\n",
    "# print(\"Loss TI:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8904, 0.4555, 0.2070])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wi[1][masks[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 5\n",
    "num_classes = 3\n",
    "\n",
    "# Tạo dữ liệu giả cho pi_star và pi_golden (giả sử xác suất phân bố đều cho pi_star, và pi_golden là các vector nhãn)\n",
    "pi_star = torch.rand(batch_size, seq_len, num_classes)  # Xác suất dự đoán ngẫu nhiên từ [0, 1]\n",
    "pi_star = pi_star / pi_star.sum(dim=-1, keepdim=True)  # Chuẩn hóa về tổng = 1 (xác suất)\n",
    "\n",
    "pi_golden = torch.randint(0, 2, (batch_size, seq_len, num_classes)).float()  # Nhãn thực (0 hoặc 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1151, 0.4943, 1.0000, 0.3761, 0.3852],\n",
       "        [0.4031, 0.7366, 1.0000, 0.8397, 0.1146],\n",
       "        [1.0000, 0.6542, 0.8769, 1.0000, 0.4036],\n",
       "        [0.8367, 0.6056, 0.7390, 0.0000, 0.3532]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pi_golden*pi_star,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.1151, 0.0000],\n",
       "         [0.1963, 0.2979, 0.0000],\n",
       "         [0.2573, 0.4445, 0.2983],\n",
       "         [0.0000, 0.3761, 0.0000],\n",
       "         [0.0000, 0.3852, 0.0000]],\n",
       "\n",
       "        [[0.4031, 0.0000, 0.0000],\n",
       "         [0.0000, 0.3005, 0.4361],\n",
       "         [0.4377, 0.2548, 0.3074],\n",
       "         [0.3200, 0.0000, 0.5196],\n",
       "         [0.0246, 0.0000, 0.0900]],\n",
       "\n",
       "        [[0.2333, 0.3558, 0.4110],\n",
       "         [0.1819, 0.0000, 0.4723],\n",
       "         [0.6229, 0.0000, 0.2540],\n",
       "         [0.2883, 0.2216, 0.4901],\n",
       "         [0.0000, 0.0000, 0.4036]],\n",
       "\n",
       "        [[0.0000, 0.3779, 0.4588],\n",
       "         [0.0000, 0.6056, 0.0000],\n",
       "         [0.2351, 0.5038, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.3532, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_golden*pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_pytorch(M, a, b, lambda_sh, numItermax=1000, stopThr=5e-3):\n",
    "    u = torch.ones_like(a) / a.size(0)\n",
    "    v = torch.zeros_like(b)\n",
    "    K = torch.exp(-M * lambda_sh)\n",
    "\n",
    "    cpt = 0\n",
    "    err = 1.0\n",
    "\n",
    "    def condition(cpt, u, v, err):\n",
    "        return cpt < numItermax and err > stopThr\n",
    "\n",
    "    def v_update(u, v):\n",
    "        v = b / torch.matmul(K.t(), u)\n",
    "        u = a / torch.matmul(K, v)\n",
    "        return u, v\n",
    "\n",
    "    def no_v_update(u, v):\n",
    "        return u, v\n",
    "\n",
    "    def err_f1(K, u, v, b):\n",
    "        bb = v * torch.matmul(K.t(), u)\n",
    "        err = torch.norm(torch.sum(torch.abs(bb - b), dim=0), p=float('inf'))\n",
    "        return err\n",
    "\n",
    "    def err_f2(err):\n",
    "        return err\n",
    "\n",
    "    def loop_func(cpt, u, v, err):\n",
    "        u = a / torch.matmul(K, b / torch.matmul(u.T, K).T)\n",
    "        cpt = cpt + 1\n",
    "        if cpt % 20 == 1 or cpt == numItermax:\n",
    "            u, v = v_update(u, v)\n",
    "            err = err_f1(K, u, v, b)\n",
    "        else:\n",
    "            u, v = no_v_update(u, v)\n",
    "            err = err_f2(err)\n",
    "        return cpt, u, v, err\n",
    "\n",
    "    while condition(cpt, u, v, err):\n",
    "        cpt, u, v, err = loop_func(cpt, u, v, err)\n",
    "\n",
    "    sinkhorn_divergences = torch.sum(u * torch.matmul(K * M, v), dim=0)\n",
    "    return sinkhorn_divergences\n",
    "\n",
    "def compute_pi_star(M, a, b, lambda_sh, numItermax=1000, stopThr=5e-3):\n",
    "    # Gọi hàm Sinkhorn để tính u, v và ma trận K\n",
    "    u, v, K = sinkhorn_pytorch(M, a, b, lambda_sh, numItermax, stopThr)\n",
    "    \n",
    "    # Tính ma trận đồng nhất tối ưu pi_star bằng công thức pi_star = diag(u) * K * diag(v)\n",
    "    pi_star = torch.diag(u) @ K @ torch.diag(v)\n",
    "    \n",
    "    return pi_star\n",
    "\n",
    "# batch_size = 4\n",
    "# num_class = 11\n",
    "# num_token = 122\n",
    "\n",
    "# word_preference = torch.randn(batch_size,num_token)\n",
    "# type_preference = torch.randn(batch_size,num_class)\n",
    "# cost_matrix = torch.randn(batch_size,num_class,num_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class, num_token = 11, 122\n",
    "\n",
    "# Tạo ma trận chi phí ngẫu nhiên kích thước (n, m) = (11, 122)\n",
    "M = torch.rand(num_class, num_token)\n",
    "\n",
    "# Tạo các vector a và b là phân phối xác suất (tổng = 1)\n",
    "a = torch.tensor([1.0 / num_class] * num_class)  # Phân phối xác suất cho a (có tổng = 1)\n",
    "b = torch.tensor([1.0 / num_token] * num_token)  # Phân phối xác suất cho b (có tổng = 1)\n",
    "\n",
    "# Tham số điều chỉnh regularization\n",
    "lambda_sh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 122, 11, 768])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([4,122,768])\n",
    "y = torch.randn([11,768])\n",
    "(x.unsqueeze(2)-y.unsqueeze(0).unsqueeze(0)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.unsqueeze(2).repeat(1,1,11,1)\n",
    "y1=y.unsqueeze(0).unsqueeze(0).repeat(4,122,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 122, 11])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-torch.nn.functional.cosine_similarity(x1=x1,x2=y1,dim=3)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.randint(1,20,(4,11,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 1, 12, 7, 5, 17, 16, 11, 14, 13, 15, 6]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict.fromkeys(test[0].flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 1, 12, 7, 1, 5, 17, 16, 11, 14, 1, 13, 5, 11, 5, 15, 2, 17, 6, 11, 1]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 8, 8, 1, 1, 12, 13, 7, 5, 17, 16, 11, 2, 14, 13, 15, 6]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2, 2, 8, 8, 1, 1, 12, 13, 7, 5, 17, 16, 11,2, 14, 13, 15, 6]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 1, 12, 13, 7, 5, 17, 16, 11, 14, 15, 6]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict.fromkeys(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([4,122,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 768])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][list(dict.fromkeys(a)),:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(768,1)\n",
    "input = torch.rand([17,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.sigmoid(linear(input))\n",
    "output1 = (linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3153, -0.2226, -0.1029, -0.2959, -0.4649, -0.2998, -0.5438, -0.1999,\n",
       "        -0.1981, -0.6199, -0.4724, -0.2184, -0.0811, -0.1304, -0.4779, -0.1843,\n",
       "        -0.3864], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0576, 0.0632, 0.0713, 0.0588, 0.0496, 0.0585, 0.0458, 0.0647, 0.0648,\n",
       "        0.0425, 0.0492, 0.0635, 0.0728, 0.0693, 0.0490, 0.0657, 0.0537],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output.squeeze(1),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_y(y):\n",
    "    true_y = []\n",
    "    for i in range(len(y)):\n",
    "        filter_y = (y[i] != 0).int()\n",
    "        true_y.append(filter_y)\n",
    "    return true_y\n",
    "\n",
    "test = torch.randint(0,10,[4,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 9, 6, 3, 6, 7, 5, 4, 6, 6],\n",
       "        [6, 4, 7, 1, 3, 0, 1, 7, 9, 3],\n",
       "        [0, 0, 2, 1, 9, 3, 3, 9, 4, 7],\n",
       "        [9, 1, 3, 8, 7, 9, 7, 3, 6, 7]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_true_y(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6034)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_loss_TI(p_wi, true_y):\n",
    "    loss_TI = 0.0\n",
    "    for i in range(len(true_y)):\n",
    "        # mask padding token\n",
    "        loss_TI += -torch.dot(true_y[i].float(), torch.log(p_wi[i])) - torch.dot(\n",
    "            (1 - true_y[i].float()), torch.log(1 - p_wi[i])\n",
    "        )\n",
    "\n",
    "    return loss_TI / len(true_y)\n",
    "\n",
    "\n",
    "p_wi = [torch.tensor([0.9,0.05,0.1]), torch.tensor([0.6,0.1,0.1,0.2])]\n",
    "true_y = [torch.tensor([1,0,0]),torch.tensor([1,0,0,0])]\n",
    "compute_loss_TI(p_wi,true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_y(y,num_class):\n",
    "    true_trig,true_label = [],[]\n",
    "    for i in range(len(y)):\n",
    "        true_label_loop = torch.zeros(num_class)\n",
    "        set_label = set(y[i].tolist())\n",
    "        for label in set_label:\n",
    "            if label != 0:\n",
    "                true_label_loop += torch.nn.functional.one_hot(torch.tensor(label),num_classes=num_class)\n",
    "        \n",
    "        filter_y = (y[i] != 0).int()\n",
    "        true_trig.append(filter_y)\n",
    "        true_label.append(true_label_loop)\n",
    "    return true_trig,true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [torch.tensor([2,3,5,0,0,0]),torch.tensor([7,8,0]),torch.tensor([1,1,2,0])]\n",
    "num_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trig, true_label =get_true_y(y,num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 1, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([1, 1, 0], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 0], dtype=torch.int32)]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_trig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 1., 1., 0., 1., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    true_trigger = []\n",
    "    seq_len = len(\n",
    "        data_instance[\"piece_ids\"]\n",
    "    )  # because start_index of piece_ids is 1 instead of 0\n",
    "    matrix_word_is_label = torch.zeros(seq_len, num_label, dtype=int)\n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            true_trigger.append(data_instance[\"span\"][i])\n",
    "            for word_is_trigger in data_instance[\"span\"][i]:\n",
    "                matrix_word_is_label[word_is_trigger, data_instance[\"label\"][i]] = 1\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(num_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(seq_len)\n",
    "\n",
    "    set_label_in_one_sentence = set([label.item() for label in true_label])\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_label_vector += torch.eye(num_label)[i]\n",
    "\n",
    "    list_trigger = [trigger.tolist() for trigger in true_trigger]\n",
    "    trigger = []\n",
    "    for i in list_trigger:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig_in_one_sentence = set(trigger)\n",
    "\n",
    "    for i in set_trig_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(seq_len)[i]\n",
    "    true_one_hot_trigger_vector = true_one_hot_trigger_vector.to(device)\n",
    "    true_one_hot_label_vector = true_one_hot_label_vector.to(device)\n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label\n",
    "\n",
    "\n",
    "def true_label_and_trigger(train_x, train_y, train_masks, train_span, class_num):\n",
    "    num_instance = len(train_x)\n",
    "    true_one_hot_label_vectors = []\n",
    "    true_one_hot_trigger_vectors = []\n",
    "    golden_matrix = []\n",
    "    for i in range(num_instance):\n",
    "        data_instace = {\n",
    "            \"piece_ids\": train_x[i],\n",
    "            \"label\": train_y[i],\n",
    "            \"span\": train_span[i],\n",
    "            \"mask\": train_masks[i],\n",
    "        }\n",
    "\n",
    "        true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label = (\n",
    "            get_one_hot_true_label_and_true_trigger(\n",
    "                data_instance=data_instace, num_label=class_num\n",
    "            )\n",
    "        )\n",
    "        true_one_hot_trigger_vectors.append(true_one_hot_trigger_vector)\n",
    "        true_one_hot_label_vectors.append(true_one_hot_label_vector)\n",
    "        golden_matrix.append(matrix_word_is_label)\n",
    "    true_one_hot_trigger_vectors = torch.stack(\n",
    "        [x.to(device) for x in true_one_hot_trigger_vectors]\n",
    "    )\n",
    "    true_one_hot_label_vectors = torch.stack(\n",
    "        [x.to(device) for x in true_one_hot_label_vectors]\n",
    "    )\n",
    "    pi_golden_matrix = torch.stack([x.to(device) for x in golden_matrix])\n",
    "    return true_one_hot_trigger_vectors, true_one_hot_label_vectors, pi_golden_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_order = [torch.rand([11,768]),torch.rand([5,768]),torch.rand([6,768]),torch.rand(20,768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embedding = torch.rand([11,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11, 768])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embedding.unsqueeze(0).repeat([5,1,1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11, 768])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state_order[1].unsqueeze(1).repeat([1,11,1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-torch.nn.functional.cosine_similarity(label_embedding.unsqueeze(0).repeat([5,1,1]),last_hidden_state_order[1].unsqueeze(1).repeat([1,11,1]),dim=-1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_transport(last_hidden_state_order, label_embedding, num_classes = 11):\n",
    "    # last_hidden_state_order: [batch_size, num_span, hidden_dim]\n",
    "    # label_embedding: [num_class, hidden_dim]\n",
    "    \n",
    "    batch_size = len(last_hidden_state_order)\n",
    "    cost_matrix = []\n",
    "    for i in range(batch_size):\n",
    "        num_span = last_hidden_state_order[i].size(0)\n",
    "        label_embedding_scale = label_embedding.unsqueeze(0).repeat([num_span,1,1])\n",
    "        last_hidden_state_order_scale = last_hidden_state_order[i].unsqueeze(1).repeat([1,num_classes,1])\n",
    "        cost = 1-torch.nn.functional.cosine_similarity(last_hidden_state_order_scale,label_embedding_scale,dim=-1)\n",
    "        cost_matrix.append(cost)\n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix = compute_cost_transport(last_hidden_state_order=last_hidden_state_order,label_embedding=label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of cost_matrix[0]: torch.Size([11, 11])\n",
      "size of cost_matrix[1]: torch.Size([5, 11])\n",
      "size of cost_matrix[2]: torch.Size([6, 11])\n",
      "size of cost_matrix[3]: torch.Size([20, 11])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cost_matrix)):\n",
    "    print(f'size of cost_matrix[{i}]: {cost_matrix[i].size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(768,1)\n",
    "D_W_P = []\n",
    "for sentence in range(len(last_hidden_state_order)):\n",
    "    D_W_P.append(torch.softmax(torch.sigmoid(linear(last_hidden_state_order[sentence])).flatten(),dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(D_W_P[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_cls = torch.rand([4,768])\n",
    "e_cls_scale = e_cls.unsqueeze(1).repeat([1,11,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11, 768])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_cls_scale.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embedding_scale = label_embedding.unsqueeze(0).repeat([4,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = torch.cat([e_cls_scale,label_embedding_scale],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffp = torch.nn.Linear(768*2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_T_P = torch.softmax(torch.sigmoid(ffp(concat).squeeze(-1)),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(D_T_P[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "def compute_optimal_transport(p, q, C, epsilon=0.05):\n",
    "    device = p.device\n",
    "\n",
    "    p_i = p.detach().cpu().numpy()\n",
    "    q_i = q.detach().cpu().numpy()\n",
    "    C_i = C.detach().cpu().numpy()\n",
    "\n",
    "    pi_i = ot.sinkhorn(p_i, q_i, C_i, reg=epsilon)\n",
    "\n",
    "    pi_i_tensor = torch.tensor(pi_i, device=device)\n",
    "\n",
    "    return pi_i_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0921, 0.0961, 0.0943, 0.0922, 0.0912, 0.0915, 0.0877, 0.0861, 0.0822,\n",
      "        0.0953, 0.0914], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.0918, 0.0882, 0.0929, 0.0894, 0.0941, 0.0892, 0.0920, 0.0894, 0.0907,\n",
      "        0.0916, 0.0907], grad_fn=<SelectBackward0>)\n",
      "tensor([[0.2582, 0.2447, 0.2369, 0.2448, 0.2562, 0.2636, 0.2619, 0.2515, 0.2402,\n",
      "         0.2487, 0.2451],\n",
      "        [0.2566, 0.2528, 0.2547, 0.2515, 0.2473, 0.2396, 0.2482, 0.2400, 0.2459,\n",
      "         0.2446, 0.2357],\n",
      "        [0.2433, 0.2444, 0.2473, 0.2426, 0.2496, 0.2442, 0.2377, 0.2474, 0.2523,\n",
      "         0.2609, 0.2431],\n",
      "        [0.2412, 0.2486, 0.2462, 0.2530, 0.2683, 0.2550, 0.2419, 0.2410, 0.2575,\n",
      "         0.2339, 0.2480],\n",
      "        [0.2692, 0.2484, 0.2487, 0.2553, 0.2570, 0.2561, 0.2587, 0.2569, 0.2745,\n",
      "         0.2466, 0.2502],\n",
      "        [0.2492, 0.2455, 0.2577, 0.2404, 0.2465, 0.2552, 0.2486, 0.2314, 0.2603,\n",
      "         0.2404, 0.2526],\n",
      "        [0.2706, 0.2537, 0.2363, 0.2622, 0.2615, 0.2597, 0.2703, 0.2486, 0.2498,\n",
      "         0.2531, 0.2637],\n",
      "        [0.2617, 0.2512, 0.2416, 0.2437, 0.2407, 0.2379, 0.2499, 0.2319, 0.2581,\n",
      "         0.2487, 0.2485],\n",
      "        [0.2428, 0.2718, 0.2489, 0.2628, 0.2479, 0.2384, 0.2459, 0.2548, 0.2578,\n",
      "         0.2620, 0.2306],\n",
      "        [0.2512, 0.2411, 0.2631, 0.2454, 0.2564, 0.2249, 0.2464, 0.2427, 0.2435,\n",
      "         0.2655, 0.2357],\n",
      "        [0.2515, 0.2465, 0.2444, 0.2596, 0.2401, 0.2355, 0.2340, 0.2434, 0.2350,\n",
      "         0.2432, 0.2404]])\n"
     ]
    }
   ],
   "source": [
    "print(D_W_P[0])\n",
    "print(D_T_P[0])\n",
    "print(cost_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhatminhnguyen/anaconda3/envs/natmin/lib/python3.11/site-packages/ot/bregman/_sinkhorn.py:531: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\"Sinkhorn did not converge. You might want to \"\n"
     ]
    }
   ],
   "source": [
    "pi_i = compute_optimal_transport(D_W_P[0],D_T_P[0],cost_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 11])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([6, 11])\n",
      "torch.Size([20, 11])\n"
     ]
    }
   ],
   "source": [
    "pi_star = []\n",
    "for sentence in range(len(D_W_P)):\n",
    "    pi_i = compute_optimal_transport(D_W_P[sentence],D_T_P[sentence],cost_matrix[sentence])\n",
    "    print(pi_i.size())\n",
    "    pi_star.append(pi_i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 11])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 0, 2, 1])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0156, 0.0163, 0.0175, 0.0116, 0.0145, 0.0117, 0.0144, 0.0162, 0.0179,\n",
       "         0.0147, 0.0167],\n",
       "        [0.0143, 0.0193, 0.0125, 0.0198, 0.0181, 0.0120, 0.0152, 0.0175, 0.0158,\n",
       "         0.0135, 0.0192],\n",
       "        [0.0169, 0.0132, 0.0133, 0.0175, 0.0171, 0.0154, 0.0146, 0.0175, 0.0179,\n",
       "         0.0150, 0.0152],\n",
       "        [0.0155, 0.0158, 0.0168, 0.0133, 0.0146, 0.0143, 0.0155, 0.0131, 0.0137,\n",
       "         0.0170, 0.0110],\n",
       "        [0.0150, 0.0121, 0.0126, 0.0138, 0.0134, 0.0193, 0.0117, 0.0091, 0.0139,\n",
       "         0.0174, 0.0168],\n",
       "        [0.0145, 0.0116, 0.0201, 0.0134, 0.0163, 0.0165, 0.0206, 0.0161, 0.0114,\n",
       "         0.0140, 0.0118]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y_3 = torch.randint(0,3,[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_y_3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 11])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.8244)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(-torch.log(pi_star[2].gather(1,true_y_3.unsqueeze(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0078, 0.0090, 0.0106, 0.0094, 0.0080, 0.0058, 0.0066, 0.0072, 0.0106,\n",
       "          0.0086, 0.0084],\n",
       "         [0.0080, 0.0076, 0.0074, 0.0081, 0.0095, 0.0093, 0.0086, 0.0090, 0.0094,\n",
       "          0.0092, 0.0100],\n",
       "         [0.0101, 0.0087, 0.0083, 0.0094, 0.0088, 0.0082, 0.0104, 0.0075, 0.0080,\n",
       "          0.0065, 0.0084],\n",
       "         [0.0107, 0.0081, 0.0086, 0.0077, 0.0061, 0.0067, 0.0096, 0.0086, 0.0073,\n",
       "          0.0112, 0.0077],\n",
       "         [0.0071, 0.0095, 0.0095, 0.0086, 0.0089, 0.0076, 0.0080, 0.0073, 0.0060,\n",
       "          0.0101, 0.0085],\n",
       "         [0.0089, 0.0085, 0.0067, 0.0097, 0.0093, 0.0065, 0.0083, 0.0103, 0.0068,\n",
       "          0.0097, 0.0069],\n",
       "         [0.0067, 0.0082, 0.0118, 0.0072, 0.0079, 0.0069, 0.0061, 0.0084, 0.0096,\n",
       "          0.0086, 0.0063],\n",
       "         [0.0064, 0.0070, 0.0085, 0.0084, 0.0096, 0.0085, 0.0074, 0.0094, 0.0065,\n",
       "          0.0075, 0.0069],\n",
       "         [0.0096, 0.0047, 0.0076, 0.0059, 0.0086, 0.0087, 0.0082, 0.0061, 0.0067,\n",
       "          0.0059, 0.0101],\n",
       "         [0.0087, 0.0094, 0.0061, 0.0090, 0.0077, 0.0122, 0.0087, 0.0083, 0.0096,\n",
       "          0.0059, 0.0098],\n",
       "         [0.0077, 0.0075, 0.0079, 0.0060, 0.0096, 0.0088, 0.0100, 0.0073, 0.0102,\n",
       "          0.0083, 0.0079]]),\n",
       " tensor([[0.0161, 0.0220, 0.0215, 0.0163, 0.0225, 0.0205, 0.0198, 0.0251, 0.0166,\n",
       "          0.0209, 0.0187],\n",
       "         [0.0221, 0.0140, 0.0167, 0.0190, 0.0155, 0.0212, 0.0189, 0.0179, 0.0139,\n",
       "          0.0156, 0.0219],\n",
       "         [0.0162, 0.0154, 0.0178, 0.0216, 0.0180, 0.0135, 0.0184, 0.0142, 0.0211,\n",
       "          0.0203, 0.0150],\n",
       "         [0.0181, 0.0163, 0.0126, 0.0171, 0.0218, 0.0195, 0.0155, 0.0163, 0.0199,\n",
       "          0.0173, 0.0142],\n",
       "         [0.0192, 0.0204, 0.0244, 0.0155, 0.0164, 0.0145, 0.0195, 0.0159, 0.0194,\n",
       "          0.0175, 0.0209]]),\n",
       " tensor([[0.0156, 0.0163, 0.0175, 0.0116, 0.0145, 0.0117, 0.0144, 0.0162, 0.0179,\n",
       "          0.0147, 0.0167],\n",
       "         [0.0143, 0.0193, 0.0125, 0.0198, 0.0181, 0.0120, 0.0152, 0.0175, 0.0158,\n",
       "          0.0135, 0.0192],\n",
       "         [0.0169, 0.0132, 0.0133, 0.0175, 0.0171, 0.0154, 0.0146, 0.0175, 0.0179,\n",
       "          0.0150, 0.0152],\n",
       "         [0.0155, 0.0158, 0.0168, 0.0133, 0.0146, 0.0143, 0.0155, 0.0131, 0.0137,\n",
       "          0.0170, 0.0110],\n",
       "         [0.0150, 0.0121, 0.0126, 0.0138, 0.0134, 0.0193, 0.0117, 0.0091, 0.0139,\n",
       "          0.0174, 0.0168],\n",
       "         [0.0145, 0.0116, 0.0201, 0.0134, 0.0163, 0.0165, 0.0206, 0.0161, 0.0114,\n",
       "          0.0140, 0.0118]]),\n",
       " tensor([[0.0032, 0.0037, 0.0040, 0.0030, 0.0054, 0.0055, 0.0049, 0.0036, 0.0056,\n",
       "          0.0054, 0.0045],\n",
       "         [0.0058, 0.0050, 0.0049, 0.0039, 0.0048, 0.0046, 0.0054, 0.0050, 0.0035,\n",
       "          0.0043, 0.0042],\n",
       "         [0.0050, 0.0051, 0.0040, 0.0046, 0.0045, 0.0058, 0.0048, 0.0041, 0.0051,\n",
       "          0.0051, 0.0059],\n",
       "         [0.0045, 0.0044, 0.0042, 0.0045, 0.0045, 0.0054, 0.0037, 0.0043, 0.0036,\n",
       "          0.0040, 0.0054],\n",
       "         [0.0049, 0.0057, 0.0052, 0.0049, 0.0038, 0.0044, 0.0042, 0.0050, 0.0042,\n",
       "          0.0057, 0.0037],\n",
       "         [0.0038, 0.0051, 0.0046, 0.0052, 0.0054, 0.0054, 0.0047, 0.0058, 0.0046,\n",
       "          0.0046, 0.0040],\n",
       "         [0.0037, 0.0043, 0.0064, 0.0042, 0.0055, 0.0029, 0.0065, 0.0057, 0.0038,\n",
       "          0.0032, 0.0030],\n",
       "         [0.0060, 0.0041, 0.0051, 0.0036, 0.0042, 0.0053, 0.0038, 0.0042, 0.0036,\n",
       "          0.0060, 0.0069],\n",
       "         [0.0045, 0.0046, 0.0060, 0.0034, 0.0038, 0.0046, 0.0046, 0.0046, 0.0049,\n",
       "          0.0049, 0.0030],\n",
       "         [0.0036, 0.0045, 0.0032, 0.0047, 0.0064, 0.0042, 0.0048, 0.0058, 0.0033,\n",
       "          0.0044, 0.0053],\n",
       "         [0.0033, 0.0047, 0.0051, 0.0053, 0.0032, 0.0050, 0.0042, 0.0043, 0.0054,\n",
       "          0.0034, 0.0048],\n",
       "         [0.0055, 0.0040, 0.0049, 0.0046, 0.0036, 0.0036, 0.0048, 0.0031, 0.0048,\n",
       "          0.0042, 0.0044],\n",
       "         [0.0065, 0.0043, 0.0055, 0.0043, 0.0051, 0.0035, 0.0034, 0.0038, 0.0051,\n",
       "          0.0037, 0.0040],\n",
       "         [0.0040, 0.0035, 0.0057, 0.0040, 0.0059, 0.0045, 0.0037, 0.0038, 0.0059,\n",
       "          0.0034, 0.0047],\n",
       "         [0.0035, 0.0050, 0.0044, 0.0050, 0.0031, 0.0030, 0.0055, 0.0041, 0.0047,\n",
       "          0.0051, 0.0041],\n",
       "         [0.0050, 0.0031, 0.0038, 0.0049, 0.0046, 0.0048, 0.0052, 0.0043, 0.0048,\n",
       "          0.0047, 0.0042],\n",
       "         [0.0051, 0.0053, 0.0043, 0.0059, 0.0042, 0.0036, 0.0048, 0.0047, 0.0039,\n",
       "          0.0058, 0.0049],\n",
       "         [0.0046, 0.0034, 0.0040, 0.0039, 0.0050, 0.0044, 0.0038, 0.0036, 0.0046,\n",
       "          0.0050, 0.0051],\n",
       "         [0.0047, 0.0039, 0.0039, 0.0057, 0.0049, 0.0037, 0.0044, 0.0058, 0.0049,\n",
       "          0.0051, 0.0041],\n",
       "         [0.0047, 0.0045, 0.0036, 0.0040, 0.0060, 0.0052, 0.0046, 0.0037, 0.0044,\n",
       "          0.0035, 0.0044]])]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9883)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(pi_star)\n",
    "Dist = 0.0\n",
    "for i in range(batch_size):\n",
    "    Dist+=torch.sum(pi_star[i]*cost_matrix[i])\n",
    "Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test = torch.randint(0,4, [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 0, 2, 2, 2, 0, 1, 0, 3])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "for i in true_test:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor(1),num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.nn.functional.one_hot(x, num_classes=11) for x in true_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi_g(y_true, num_classes=11):\n",
    "    batch_size = len(y_true)\n",
    "    pi_g = []\n",
    "    for i in range(batch_size):\n",
    "        pi_g_i = torch.stack(\n",
    "            [torch.nn.functional.one_hot(x, num_classes=num_classes) for x in y_true[i]]\n",
    "        )\n",
    "        pi_g.append(pi_g_i)\n",
    "\n",
    "    return pi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = [torch.randint(0,10,[5]), torch.randint(0,10,[11]), torch.randint(0,10,[6]),torch.randint(0,10,[20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8, 7, 2, 0, 8]),\n",
       " tensor([9, 1, 9, 8, 3, 5, 7, 0, 1, 8, 7]),\n",
       " tensor([7, 9, 5, 8, 5, 6]),\n",
       " tensor([5, 4, 1, 2, 9, 8, 0, 6, 3, 5, 5, 4, 7, 8, 1, 3, 6, 6, 0, 7])]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_g = get_pi_g(y_true=y_true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dist_pi_star = torch.rand([4])\n",
    "Dist_pi_g = torch.rand([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4224, 0.2023, 0.4167, 0.7400])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dist_pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0913, 0.7820, 0.7226, 0.8431])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dist_pi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3311, 0.5797, 0.3059, 0.1032])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(Dist_pi_star-Dist_pi_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3300)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(Dist_pi_star-Dist_pi_g).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_OT(Dist_pi_star, Dist_pi_g):\n",
    "    return torch.abs(Dist_pi_star-Dist_pi_g).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3300)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_OT(Dist_pi_star=Dist_pi_star,Dist_pi_g=Dist_pi_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 2, 0, 8])"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pi_g[0],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred(pi_star):\n",
    "    batch_size = len(pi_star)\n",
    "    y_pred = []\n",
    "    for i in range(batch_size):\n",
    "        y_pred_i = torch.argmax(pi_star[i],dim=-1).to(device)\n",
    "        y_pred.append(y_pred_i)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_y_pred(pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 10,  6,  9,  9,  7,  2,  4, 10,  5,  8,  7,  0,  3,  4,  2,  8,  3,\n",
       "         8,  9,  5,  6,  8,  0, 10, 10,  1,  7,  6, 10,  2,  4,  8,  0,  0,  4,\n",
       "         6,  6,  3, 10,  7,  4])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2, 10,  6,  9,  9,  7,  2,  4, 10,  5,  8]),\n",
       " tensor([7, 0, 3, 4, 2]),\n",
       " tensor([8, 3, 8, 9, 5, 6]),\n",
       " tensor([ 8,  0, 10, 10,  1,  7,  6, 10,  2,  4,  8,  0,  0,  4,  6,  6,  3, 10,\n",
       "          7,  4])]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_pytorch(a, b, M, lambda_sh=20, numItermax=1000, stopThr=5e-3):\n",
    "    \"\"\"\n",
    "    Compute the Sinkhorn optimal transport matrix using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        M (torch.Tensor): Cost matrix of shape (n, m).\n",
    "        a (torch.Tensor): Source distribution of shape (n,).\n",
    "        b (torch.Tensor): Target distribution of shape (m,).\n",
    "        lambda_sh (float): Regularization parameter (1 / epsilon).\n",
    "        numItermax (int): Maximum number of iterations.\n",
    "        stopThr (float): Stopping threshold for convergence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal transport matrix π of shape (n, m).\n",
    "    \"\"\"\n",
    "    K = torch.exp(-M * lambda_sh)  # Kernel matrix\n",
    "    u = torch.ones_like(a)  # Initialize u\n",
    "    v = torch.ones_like(b)  # Initialize v\n",
    "\n",
    "    for _ in range(numItermax):\n",
    "        u_prev = (\n",
    "            u.clone()\n",
    "        )  # Keep track of the previous value of u for convergence check\n",
    "        u = a / (K @ v)\n",
    "        v = b / (K.t() @ u)\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.norm(u - u_prev, p=1) < stopThr:\n",
    "            break\n",
    "\n",
    "    # Compute the optimal transport matrix π\n",
    "    pi = torch.diag(u) @ K @ torch.diag(v)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0156, 0.0163, 0.0175, 0.0116, 0.0145, 0.0117, 0.0144, 0.0162, 0.0179,\n",
       "         0.0147, 0.0167],\n",
       "        [0.0143, 0.0193, 0.0125, 0.0198, 0.0181, 0.0120, 0.0152, 0.0175, 0.0158,\n",
       "         0.0135, 0.0192],\n",
       "        [0.0169, 0.0132, 0.0133, 0.0175, 0.0171, 0.0154, 0.0146, 0.0175, 0.0179,\n",
       "         0.0150, 0.0152],\n",
       "        [0.0155, 0.0158, 0.0168, 0.0133, 0.0146, 0.0143, 0.0155, 0.0131, 0.0137,\n",
       "         0.0170, 0.0110],\n",
       "        [0.0150, 0.0121, 0.0126, 0.0138, 0.0134, 0.0193, 0.0117, 0.0091, 0.0139,\n",
       "         0.0174, 0.0168],\n",
       "        [0.0145, 0.0116, 0.0201, 0.0134, 0.0163, 0.0165, 0.0206, 0.0161, 0.0114,\n",
       "         0.0140, 0.0118]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinkhorn_pytorch(cost_matrix[2],D_W_P[2],D_T_P[2],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# OPTIMAL TRANSPORT NODE\n",
    "# Implementation of differentiable optimal transport using implicit differentiation. Makes use of Sinkhorn normalization\n",
    "# to solve the entropy regularized problem (Cuturi, NeurIPS 2013) in the forward pass. The problem can be written as\n",
    "# Let us write the entropy regularized optimal transport problem in the following form,\n",
    "#\n",
    "#    minimize (over P) <P, M> + 1/gamma KL(P || rc^T)\n",
    "#    subject to        P1 = r and P^T1 = c\n",
    "#\n",
    "# where r and c are m- and n-dimensional positive vectors, respectively, each summing to one. Here m-by-n matrix M is\n",
    "# the input and m-by-n dimensional positive matrix P is the output. The above problem leads to a solution of the form\n",
    "#\n",
    "#   P_{ij} = alpha_i beta_j e^{-gamma M_{ij}}\n",
    "#\n",
    "# where alpha and beta are found by iteratively applying row and column normalizations.\n",
    "#\n",
    "# We also provide an option to parametrize the input in log-space as M_{ij} = -log Q_{ij} where Q is a positive matrix.\n",
    "# The matrix Q becomes the input. This is more numerically stable for inputs M with large positive or negative values.\n",
    "#\n",
    "# See accompanying Jupyter Notebook at https://deepdeclarativenetworks.com.\n",
    "#\n",
    "# Stephen Gould <stephen.gould@anu.edu.au>\n",
    "# Dylan Campbell <dylan.campbell@anu.edu.au>\n",
    "# Fred Zhang <frederic.zhang@anu.edu.au>\n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "\n",
    "\n",
    "def sinkhorn(M, r=None, c=None, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False):\n",
    "    \"\"\"\n",
    "    PyTorch function for entropy regularized optimal transport. Assumes batched inputs as follows:\n",
    "        M:  (B,H,W) tensor\n",
    "        r:  (B,H) tensor, (1,H) tensor or None for constant uniform vector 1/H\n",
    "        c:  (B,W) tensor, (1,W) tensor or None for constant uniform vector 1/W\n",
    "\n",
    "    You can back propagate through this function in O(TBWH) time where T is the number of iterations taken to converge.\n",
    "    \"\"\"\n",
    "\n",
    "    B, H, W = M.shape\n",
    "    assert r is None or r.shape == (B, H) or r.shape == (1, H)\n",
    "    assert c is None or c.shape == (B, W) or c.shape == (1, W)\n",
    "    assert not logspace or torch.all(M > 0.0)\n",
    "\n",
    "    r = 1.0 / H if r is None else r.unsqueeze(dim=2)\n",
    "    c = 1.0 / W if c is None else c.unsqueeze(dim=1)\n",
    "\n",
    "    if logspace:\n",
    "        P = torch.pow(M, gamma)\n",
    "    else:\n",
    "        P = torch.exp(-1.0 * gamma * (M - torch.amin(M, 2, keepdim=True)))\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        alpha = torch.sum(P, 2)\n",
    "        # Perform division first for numerical stability\n",
    "        P = P / alpha.view(B, H, 1) * r\n",
    "\n",
    "        beta = torch.sum(P, 1)\n",
    "        if torch.max(torch.abs(beta - c)) <= eps:\n",
    "            break\n",
    "        P = P / beta.view(B, 1, W) * c\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "def _sinkhorn_inline(M, r=None, c=None, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False):\n",
    "    \"\"\"As above but with inline calculations for when autograd is not needed.\"\"\"\n",
    "\n",
    "    B, H, W = M.shape\n",
    "    assert r is None or r.shape == (B, H) or r.shape == (1, H)\n",
    "    assert c is None or c.shape == (B, W) or c.shape == (1, W)\n",
    "    assert not logspace or torch.all(M > 0.0)\n",
    "\n",
    "    r = 1.0 / H if r is None else r.unsqueeze(dim=2)\n",
    "    c = 1.0 / W if c is None else c.unsqueeze(dim=1)\n",
    "\n",
    "    if logspace:\n",
    "        P = torch.pow(M, gamma)\n",
    "    else:\n",
    "        P = torch.exp(-1.0 * gamma * (M - torch.amin(M, 2, keepdim=True)))\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        alpha = torch.sum(P, 2)\n",
    "        # Perform division first for numerical stability\n",
    "        P /= alpha.view(B, H, 1)\n",
    "        P *= r\n",
    "\n",
    "        beta = torch.sum(P, 1)\n",
    "        if torch.max(torch.abs(beta - c)) <= eps:\n",
    "            break\n",
    "        P /= beta.view(B, 1, W)\n",
    "        P *= c\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "class OptimalTransportFcn(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    PyTorch autograd function for entropy regularized optimal transport. Assumes batched inputs as follows:\n",
    "        M:  (B,H,W) tensor\n",
    "        r:  (B,H) tensor, (1,H) tensor or None for constant uniform vector\n",
    "        c:  (B,W) tensor, (1,W) tensor or None for constant uniform vector\n",
    "\n",
    "    Allows for approximate gradient calculations, which is faster and may be useful during early stages of learning,\n",
    "    when exp(-gamma M) is already nearly doubly stochastic, or when gradients are otherwise noisy.\n",
    "\n",
    "    Both r and c must be positive, if provided. They will be normalized to sum to one.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, M, r=None, c=None, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False, method='block'):\n",
    "        \"\"\"Solve optimal transport using skinhorn. Method can be 'block', 'full', 'fullchol' or 'approx'.\"\"\"\n",
    "        assert method in ('block', 'full', 'fullchol', 'approx')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # normalize r and c to ensure that they sum to one (and save normalization factor for backward pass)\n",
    "            if r is not None:\n",
    "                ctx.inv_r_sum = 1.0 / torch.sum(r, dim=1, keepdim=True)\n",
    "                r = ctx.inv_r_sum * r\n",
    "            if c is not None:\n",
    "                ctx.inv_c_sum = 1.0 / torch.sum(c, dim=1, keepdim=True)\n",
    "                c = ctx.inv_c_sum * c\n",
    "\n",
    "            # run sinkhorn\n",
    "            P = _sinkhorn_inline(M, r, c, gamma, eps, maxiters, logspace)\n",
    "\n",
    "        ctx.save_for_backward(M, r, c, P)\n",
    "        ctx.gamma = gamma\n",
    "        ctx.logspace = logspace\n",
    "        ctx.method = method\n",
    "\n",
    "        return P\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dJdP):\n",
    "        \"\"\"Implement backward pass using implicit differentiation.\"\"\"\n",
    "\n",
    "        M, r, c, P = ctx.saved_tensors\n",
    "        B, H, W = M.shape\n",
    "\n",
    "        # initialize backward gradients (-v^T H^{-1} B with v = dJdP and B = I or B = -1/r or B = -1/c)\n",
    "        dJdM = -1.0 * ctx.gamma * P * dJdP\n",
    "        dJdr = None if not ctx.needs_input_grad[1] else torch.zeros_like(r)\n",
    "        dJdc = None if not ctx.needs_input_grad[2] else torch.zeros_like(c)\n",
    "\n",
    "        # return approximate gradients\n",
    "        if ctx.method == 'approx':\n",
    "            return dJdM, dJdr, dJdc, None, None, None, None, None, None\n",
    "\n",
    "        # compute exact row and column sums (in case of small numerical errors or forward pass not converging)\n",
    "        alpha = torch.sum(P, dim=2)\n",
    "        beta = torch.sum(P, dim=1)\n",
    "\n",
    "        # compute [vHAt1, vHAt2] = v^T H^{-1} A^T as two blocks\n",
    "        vHAt1 = torch.sum(dJdM[:, 1:H, 0:W], dim=2).view(B, H - 1, 1)\n",
    "        vHAt2 = torch.sum(dJdM, dim=1).view(B, W, 1)\n",
    "\n",
    "        # compute [v1, v2] = -v^T H^{-1} A^T (A H^{-1] A^T)^{-1}\n",
    "        if ctx.method == 'block':\n",
    "            # by block inverse of (A H^{-1] A^T)\n",
    "            PdivC = P[:, 1:H, 0:W] / beta.view(B, 1, W)\n",
    "            RminusPPdivC = torch.diag_embed(alpha[:, 1:H]) - torch.bmm(P[:, 1:H, 0:W], PdivC.transpose(1, 2))\n",
    "            try:\n",
    "                block_11 = torch.linalg.cholesky(RminusPPdivC)\n",
    "            except:\n",
    "                # block_11 = torch.ones((B, H-1, H-1), device=M.device, dtype=M.dtype)\n",
    "                block_11 = torch.eye(H - 1, device=M.device, dtype=M.dtype).view(1, H - 1, H - 1).repeat(B, 1, 1)\n",
    "                for b in range(B):\n",
    "                    try:\n",
    "                        block_11[b, :, :] = torch.linalg.cholesky(RminusPPdivC[b, :, :])\n",
    "                    except:\n",
    "                        # keep initialized values (gradient will be close to zero)\n",
    "                        warnings.warn(\"backward pass encountered a singular matrix\")\n",
    "                        pass\n",
    "\n",
    "            block_12 = torch.cholesky_solve(PdivC, block_11)\n",
    "            #block_22 = torch.diag_embed(1.0 / beta) + torch.bmm(block_12.transpose(1, 2), PdivC)\n",
    "            block_22 = torch.bmm(block_12.transpose(1, 2), PdivC)\n",
    "\n",
    "            v1 = torch.cholesky_solve(vHAt1, block_11) - torch.bmm(block_12, vHAt2)\n",
    "            #v2 = torch.bmm(block_22, vHAt2) - torch.bmm(block_12.transpose(1, 2), vHAt1)\n",
    "            v2 = vHAt2 / beta.view(B, W, 1) + torch.bmm(block_22, vHAt2) - torch.bmm(block_12.transpose(1, 2), vHAt1)\n",
    "\n",
    "        else:\n",
    "            # by full inverse of (A H^{-1] A^T)\n",
    "            AinvHAt = torch.empty((B, H + W - 1, H + W - 1), device=M.device, dtype=M.dtype)\n",
    "            AinvHAt[:, 0:H - 1, 0:H - 1] = torch.diag_embed(alpha[:, 1:H])\n",
    "            AinvHAt[:, H - 1:H + W - 1, H - 1:H + W - 1] = torch.diag_embed(beta)\n",
    "            AinvHAt[:, 0:H - 1, H - 1:H + W - 1] = P[:, 1:H, 0:W]\n",
    "            AinvHAt[:, H - 1:H + W - 1, 0:H - 1] = P[:, 1:H, 0:W].transpose(1, 2)\n",
    "\n",
    "            if ctx.method == 'fullchol':\n",
    "                v = torch.cholesky_solve(torch.cat((vHAt1, vHAt2), dim=1), torch.linalg.cholesky(AinvHAt))\n",
    "            else:\n",
    "                v = torch.bmm(torch.inverse(AinvHAt), torch.cat((vHAt1, vHAt2), dim=1))\n",
    "                #v = torch.linalg.solve(AinvHAt, torch.cat((vHAt1, vHAt2), dim=1))\n",
    "\n",
    "            v1 = v[:, 0:H - 1].view(B, H - 1, 1)\n",
    "            v2 = v[:, H - 1:H + W - 1].view(B, W, 1)\n",
    "\n",
    "        # compute v^T H^{-1} A^T (A H^{-1] A^T)^{-1} A H^{-1} B - v^T H^{-1} B\n",
    "        dJdM[:, 1:H, 0:W] -= v1 * P[:, 1:H, 0:W]\n",
    "        dJdM -= v2.view(B, 1, W) * P\n",
    "\n",
    "        # multiply by derivative of log(M) if in log-space\n",
    "        if ctx.logspace:\n",
    "            dJdM /= -1.0 * M\n",
    "\n",
    "        # compute v^T H^{-1} A^T (A H^{-1] A^T)^{-1} (A H^{-1} B - C) - v^T H^{-1} B\n",
    "        if dJdr is not None:\n",
    "            dJdr = ctx.inv_r_sum.view(r.shape[0], 1) / ctx.gamma * \\\n",
    "                   (torch.sum(r[:, 1:H] * v1.view(B, H - 1), dim=1, keepdim=True) - torch.cat((torch.zeros(B, 1, device=r.device), v1.view(B, H - 1)), dim=1))\n",
    "\n",
    "        # compute v^T H^{-1} A^T (A H^{-1] A^T)^{-1} (A H^{-1} B - C) - v^T H^{-1} B\n",
    "        if dJdc is not None:\n",
    "            dJdc = ctx.inv_c_sum.view(c.shape[0], 1) / ctx.gamma * (torch.sum(c * v2.view(B, W), dim=1, keepdim=True) - v2.view(B, W))\n",
    "\n",
    "        # return gradients (None for gamma, eps, maxiters and logspace)\n",
    "        return dJdM, dJdr, dJdc, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class OptimalTransportLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network layer to implement optimal transport.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gamma: float, default: 1.0\n",
    "        Inverse of the coefficient on the entropy regularisation term.\n",
    "    eps: float, default: 1.0e-6\n",
    "        Tolerance used to determine the stop condition.\n",
    "    maxiters: int, default: 1000\n",
    "        The maximum number of iterations.\n",
    "    logspace: bool, default: False\n",
    "        If `True`, assumes that the input is provided as log M\n",
    "        If `False`, assumes that the input is provided as M (standard optimal transport)\n",
    "    method: str, default: 'block'\n",
    "        If `approx`, approximate the gradient by assuming exp(-gamma M) is already nearly doubly stochastic.\n",
    "        It is faster and could potentially be useful during early stages of training.\n",
    "        If `block`, exploit the block structure of matrix A H^{-1] A^T.\n",
    "        If `full`, invert the full A H^{-1} A^T matrix without exploiting the block structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False, method='block'):\n",
    "        super(OptimalTransportLayer, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.maxiters = maxiters\n",
    "        self.logspace = logspace\n",
    "        self.method = method\n",
    "\n",
    "    def forward(self, M, r=None, c=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        M: torch.Tensor\n",
    "            Input matrix/matrices of size (H, W) or (B, H, W)\n",
    "        r: torch.Tensor, optional\n",
    "            Row sum constraint in the form of a 1xH or BxH matrix. Are assigned uniformly as 1/H by default.\n",
    "        c: torch.Tensor, optional\n",
    "            Column sum constraint in the form of a 1xW or BxW matrix. Are assigned uniformly as 1/W by default.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Normalised matrix/matrices, with the same shape as the inputs\n",
    "        \"\"\"\n",
    "        M_shape = M.shape\n",
    "        # Check the number of dimensions\n",
    "        ndim = len(M_shape)\n",
    "        if ndim == 2:\n",
    "            M = M.unsqueeze(dim=0)\n",
    "        elif ndim != 3:\n",
    "            raise ValueError(f\"The shape of the input tensor {M_shape} does not match that of an matrix\")\n",
    "\n",
    "        # Handle special case of 1x1 matrices\n",
    "        nr, nc = M_shape[-2:]\n",
    "        if nr == 1 and nc == 1:\n",
    "            P = torch.ones_like(M)\n",
    "        else:\n",
    "            P = OptimalTransportFcn.apply(M, r, c, self.gamma, self.eps, self.maxiters, self.logspace, self.method)\n",
    "\n",
    "        return P.view(*M_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT_layer = OptimalTransportLayer()\n",
    "output_pi_star = OT_layer(cost_matrix[2],D_W_P[2].unsqueeze(0),D_T_P[2].unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(cost_matrix)\n",
    "pi_star_tessss = []\n",
    "for i in range(batch_size):\n",
    "    pi_star_i = OT_layer(cost_matrix[i],D_W_P[i].unsqueeze(0), D_T_P[i].unsqueeze(0))\n",
    "    pi_star_tessss.append(pi_star_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0084, 0.0082, 0.0087, 0.0083, 0.0086, 0.0081, 0.0084, 0.0082, 0.0085,\n",
       "          0.0085, 0.0084],\n",
       "         [0.0088, 0.0084, 0.0088, 0.0086, 0.0091, 0.0086, 0.0088, 0.0086, 0.0088,\n",
       "          0.0088, 0.0088],\n",
       "         [0.0087, 0.0083, 0.0087, 0.0085, 0.0089, 0.0084, 0.0088, 0.0084, 0.0085,\n",
       "          0.0085, 0.0085],\n",
       "         [0.0086, 0.0081, 0.0086, 0.0082, 0.0085, 0.0081, 0.0085, 0.0083, 0.0083,\n",
       "          0.0086, 0.0083],\n",
       "         [0.0083, 0.0081, 0.0085, 0.0082, 0.0086, 0.0081, 0.0084, 0.0081, 0.0081,\n",
       "          0.0084, 0.0083],\n",
       "         [0.0084, 0.0081, 0.0084, 0.0083, 0.0086, 0.0081, 0.0084, 0.0083, 0.0082,\n",
       "          0.0084, 0.0082],\n",
       "         [0.0080, 0.0078, 0.0083, 0.0078, 0.0082, 0.0078, 0.0080, 0.0079, 0.0080,\n",
       "          0.0081, 0.0079],\n",
       "         [0.0078, 0.0076, 0.0080, 0.0077, 0.0082, 0.0077, 0.0079, 0.0078, 0.0077,\n",
       "          0.0079, 0.0078],\n",
       "         [0.0076, 0.0071, 0.0076, 0.0073, 0.0078, 0.0074, 0.0076, 0.0073, 0.0074,\n",
       "          0.0074, 0.0076],\n",
       "         [0.0088, 0.0085, 0.0087, 0.0086, 0.0089, 0.0087, 0.0088, 0.0085, 0.0087,\n",
       "          0.0086, 0.0087],\n",
       "         [0.0084, 0.0080, 0.0085, 0.0081, 0.0086, 0.0082, 0.0085, 0.0081, 0.0084,\n",
       "          0.0084, 0.0083]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0200, 0.0195, 0.0205, 0.0195, 0.0208, 0.0197, 0.0202, 0.0199, 0.0198,\n",
       "          0.0202, 0.0199],\n",
       "         [0.0182, 0.0172, 0.0182, 0.0177, 0.0184, 0.0177, 0.0181, 0.0176, 0.0176,\n",
       "          0.0179, 0.0180],\n",
       "         [0.0175, 0.0168, 0.0178, 0.0173, 0.0180, 0.0169, 0.0177, 0.0170, 0.0175,\n",
       "          0.0177, 0.0172],\n",
       "         [0.0173, 0.0166, 0.0172, 0.0169, 0.0179, 0.0169, 0.0172, 0.0168, 0.0172,\n",
       "          0.0173, 0.0169],\n",
       "         [0.0187, 0.0181, 0.0192, 0.0181, 0.0190, 0.0180, 0.0188, 0.0181, 0.0185,\n",
       "          0.0186, 0.0186]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0154, 0.0149, 0.0156, 0.0148, 0.0157, 0.0148, 0.0154, 0.0150, 0.0153,\n",
       "          0.0153, 0.0153],\n",
       "         [0.0161, 0.0158, 0.0162, 0.0160, 0.0167, 0.0156, 0.0162, 0.0159, 0.0161,\n",
       "          0.0161, 0.0162],\n",
       "         [0.0160, 0.0152, 0.0160, 0.0156, 0.0164, 0.0155, 0.0159, 0.0156, 0.0159,\n",
       "          0.0159, 0.0157],\n",
       "         [0.0148, 0.0143, 0.0150, 0.0143, 0.0151, 0.0143, 0.0148, 0.0143, 0.0145,\n",
       "          0.0148, 0.0144],\n",
       "         [0.0143, 0.0136, 0.0143, 0.0139, 0.0145, 0.0141, 0.0141, 0.0136, 0.0141,\n",
       "          0.0144, 0.0142],\n",
       "         [0.0152, 0.0145, 0.0157, 0.0148, 0.0157, 0.0149, 0.0155, 0.0150, 0.0149,\n",
       "          0.0152, 0.0149]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0044, 0.0043, 0.0045, 0.0043, 0.0046, 0.0044, 0.0045, 0.0043, 0.0045,\n",
       "          0.0045, 0.0044],\n",
       "         [0.0048, 0.0046, 0.0048, 0.0046, 0.0049, 0.0046, 0.0048, 0.0046, 0.0046,\n",
       "          0.0047, 0.0047],\n",
       "         [0.0050, 0.0048, 0.0050, 0.0048, 0.0051, 0.0049, 0.0050, 0.0048, 0.0049,\n",
       "          0.0050, 0.0049],\n",
       "         [0.0044, 0.0043, 0.0045, 0.0043, 0.0045, 0.0044, 0.0044, 0.0043, 0.0043,\n",
       "          0.0044, 0.0044],\n",
       "         [0.0048, 0.0046, 0.0048, 0.0046, 0.0048, 0.0046, 0.0047, 0.0046, 0.0047,\n",
       "          0.0048, 0.0046],\n",
       "         [0.0048, 0.0047, 0.0049, 0.0048, 0.0050, 0.0048, 0.0049, 0.0048, 0.0048,\n",
       "          0.0049, 0.0048],\n",
       "         [0.0045, 0.0044, 0.0047, 0.0044, 0.0047, 0.0043, 0.0046, 0.0045, 0.0044,\n",
       "          0.0044, 0.0044],\n",
       "         [0.0049, 0.0046, 0.0049, 0.0047, 0.0049, 0.0047, 0.0048, 0.0047, 0.0047,\n",
       "          0.0049, 0.0049],\n",
       "         [0.0045, 0.0043, 0.0046, 0.0043, 0.0046, 0.0044, 0.0045, 0.0044, 0.0045,\n",
       "          0.0045, 0.0044],\n",
       "         [0.0045, 0.0044, 0.0046, 0.0045, 0.0048, 0.0045, 0.0046, 0.0045, 0.0045,\n",
       "          0.0046, 0.0046],\n",
       "         [0.0044, 0.0043, 0.0046, 0.0044, 0.0045, 0.0044, 0.0045, 0.0044, 0.0045,\n",
       "          0.0044, 0.0045],\n",
       "         [0.0044, 0.0042, 0.0044, 0.0043, 0.0044, 0.0042, 0.0044, 0.0042, 0.0043,\n",
       "          0.0043, 0.0043],\n",
       "         [0.0046, 0.0043, 0.0046, 0.0044, 0.0047, 0.0043, 0.0045, 0.0044, 0.0045,\n",
       "          0.0045, 0.0044],\n",
       "         [0.0045, 0.0043, 0.0046, 0.0044, 0.0047, 0.0044, 0.0045, 0.0044, 0.0045,\n",
       "          0.0044, 0.0045],\n",
       "         [0.0043, 0.0042, 0.0044, 0.0043, 0.0044, 0.0042, 0.0044, 0.0043, 0.0043,\n",
       "          0.0044, 0.0043],\n",
       "         [0.0046, 0.0043, 0.0046, 0.0045, 0.0047, 0.0044, 0.0046, 0.0044, 0.0045,\n",
       "          0.0045, 0.0045],\n",
       "         [0.0048, 0.0047, 0.0048, 0.0047, 0.0049, 0.0046, 0.0048, 0.0047, 0.0047,\n",
       "          0.0048, 0.0048],\n",
       "         [0.0044, 0.0041, 0.0044, 0.0042, 0.0045, 0.0042, 0.0043, 0.0042, 0.0043,\n",
       "          0.0044, 0.0043],\n",
       "         [0.0047, 0.0045, 0.0047, 0.0046, 0.0048, 0.0045, 0.0047, 0.0046, 0.0047,\n",
       "          0.0047, 0.0046],\n",
       "         [0.0045, 0.0043, 0.0045, 0.0043, 0.0046, 0.0044, 0.0045, 0.0043, 0.0044,\n",
       "          0.0044, 0.0044]], grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star_tessss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/kkv19kbs5yb5yfn9jbkbfnl40000gn/T/ipykernel_17273/4276768131.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.rand([11,768]),requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(torch.rand([11,768]),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0228,  0.0673,  0.0134,  ...,  0.0033,  0.0552,  0.0141],\n",
       "        [ 0.0050,  0.0851,  0.0199,  ...,  0.0821, -0.0139,  0.0142],\n",
       "        [ 0.0420,  0.0297, -0.0061,  ..., -0.0962,  0.0189,  0.0341],\n",
       "        ...,\n",
       "        [ 0.0117,  0.0637, -0.0327,  ...,  0.0016,  0.0727, -0.0523],\n",
       "        [-0.0041,  0.0765,  0.0267,  ..., -0.0005,  0.0262,  0.0568],\n",
       "        [ 0.0381, -0.0104, -0.0840,  ..., -0.0699, -0.0283,  0.0368]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_normal_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0142,  0.0402, -0.0456,  ...,  0.0238, -0.0007, -0.0039],\n",
       "        [-0.0116,  0.0412,  0.0567,  ..., -0.0134, -0.0130, -0.0147],\n",
       "        [ 0.0558,  0.0363,  0.0165,  ...,  0.0330,  0.0237,  0.0063],\n",
       "        ...,\n",
       "        [-0.0449,  0.0437, -0.0218,  ..., -0.1183,  0.1327, -0.1092],\n",
       "        [-0.0793,  0.0837,  0.0344,  ..., -0.0134, -0.0063, -0.1125],\n",
       "        [ 0.0041, -0.1012,  0.0080,  ..., -0.0110, -0.0322,  0.0109]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.empty([11,767],requires_grad=True)\n",
    "torch.nn.init.xavier_normal_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000], Loss: 6.9158\n",
      "Epoch [100/100000], Loss: 6.8985\n",
      "Epoch [200/100000], Loss: 6.8813\n",
      "Epoch [300/100000], Loss: 6.8641\n",
      "Epoch [400/100000], Loss: 6.8469\n",
      "Epoch [500/100000], Loss: 6.8297\n",
      "Epoch [600/100000], Loss: 6.8125\n",
      "Epoch [700/100000], Loss: 6.7953\n",
      "Epoch [800/100000], Loss: 6.7782\n",
      "Epoch [900/100000], Loss: 6.7611\n",
      "Epoch [1000/100000], Loss: 6.7440\n",
      "Epoch [1100/100000], Loss: 6.7269\n",
      "Epoch [1200/100000], Loss: 6.7098\n",
      "Epoch [1300/100000], Loss: 6.6927\n",
      "Epoch [1400/100000], Loss: 6.6756\n",
      "Epoch [1500/100000], Loss: 6.6585\n",
      "Epoch [1600/100000], Loss: 6.6415\n",
      "Epoch [1700/100000], Loss: 6.6244\n",
      "Epoch [1800/100000], Loss: 6.6074\n",
      "Epoch [1900/100000], Loss: 6.5903\n",
      "Epoch [2000/100000], Loss: 6.5733\n",
      "Epoch [2100/100000], Loss: 6.5563\n",
      "Epoch [2200/100000], Loss: 6.5392\n",
      "Epoch [2300/100000], Loss: 6.5222\n",
      "Epoch [2400/100000], Loss: 6.5051\n",
      "Epoch [2500/100000], Loss: 6.4881\n",
      "Epoch [2600/100000], Loss: 6.4711\n",
      "Epoch [2700/100000], Loss: 6.4541\n",
      "Epoch [2800/100000], Loss: 6.4370\n",
      "Epoch [2900/100000], Loss: 6.4200\n",
      "Epoch [3000/100000], Loss: 6.4030\n",
      "Epoch [3100/100000], Loss: 6.3860\n",
      "Epoch [3200/100000], Loss: 6.3690\n",
      "Epoch [3300/100000], Loss: 6.3519\n",
      "Epoch [3400/100000], Loss: 6.3349\n",
      "Epoch [3500/100000], Loss: 6.3179\n",
      "Epoch [3600/100000], Loss: 6.3009\n",
      "Epoch [3700/100000], Loss: 6.2839\n",
      "Epoch [3800/100000], Loss: 6.2669\n",
      "Epoch [3900/100000], Loss: 6.2499\n",
      "Epoch [4000/100000], Loss: 6.2329\n",
      "Epoch [4100/100000], Loss: 6.2159\n",
      "Epoch [4200/100000], Loss: 6.1989\n",
      "Epoch [4300/100000], Loss: 6.1819\n",
      "Epoch [4400/100000], Loss: 6.1649\n",
      "Epoch [4500/100000], Loss: 6.1479\n",
      "Epoch [4600/100000], Loss: 6.1309\n",
      "Epoch [4700/100000], Loss: 6.1139\n",
      "Epoch [4800/100000], Loss: 6.0969\n",
      "Epoch [4900/100000], Loss: 6.0799\n",
      "Epoch [5000/100000], Loss: 6.0629\n",
      "Epoch [5100/100000], Loss: 6.0460\n",
      "Epoch [5200/100000], Loss: 6.0290\n",
      "Epoch [5300/100000], Loss: 6.0120\n",
      "Epoch [5400/100000], Loss: 5.9950\n",
      "Epoch [5500/100000], Loss: 5.9780\n",
      "Epoch [5600/100000], Loss: 5.9611\n",
      "Epoch [5700/100000], Loss: 5.9441\n",
      "Epoch [5800/100000], Loss: 5.9271\n",
      "Epoch [5900/100000], Loss: 5.9102\n",
      "Epoch [6000/100000], Loss: 5.8932\n",
      "Epoch [6100/100000], Loss: 5.8762\n",
      "Epoch [6200/100000], Loss: 5.8593\n",
      "Epoch [6300/100000], Loss: 5.8423\n",
      "Epoch [6400/100000], Loss: 5.8254\n",
      "Epoch [6500/100000], Loss: 5.8084\n",
      "Epoch [6600/100000], Loss: 5.7915\n",
      "Epoch [6700/100000], Loss: 5.7745\n",
      "Epoch [6800/100000], Loss: 5.7576\n",
      "Epoch [6900/100000], Loss: 5.7406\n",
      "Epoch [7000/100000], Loss: 5.7237\n",
      "Epoch [7100/100000], Loss: 5.7067\n",
      "Epoch [7200/100000], Loss: 5.6898\n",
      "Epoch [7300/100000], Loss: 5.6729\n",
      "Epoch [7400/100000], Loss: 5.6559\n",
      "Epoch [7500/100000], Loss: 5.6390\n",
      "Epoch [7600/100000], Loss: 5.6221\n",
      "Epoch [7700/100000], Loss: 5.6052\n",
      "Epoch [7800/100000], Loss: 5.5883\n",
      "Epoch [7900/100000], Loss: 5.5713\n",
      "Epoch [8000/100000], Loss: 5.5544\n",
      "Epoch [8100/100000], Loss: 5.5375\n",
      "Epoch [8200/100000], Loss: 5.5206\n",
      "Epoch [8300/100000], Loss: 5.5037\n",
      "Epoch [8400/100000], Loss: 5.4868\n",
      "Epoch [8500/100000], Loss: 5.4699\n",
      "Epoch [8600/100000], Loss: 5.4530\n",
      "Epoch [8700/100000], Loss: 5.4361\n",
      "Epoch [8800/100000], Loss: 5.4192\n",
      "Epoch [8900/100000], Loss: 5.4023\n",
      "Epoch [9000/100000], Loss: 5.3854\n",
      "Epoch [9100/100000], Loss: 5.3685\n",
      "Epoch [9200/100000], Loss: 5.3516\n",
      "Epoch [9300/100000], Loss: 5.3347\n",
      "Epoch [9400/100000], Loss: 5.3178\n",
      "Epoch [9500/100000], Loss: 5.3010\n",
      "Epoch [9600/100000], Loss: 5.2841\n",
      "Epoch [9700/100000], Loss: 5.2672\n",
      "Epoch [9800/100000], Loss: 5.2503\n",
      "Epoch [9900/100000], Loss: 5.2335\n",
      "Epoch [10000/100000], Loss: 5.2166\n",
      "Epoch [10100/100000], Loss: 5.1997\n",
      "Epoch [10200/100000], Loss: 5.1829\n",
      "Epoch [10300/100000], Loss: 5.1660\n",
      "Epoch [10400/100000], Loss: 5.1492\n",
      "Epoch [10500/100000], Loss: 5.1323\n",
      "Epoch [10600/100000], Loss: 5.1155\n",
      "Epoch [10700/100000], Loss: 5.0986\n",
      "Epoch [10800/100000], Loss: 5.0818\n",
      "Epoch [10900/100000], Loss: 5.0650\n",
      "Epoch [11000/100000], Loss: 5.0481\n",
      "Epoch [11100/100000], Loss: 5.0313\n",
      "Epoch [11200/100000], Loss: 5.0145\n",
      "Epoch [11300/100000], Loss: 4.9976\n",
      "Epoch [11400/100000], Loss: 4.9808\n",
      "Epoch [11500/100000], Loss: 4.9640\n",
      "Epoch [11600/100000], Loss: 4.9472\n",
      "Epoch [11700/100000], Loss: 4.9304\n",
      "Epoch [11800/100000], Loss: 4.9136\n",
      "Epoch [11900/100000], Loss: 4.8968\n",
      "Epoch [12000/100000], Loss: 4.8800\n",
      "Epoch [12100/100000], Loss: 4.8632\n",
      "Epoch [12200/100000], Loss: 4.8464\n",
      "Epoch [12300/100000], Loss: 4.8296\n",
      "Epoch [12400/100000], Loss: 4.8128\n",
      "Epoch [12500/100000], Loss: 4.7960\n",
      "Epoch [12600/100000], Loss: 4.7792\n",
      "Epoch [12700/100000], Loss: 4.7625\n",
      "Epoch [12800/100000], Loss: 4.7457\n",
      "Epoch [12900/100000], Loss: 4.7289\n",
      "Epoch [13000/100000], Loss: 4.7122\n",
      "Epoch [13100/100000], Loss: 4.6954\n",
      "Epoch [13200/100000], Loss: 4.6787\n",
      "Epoch [13300/100000], Loss: 4.6619\n",
      "Epoch [13400/100000], Loss: 4.6452\n",
      "Epoch [13500/100000], Loss: 4.6284\n",
      "Epoch [13600/100000], Loss: 4.6117\n",
      "Epoch [13700/100000], Loss: 4.5950\n",
      "Epoch [13800/100000], Loss: 4.5782\n",
      "Epoch [13900/100000], Loss: 4.5615\n",
      "Epoch [14000/100000], Loss: 4.5448\n",
      "Epoch [14100/100000], Loss: 4.5281\n",
      "Epoch [14200/100000], Loss: 4.5114\n",
      "Epoch [14300/100000], Loss: 4.4947\n",
      "Epoch [14400/100000], Loss: 4.4780\n",
      "Epoch [14500/100000], Loss: 4.4613\n",
      "Epoch [14600/100000], Loss: 4.4446\n",
      "Epoch [14700/100000], Loss: 4.4279\n",
      "Epoch [14800/100000], Loss: 4.4112\n",
      "Epoch [14900/100000], Loss: 4.3945\n",
      "Epoch [15000/100000], Loss: 4.3779\n",
      "Epoch [15100/100000], Loss: 4.3612\n",
      "Epoch [15200/100000], Loss: 4.3445\n",
      "Epoch [15300/100000], Loss: 4.3279\n",
      "Epoch [15400/100000], Loss: 4.3112\n",
      "Epoch [15500/100000], Loss: 4.2946\n",
      "Epoch [15600/100000], Loss: 4.2779\n",
      "Epoch [15700/100000], Loss: 4.2613\n",
      "Epoch [15800/100000], Loss: 4.2447\n",
      "Epoch [15900/100000], Loss: 4.2281\n",
      "Epoch [16000/100000], Loss: 4.2114\n",
      "Epoch [16100/100000], Loss: 4.1948\n",
      "Epoch [16200/100000], Loss: 4.1782\n",
      "Epoch [16300/100000], Loss: 4.1616\n",
      "Epoch [16400/100000], Loss: 4.1450\n",
      "Epoch [16500/100000], Loss: 4.1284\n",
      "Epoch [16600/100000], Loss: 4.1118\n",
      "Epoch [16700/100000], Loss: 4.0953\n",
      "Epoch [16800/100000], Loss: 4.0787\n",
      "Epoch [16900/100000], Loss: 4.0621\n",
      "Epoch [17000/100000], Loss: 4.0455\n",
      "Epoch [17100/100000], Loss: 4.0290\n",
      "Epoch [17200/100000], Loss: 4.0124\n",
      "Epoch [17300/100000], Loss: 3.9959\n",
      "Epoch [17400/100000], Loss: 3.9794\n",
      "Epoch [17500/100000], Loss: 3.9628\n",
      "Epoch [17600/100000], Loss: 3.9463\n",
      "Epoch [17700/100000], Loss: 3.9298\n",
      "Epoch [17800/100000], Loss: 3.9133\n",
      "Epoch [17900/100000], Loss: 3.8968\n",
      "Epoch [18000/100000], Loss: 3.8803\n",
      "Epoch [18100/100000], Loss: 3.8638\n",
      "Epoch [18200/100000], Loss: 3.8473\n",
      "Epoch [18300/100000], Loss: 3.8309\n",
      "Epoch [18400/100000], Loss: 3.8144\n",
      "Epoch [18500/100000], Loss: 3.7979\n",
      "Epoch [18600/100000], Loss: 3.7815\n",
      "Epoch [18700/100000], Loss: 3.7651\n",
      "Epoch [18800/100000], Loss: 3.7486\n",
      "Epoch [18900/100000], Loss: 3.7322\n",
      "Epoch [19000/100000], Loss: 3.7158\n",
      "Epoch [19100/100000], Loss: 3.6993\n",
      "Epoch [19200/100000], Loss: 3.6830\n",
      "Epoch [19300/100000], Loss: 3.6666\n",
      "Epoch [19400/100000], Loss: 3.6502\n",
      "Epoch [19500/100000], Loss: 3.6338\n",
      "Epoch [19600/100000], Loss: 3.6174\n",
      "Epoch [19700/100000], Loss: 3.6011\n",
      "Epoch [19800/100000], Loss: 3.5847\n",
      "Epoch [19900/100000], Loss: 3.5684\n",
      "Epoch [20000/100000], Loss: 3.5521\n",
      "Epoch [20100/100000], Loss: 3.5357\n",
      "Epoch [20200/100000], Loss: 3.5194\n",
      "Epoch [20300/100000], Loss: 3.5031\n",
      "Epoch [20400/100000], Loss: 3.4868\n",
      "Epoch [20500/100000], Loss: 3.4705\n",
      "Epoch [20600/100000], Loss: 3.4542\n",
      "Epoch [20700/100000], Loss: 3.4380\n",
      "Epoch [20800/100000], Loss: 3.4217\n",
      "Epoch [20900/100000], Loss: 3.4055\n",
      "Epoch [21000/100000], Loss: 3.3892\n",
      "Epoch [21100/100000], Loss: 3.3730\n",
      "Epoch [21200/100000], Loss: 3.3568\n",
      "Epoch [21300/100000], Loss: 3.3406\n",
      "Epoch [21400/100000], Loss: 3.3244\n",
      "Epoch [21500/100000], Loss: 3.3082\n",
      "Epoch [21600/100000], Loss: 3.2920\n",
      "Epoch [21700/100000], Loss: 3.2759\n",
      "Epoch [21800/100000], Loss: 3.2597\n",
      "Epoch [21900/100000], Loss: 3.2436\n",
      "Epoch [22000/100000], Loss: 3.2275\n",
      "Epoch [22100/100000], Loss: 3.2114\n",
      "Epoch [22200/100000], Loss: 3.1953\n",
      "Epoch [22300/100000], Loss: 3.1792\n",
      "Epoch [22400/100000], Loss: 3.1631\n",
      "Epoch [22500/100000], Loss: 3.1471\n",
      "Epoch [22600/100000], Loss: 3.1310\n",
      "Epoch [22700/100000], Loss: 3.1150\n",
      "Epoch [22800/100000], Loss: 3.0990\n",
      "Epoch [22900/100000], Loss: 3.0830\n",
      "Epoch [23000/100000], Loss: 3.0670\n",
      "Epoch [23100/100000], Loss: 3.0510\n",
      "Epoch [23200/100000], Loss: 3.0350\n",
      "Epoch [23300/100000], Loss: 3.0191\n",
      "Epoch [23400/100000], Loss: 3.0031\n",
      "Epoch [23500/100000], Loss: 2.9872\n",
      "Epoch [23600/100000], Loss: 2.9713\n",
      "Epoch [23700/100000], Loss: 2.9554\n",
      "Epoch [23800/100000], Loss: 2.9395\n",
      "Epoch [23900/100000], Loss: 2.9237\n",
      "Epoch [24000/100000], Loss: 2.9078\n",
      "Epoch [24100/100000], Loss: 2.8920\n",
      "Epoch [24200/100000], Loss: 2.8762\n",
      "Epoch [24300/100000], Loss: 2.8604\n",
      "Epoch [24400/100000], Loss: 2.8446\n",
      "Epoch [24500/100000], Loss: 2.8289\n",
      "Epoch [24600/100000], Loss: 2.8131\n",
      "Epoch [24700/100000], Loss: 2.7974\n",
      "Epoch [24800/100000], Loss: 2.7817\n",
      "Epoch [24900/100000], Loss: 2.7660\n",
      "Epoch [25000/100000], Loss: 2.7503\n",
      "Epoch [25100/100000], Loss: 2.7347\n",
      "Epoch [25200/100000], Loss: 2.7191\n",
      "Epoch [25300/100000], Loss: 2.7035\n",
      "Epoch [25400/100000], Loss: 2.6879\n",
      "Epoch [25500/100000], Loss: 2.6723\n",
      "Epoch [25600/100000], Loss: 2.6567\n",
      "Epoch [25700/100000], Loss: 2.6412\n",
      "Epoch [25800/100000], Loss: 2.6257\n",
      "Epoch [25900/100000], Loss: 2.6102\n",
      "Epoch [26000/100000], Loss: 2.5948\n",
      "Epoch [26100/100000], Loss: 2.5793\n",
      "Epoch [26200/100000], Loss: 2.5639\n",
      "Epoch [26300/100000], Loss: 2.5485\n",
      "Epoch [26400/100000], Loss: 2.5331\n",
      "Epoch [26500/100000], Loss: 2.5178\n",
      "Epoch [26600/100000], Loss: 2.5025\n",
      "Epoch [26700/100000], Loss: 2.4872\n",
      "Epoch [26800/100000], Loss: 2.4719\n",
      "Epoch [26900/100000], Loss: 2.4566\n",
      "Epoch [27000/100000], Loss: 2.4414\n",
      "Epoch [27100/100000], Loss: 2.4262\n",
      "Epoch [27200/100000], Loss: 2.4111\n",
      "Epoch [27300/100000], Loss: 2.3959\n",
      "Epoch [27400/100000], Loss: 2.3808\n",
      "Epoch [27500/100000], Loss: 2.3657\n",
      "Epoch [27600/100000], Loss: 2.3507\n",
      "Epoch [27700/100000], Loss: 2.3356\n",
      "Epoch [27800/100000], Loss: 2.3206\n",
      "Epoch [27900/100000], Loss: 2.3057\n",
      "Epoch [28000/100000], Loss: 2.2907\n",
      "Epoch [28100/100000], Loss: 2.2758\n",
      "Epoch [28200/100000], Loss: 2.2610\n",
      "Epoch [28300/100000], Loss: 2.2461\n",
      "Epoch [28400/100000], Loss: 2.2313\n",
      "Epoch [28500/100000], Loss: 2.2166\n",
      "Epoch [28600/100000], Loss: 2.2018\n",
      "Epoch [28700/100000], Loss: 2.1871\n",
      "Epoch [28800/100000], Loss: 2.1724\n",
      "Epoch [28900/100000], Loss: 2.1578\n",
      "Epoch [29000/100000], Loss: 2.1432\n",
      "Epoch [29100/100000], Loss: 2.1287\n",
      "Epoch [29200/100000], Loss: 2.1141\n",
      "Epoch [29300/100000], Loss: 2.0996\n",
      "Epoch [29400/100000], Loss: 2.0852\n",
      "Epoch [29500/100000], Loss: 2.0708\n",
      "Epoch [29600/100000], Loss: 2.0564\n",
      "Epoch [29700/100000], Loss: 2.0421\n",
      "Epoch [29800/100000], Loss: 2.0278\n",
      "Epoch [29900/100000], Loss: 2.0136\n",
      "Epoch [30000/100000], Loss: 1.9994\n",
      "Epoch [30100/100000], Loss: 1.9853\n",
      "Epoch [30200/100000], Loss: 1.9711\n",
      "Epoch [30300/100000], Loss: 1.9571\n",
      "Epoch [30400/100000], Loss: 1.9431\n",
      "Epoch [30500/100000], Loss: 1.9291\n",
      "Epoch [30600/100000], Loss: 1.9152\n",
      "Epoch [30700/100000], Loss: 1.9013\n",
      "Epoch [30800/100000], Loss: 1.8875\n",
      "Epoch [30900/100000], Loss: 1.8737\n",
      "Epoch [31000/100000], Loss: 1.8600\n",
      "Epoch [31100/100000], Loss: 1.8463\n",
      "Epoch [31200/100000], Loss: 1.8327\n",
      "Epoch [31300/100000], Loss: 1.8192\n",
      "Epoch [31400/100000], Loss: 1.8057\n",
      "Epoch [31500/100000], Loss: 1.7922\n",
      "Epoch [31600/100000], Loss: 1.7788\n",
      "Epoch [31700/100000], Loss: 1.7655\n",
      "Epoch [31800/100000], Loss: 1.7522\n",
      "Epoch [31900/100000], Loss: 1.7390\n",
      "Epoch [32000/100000], Loss: 1.7258\n",
      "Epoch [32100/100000], Loss: 1.7127\n",
      "Epoch [32200/100000], Loss: 1.6996\n",
      "Epoch [32300/100000], Loss: 1.6867\n",
      "Epoch [32400/100000], Loss: 1.6737\n",
      "Epoch [32500/100000], Loss: 1.6609\n",
      "Epoch [32600/100000], Loss: 1.6481\n",
      "Epoch [32700/100000], Loss: 1.6354\n",
      "Epoch [32800/100000], Loss: 1.6227\n",
      "Epoch [32900/100000], Loss: 1.6101\n",
      "Epoch [33000/100000], Loss: 1.5976\n",
      "Epoch [33100/100000], Loss: 1.5851\n",
      "Epoch [33200/100000], Loss: 1.5727\n",
      "Epoch [33300/100000], Loss: 1.5604\n",
      "Epoch [33400/100000], Loss: 1.5482\n",
      "Epoch [33500/100000], Loss: 1.5360\n",
      "Epoch [33600/100000], Loss: 1.5238\n",
      "Epoch [33700/100000], Loss: 1.5118\n",
      "Epoch [33800/100000], Loss: 1.4998\n",
      "Epoch [33900/100000], Loss: 1.4879\n",
      "Epoch [34000/100000], Loss: 1.4761\n",
      "Epoch [34100/100000], Loss: 1.4643\n",
      "Epoch [34200/100000], Loss: 1.4526\n",
      "Epoch [34300/100000], Loss: 1.4410\n",
      "Epoch [34400/100000], Loss: 1.4295\n",
      "Epoch [34500/100000], Loss: 1.4180\n",
      "Epoch [34600/100000], Loss: 1.4066\n",
      "Epoch [34700/100000], Loss: 1.3952\n",
      "Epoch [34800/100000], Loss: 1.3839\n",
      "Epoch [34900/100000], Loss: 1.3727\n",
      "Epoch [35000/100000], Loss: 1.3616\n",
      "Epoch [35100/100000], Loss: 1.3505\n",
      "Epoch [35200/100000], Loss: 1.3395\n",
      "Epoch [35300/100000], Loss: 1.3286\n",
      "Epoch [35400/100000], Loss: 1.3177\n",
      "Epoch [35500/100000], Loss: 1.3069\n",
      "Epoch [35600/100000], Loss: 1.2961\n",
      "Epoch [35700/100000], Loss: 1.2854\n",
      "Epoch [35800/100000], Loss: 1.2747\n",
      "Epoch [35900/100000], Loss: 1.2641\n",
      "Epoch [36000/100000], Loss: 1.2536\n",
      "Epoch [36100/100000], Loss: 1.2431\n",
      "Epoch [36200/100000], Loss: 1.2326\n",
      "Epoch [36300/100000], Loss: 1.2222\n",
      "Epoch [36400/100000], Loss: 1.2118\n",
      "Epoch [36500/100000], Loss: 1.2014\n",
      "Epoch [36600/100000], Loss: 1.1911\n",
      "Epoch [36700/100000], Loss: 1.1809\n",
      "Epoch [36800/100000], Loss: 1.1706\n",
      "Epoch [36900/100000], Loss: 1.1604\n",
      "Epoch [37000/100000], Loss: 1.1502\n",
      "Epoch [37100/100000], Loss: 1.1400\n",
      "Epoch [37200/100000], Loss: 1.1299\n",
      "Epoch [37300/100000], Loss: 1.1198\n",
      "Epoch [37400/100000], Loss: 1.1097\n",
      "Epoch [37500/100000], Loss: 1.0996\n",
      "Epoch [37600/100000], Loss: 1.0895\n",
      "Epoch [37700/100000], Loss: 1.0794\n",
      "Epoch [37800/100000], Loss: 1.0694\n",
      "Epoch [37900/100000], Loss: 1.0593\n",
      "Epoch [38000/100000], Loss: 1.0493\n",
      "Epoch [38100/100000], Loss: 1.0393\n",
      "Epoch [38200/100000], Loss: 1.0293\n",
      "Epoch [38300/100000], Loss: 1.0192\n",
      "Epoch [38400/100000], Loss: 1.0092\n",
      "Epoch [38500/100000], Loss: 0.9992\n",
      "Epoch [38600/100000], Loss: 0.9892\n",
      "Epoch [38700/100000], Loss: 0.9792\n",
      "Epoch [38800/100000], Loss: 0.9692\n",
      "Epoch [38900/100000], Loss: 0.9591\n",
      "Epoch [39000/100000], Loss: 0.9491\n",
      "Epoch [39100/100000], Loss: 0.9391\n",
      "Epoch [39200/100000], Loss: 0.9291\n",
      "Epoch [39300/100000], Loss: 0.9191\n",
      "Epoch [39400/100000], Loss: 0.9091\n",
      "Epoch [39500/100000], Loss: 0.8991\n",
      "Epoch [39600/100000], Loss: 0.8890\n",
      "Epoch [39700/100000], Loss: 0.8790\n",
      "Epoch [39800/100000], Loss: 0.8690\n",
      "Epoch [39900/100000], Loss: 0.8590\n",
      "Epoch [40000/100000], Loss: 0.8490\n",
      "Epoch [40100/100000], Loss: 0.8390\n",
      "Epoch [40200/100000], Loss: 0.8290\n",
      "Epoch [40300/100000], Loss: 0.8189\n",
      "Epoch [40400/100000], Loss: 0.8090\n",
      "Epoch [40500/100000], Loss: 0.7990\n",
      "Epoch [40600/100000], Loss: 0.7890\n",
      "Epoch [40700/100000], Loss: 0.7790\n",
      "Epoch [40800/100000], Loss: 0.7690\n",
      "Epoch [40900/100000], Loss: 0.7590\n",
      "Epoch [41000/100000], Loss: 0.7490\n",
      "Epoch [41100/100000], Loss: 0.7390\n",
      "Epoch [41200/100000], Loss: 0.7290\n",
      "Epoch [41300/100000], Loss: 0.7190\n",
      "Epoch [41400/100000], Loss: 0.7091\n",
      "Epoch [41500/100000], Loss: 0.6991\n",
      "Epoch [41600/100000], Loss: 0.6891\n",
      "Epoch [41700/100000], Loss: 0.6791\n",
      "Epoch [41800/100000], Loss: 0.6691\n",
      "Epoch [41900/100000], Loss: 0.6591\n",
      "Epoch [42000/100000], Loss: 0.6491\n",
      "Epoch [42100/100000], Loss: 0.6391\n",
      "Epoch [42200/100000], Loss: 0.6291\n",
      "Epoch [42300/100000], Loss: 0.6191\n",
      "Epoch [42400/100000], Loss: 0.6092\n",
      "Epoch [42500/100000], Loss: 0.5992\n",
      "Epoch [42600/100000], Loss: 0.5892\n",
      "Epoch [42700/100000], Loss: 0.5792\n",
      "Epoch [42800/100000], Loss: 0.5692\n",
      "Epoch [42900/100000], Loss: 0.5592\n",
      "Epoch [43000/100000], Loss: 0.5492\n",
      "Epoch [43100/100000], Loss: 0.5392\n",
      "Epoch [43200/100000], Loss: 0.5292\n",
      "Epoch [43300/100000], Loss: 0.5192\n",
      "Epoch [43400/100000], Loss: 0.5093\n",
      "Epoch [43500/100000], Loss: 0.4993\n",
      "Epoch [43600/100000], Loss: 0.4893\n",
      "Epoch [43700/100000], Loss: 0.4793\n",
      "Epoch [43800/100000], Loss: 0.4693\n",
      "Epoch [43900/100000], Loss: 0.4593\n",
      "Epoch [44000/100000], Loss: 0.4493\n",
      "Epoch [44100/100000], Loss: 0.4393\n",
      "Epoch [44200/100000], Loss: 0.4293\n",
      "Epoch [44300/100000], Loss: 0.4194\n",
      "Epoch [44400/100000], Loss: 0.4094\n",
      "Epoch [44500/100000], Loss: 0.3994\n",
      "Epoch [44600/100000], Loss: 0.3894\n",
      "Epoch [44700/100000], Loss: 0.3794\n",
      "Epoch [44800/100000], Loss: 0.3694\n",
      "Epoch [44900/100000], Loss: 0.3594\n",
      "Epoch [45000/100000], Loss: 0.3494\n",
      "Epoch [45100/100000], Loss: 0.3394\n",
      "Epoch [45200/100000], Loss: 0.3294\n",
      "Epoch [45300/100000], Loss: 0.3195\n",
      "Epoch [45400/100000], Loss: 0.3095\n",
      "Epoch [45500/100000], Loss: 0.2995\n",
      "Epoch [45600/100000], Loss: 0.2895\n",
      "Epoch [45700/100000], Loss: 0.2795\n",
      "Epoch [45800/100000], Loss: 0.2695\n",
      "Epoch [45900/100000], Loss: 0.2595\n",
      "Epoch [46000/100000], Loss: 0.2495\n",
      "Epoch [46100/100000], Loss: 0.2395\n",
      "Epoch [46200/100000], Loss: 0.2295\n",
      "Epoch [46300/100000], Loss: 0.2196\n",
      "Epoch [46400/100000], Loss: 0.2096\n",
      "Epoch [46500/100000], Loss: 0.1996\n",
      "Epoch [46600/100000], Loss: 0.1896\n",
      "Epoch [46700/100000], Loss: 0.1796\n",
      "Epoch [46800/100000], Loss: 0.1696\n",
      "Epoch [46900/100000], Loss: 0.1596\n",
      "Epoch [47000/100000], Loss: 0.1496\n",
      "Epoch [47100/100000], Loss: 0.1396\n",
      "Epoch [47200/100000], Loss: 0.1296\n",
      "Epoch [47300/100000], Loss: 0.1197\n",
      "Epoch [47400/100000], Loss: 0.1097\n",
      "Epoch [47500/100000], Loss: 0.0997\n",
      "Epoch [47600/100000], Loss: 0.0897\n",
      "Epoch [47700/100000], Loss: 0.0797\n",
      "Epoch [47800/100000], Loss: 0.0697\n",
      "Epoch [47900/100000], Loss: 0.0597\n",
      "Epoch [48000/100000], Loss: 0.0497\n",
      "Epoch [48100/100000], Loss: 0.0397\n",
      "Epoch [48200/100000], Loss: 0.0298\n",
      "Epoch [48300/100000], Loss: 0.0198\n",
      "Epoch [48400/100000], Loss: 0.0098\n",
      "Epoch [48500/100000], Loss: 0.0002\n",
      "Epoch [48600/100000], Loss: 0.0000\n",
      "Epoch [48700/100000], Loss: 0.0000\n",
      "Epoch [48800/100000], Loss: 0.0000\n",
      "Epoch [48900/100000], Loss: 0.0000\n",
      "Epoch [49000/100000], Loss: 0.0000\n",
      "Epoch [49100/100000], Loss: 0.0000\n",
      "Epoch [49200/100000], Loss: 0.0000\n",
      "Epoch [49300/100000], Loss: 0.0000\n",
      "Epoch [49400/100000], Loss: 0.0000\n",
      "Epoch [49500/100000], Loss: 0.0000\n",
      "Epoch [49600/100000], Loss: 0.0000\n",
      "Epoch [49700/100000], Loss: 0.0000\n",
      "Epoch [49800/100000], Loss: 0.0000\n",
      "Epoch [49900/100000], Loss: 0.0000\n",
      "Epoch [50000/100000], Loss: 0.0000\n",
      "Epoch [50100/100000], Loss: 0.0000\n",
      "Epoch [50200/100000], Loss: 0.0000\n",
      "Epoch [50300/100000], Loss: 0.0000\n",
      "Epoch [50400/100000], Loss: 0.0000\n",
      "Epoch [50500/100000], Loss: 0.0000\n",
      "Epoch [50600/100000], Loss: 0.0000\n",
      "Epoch [50700/100000], Loss: 0.0000\n",
      "Epoch [50800/100000], Loss: 0.0000\n",
      "Epoch [50900/100000], Loss: 0.0000\n",
      "Epoch [51000/100000], Loss: 0.0000\n",
      "Epoch [51100/100000], Loss: 0.0000\n",
      "Epoch [51200/100000], Loss: 0.0000\n",
      "Epoch [51300/100000], Loss: 0.0000\n",
      "Epoch [51400/100000], Loss: 0.0000\n",
      "Epoch [51500/100000], Loss: 0.0000\n",
      "Epoch [51600/100000], Loss: 0.0000\n",
      "Epoch [51700/100000], Loss: 0.0000\n",
      "Epoch [51800/100000], Loss: 0.0000\n",
      "Epoch [51900/100000], Loss: 0.0000\n",
      "Epoch [52000/100000], Loss: 0.0000\n",
      "Epoch [52100/100000], Loss: 0.0000\n",
      "Epoch [52200/100000], Loss: 0.0000\n",
      "Epoch [52300/100000], Loss: 0.0000\n",
      "Epoch [52400/100000], Loss: 0.0000\n",
      "Epoch [52500/100000], Loss: 0.0000\n",
      "Epoch [52600/100000], Loss: 0.0000\n",
      "Epoch [52700/100000], Loss: 0.0000\n",
      "Epoch [52800/100000], Loss: 0.0000\n",
      "Epoch [52900/100000], Loss: 0.0000\n",
      "Epoch [53000/100000], Loss: 0.0000\n",
      "Epoch [53100/100000], Loss: 0.0000\n",
      "Epoch [53200/100000], Loss: 0.0000\n",
      "Epoch [53300/100000], Loss: 0.0000\n",
      "Epoch [53400/100000], Loss: 0.0000\n",
      "Epoch [53500/100000], Loss: 0.0000\n",
      "Epoch [53600/100000], Loss: 0.0000\n",
      "Epoch [53700/100000], Loss: 0.0000\n",
      "Epoch [53800/100000], Loss: 0.0000\n",
      "Epoch [53900/100000], Loss: 0.0000\n",
      "Epoch [54000/100000], Loss: 0.0000\n",
      "Epoch [54100/100000], Loss: 0.0000\n",
      "Epoch [54200/100000], Loss: 0.0000\n",
      "Epoch [54300/100000], Loss: 0.0000\n",
      "Epoch [54400/100000], Loss: 0.0000\n",
      "Epoch [54500/100000], Loss: 0.0000\n",
      "Epoch [54600/100000], Loss: 0.0000\n",
      "Epoch [54700/100000], Loss: 0.0000\n",
      "Epoch [54800/100000], Loss: 0.0000\n",
      "Epoch [54900/100000], Loss: 0.0000\n",
      "Epoch [55000/100000], Loss: 0.0000\n",
      "Epoch [55100/100000], Loss: 0.0000\n",
      "Epoch [55200/100000], Loss: 0.0000\n",
      "Epoch [55300/100000], Loss: 0.0000\n",
      "Epoch [55400/100000], Loss: 0.0000\n",
      "Epoch [55500/100000], Loss: 0.0000\n",
      "Epoch [55600/100000], Loss: 0.0000\n",
      "Epoch [55700/100000], Loss: 0.0000\n",
      "Epoch [55800/100000], Loss: 0.0000\n",
      "Epoch [55900/100000], Loss: 0.0000\n",
      "Epoch [56000/100000], Loss: 0.0000\n",
      "Epoch [56100/100000], Loss: 0.0000\n",
      "Epoch [56200/100000], Loss: 0.0000\n",
      "Epoch [56300/100000], Loss: 0.0000\n",
      "Epoch [56400/100000], Loss: 0.0000\n",
      "Epoch [56500/100000], Loss: 0.0000\n",
      "Epoch [56600/100000], Loss: 0.0000\n",
      "Epoch [56700/100000], Loss: 0.0000\n",
      "Epoch [56800/100000], Loss: 0.0000\n",
      "Epoch [56900/100000], Loss: 0.0000\n",
      "Epoch [57000/100000], Loss: 0.0000\n",
      "Epoch [57100/100000], Loss: 0.0000\n",
      "Epoch [57200/100000], Loss: 0.0000\n",
      "Epoch [57300/100000], Loss: 0.0000\n",
      "Epoch [57400/100000], Loss: 0.0000\n",
      "Epoch [57500/100000], Loss: 0.0000\n",
      "Epoch [57600/100000], Loss: 0.0000\n",
      "Epoch [57700/100000], Loss: 0.0000\n",
      "Epoch [57800/100000], Loss: 0.0000\n",
      "Epoch [57900/100000], Loss: 0.0000\n",
      "Epoch [58000/100000], Loss: 0.0000\n",
      "Epoch [58100/100000], Loss: 0.0000\n",
      "Epoch [58200/100000], Loss: 0.0000\n",
      "Epoch [58300/100000], Loss: 0.0000\n",
      "Epoch [58400/100000], Loss: 0.0000\n",
      "Epoch [58500/100000], Loss: 0.0000\n",
      "Epoch [58600/100000], Loss: 0.0000\n",
      "Epoch [58700/100000], Loss: 0.0000\n",
      "Epoch [58800/100000], Loss: 0.0000\n",
      "Epoch [58900/100000], Loss: 0.0000\n",
      "Epoch [59000/100000], Loss: 0.0000\n",
      "Epoch [59100/100000], Loss: 0.0000\n",
      "Epoch [59200/100000], Loss: 0.0000\n",
      "Epoch [59300/100000], Loss: 0.0000\n",
      "Epoch [59400/100000], Loss: 0.0000\n",
      "Epoch [59500/100000], Loss: 0.0000\n",
      "Epoch [59600/100000], Loss: 0.0000\n",
      "Epoch [59700/100000], Loss: 0.0000\n",
      "Epoch [59800/100000], Loss: 0.0000\n",
      "Epoch [59900/100000], Loss: 0.0000\n",
      "Epoch [60000/100000], Loss: 0.0000\n",
      "Epoch [60100/100000], Loss: 0.0000\n",
      "Epoch [60200/100000], Loss: 0.0000\n",
      "Epoch [60300/100000], Loss: 0.0000\n",
      "Epoch [60400/100000], Loss: 0.0000\n",
      "Epoch [60500/100000], Loss: 0.0000\n",
      "Epoch [60600/100000], Loss: 0.0000\n",
      "Epoch [60700/100000], Loss: 0.0000\n",
      "Epoch [60800/100000], Loss: 0.0000\n",
      "Epoch [60900/100000], Loss: 0.0000\n",
      "Epoch [61000/100000], Loss: 0.0000\n",
      "Epoch [61100/100000], Loss: 0.0000\n",
      "Epoch [61200/100000], Loss: 0.0000\n",
      "Epoch [61300/100000], Loss: 0.0000\n",
      "Epoch [61400/100000], Loss: 0.0000\n",
      "Epoch [61500/100000], Loss: 0.0000\n",
      "Epoch [61600/100000], Loss: 0.0000\n",
      "Epoch [61700/100000], Loss: 0.0000\n",
      "Epoch [61800/100000], Loss: 0.0000\n",
      "Epoch [61900/100000], Loss: 0.0000\n",
      "Epoch [62000/100000], Loss: 0.0000\n",
      "Epoch [62100/100000], Loss: 0.0000\n",
      "Epoch [62200/100000], Loss: 0.0000\n",
      "Epoch [62300/100000], Loss: 0.0000\n",
      "Epoch [62400/100000], Loss: 0.0000\n",
      "Epoch [62500/100000], Loss: 0.0000\n",
      "Epoch [62600/100000], Loss: 0.0000\n",
      "Epoch [62700/100000], Loss: 0.0000\n",
      "Epoch [62800/100000], Loss: 0.0000\n",
      "Epoch [62900/100000], Loss: 0.0000\n",
      "Epoch [63000/100000], Loss: 0.0000\n",
      "Epoch [63100/100000], Loss: 0.0000\n",
      "Epoch [63200/100000], Loss: 0.0000\n",
      "Epoch [63300/100000], Loss: 0.0000\n",
      "Epoch [63400/100000], Loss: 0.0000\n",
      "Epoch [63500/100000], Loss: 0.0000\n",
      "Epoch [63600/100000], Loss: 0.0000\n",
      "Epoch [63700/100000], Loss: 0.0000\n",
      "Epoch [63800/100000], Loss: 0.0000\n",
      "Epoch [63900/100000], Loss: 0.0000\n",
      "Epoch [64000/100000], Loss: 0.0000\n",
      "Epoch [64100/100000], Loss: 0.0000\n",
      "Epoch [64200/100000], Loss: 0.0000\n",
      "Epoch [64300/100000], Loss: 0.0000\n",
      "Epoch [64400/100000], Loss: 0.0000\n",
      "Epoch [64500/100000], Loss: 0.0000\n",
      "Epoch [64600/100000], Loss: 0.0000\n",
      "Epoch [64700/100000], Loss: 0.0000\n",
      "Epoch [64800/100000], Loss: 0.0000\n",
      "Epoch [64900/100000], Loss: 0.0000\n",
      "Epoch [65000/100000], Loss: 0.0000\n",
      "Epoch [65100/100000], Loss: 0.0000\n",
      "Epoch [65200/100000], Loss: 0.0000\n",
      "Epoch [65300/100000], Loss: 0.0000\n",
      "Epoch [65400/100000], Loss: 0.0000\n",
      "Epoch [65500/100000], Loss: 0.0000\n",
      "Epoch [65600/100000], Loss: 0.0000\n",
      "Epoch [65700/100000], Loss: 0.0000\n",
      "Epoch [65800/100000], Loss: 0.0000\n",
      "Epoch [65900/100000], Loss: 0.0000\n",
      "Epoch [66000/100000], Loss: 0.0000\n",
      "Epoch [66100/100000], Loss: 0.0000\n",
      "Epoch [66200/100000], Loss: 0.0000\n",
      "Epoch [66300/100000], Loss: 0.0000\n",
      "Epoch [66400/100000], Loss: 0.0000\n",
      "Epoch [66500/100000], Loss: 0.0000\n",
      "Epoch [66600/100000], Loss: 0.0000\n",
      "Epoch [66700/100000], Loss: 0.0000\n",
      "Epoch [66800/100000], Loss: 0.0000\n",
      "Epoch [66900/100000], Loss: 0.0000\n",
      "Epoch [67000/100000], Loss: 0.0000\n",
      "Epoch [67100/100000], Loss: 0.0000\n",
      "Epoch [67200/100000], Loss: 0.0000\n",
      "Epoch [67300/100000], Loss: 0.0000\n",
      "Epoch [67400/100000], Loss: 0.0000\n",
      "Epoch [67500/100000], Loss: 0.0000\n",
      "Epoch [67600/100000], Loss: 0.0000\n",
      "Epoch [67700/100000], Loss: 0.0000\n",
      "Epoch [67800/100000], Loss: 0.0000\n",
      "Epoch [67900/100000], Loss: 0.0000\n",
      "Epoch [68000/100000], Loss: 0.0000\n",
      "Epoch [68100/100000], Loss: 0.0000\n",
      "Epoch [68200/100000], Loss: 0.0000\n",
      "Epoch [68300/100000], Loss: 0.0000\n",
      "Epoch [68400/100000], Loss: 0.0000\n",
      "Epoch [68500/100000], Loss: 0.0000\n",
      "Epoch [68600/100000], Loss: 0.0000\n",
      "Epoch [68700/100000], Loss: 0.0000\n",
      "Epoch [68800/100000], Loss: 0.0000\n",
      "Epoch [68900/100000], Loss: 0.0000\n",
      "Epoch [69000/100000], Loss: 0.0000\n",
      "Epoch [69100/100000], Loss: 0.0000\n",
      "Epoch [69200/100000], Loss: 0.0000\n",
      "Epoch [69300/100000], Loss: 0.0000\n",
      "Epoch [69400/100000], Loss: 0.0000\n",
      "Epoch [69500/100000], Loss: 0.0000\n",
      "Epoch [69600/100000], Loss: 0.0000\n",
      "Epoch [69700/100000], Loss: 0.0000\n",
      "Epoch [69800/100000], Loss: 0.0000\n",
      "Epoch [69900/100000], Loss: 0.0000\n",
      "Epoch [70000/100000], Loss: 0.0000\n",
      "Epoch [70100/100000], Loss: 0.0000\n",
      "Epoch [70200/100000], Loss: 0.0000\n",
      "Epoch [70300/100000], Loss: 0.0000\n",
      "Epoch [70400/100000], Loss: 0.0000\n",
      "Epoch [70500/100000], Loss: 0.0000\n",
      "Epoch [70600/100000], Loss: 0.0000\n",
      "Epoch [70700/100000], Loss: 0.0000\n",
      "Epoch [70800/100000], Loss: 0.0000\n",
      "Epoch [70900/100000], Loss: 0.0000\n",
      "Epoch [71000/100000], Loss: 0.0000\n",
      "Epoch [71100/100000], Loss: 0.0000\n",
      "Epoch [71200/100000], Loss: 0.0000\n",
      "Epoch [71300/100000], Loss: 0.0000\n",
      "Epoch [71400/100000], Loss: 0.0000\n",
      "Epoch [71500/100000], Loss: 0.0000\n",
      "Epoch [71600/100000], Loss: 0.0000\n",
      "Epoch [71700/100000], Loss: 0.0000\n",
      "Epoch [71800/100000], Loss: 0.0000\n",
      "Epoch [71900/100000], Loss: 0.0000\n",
      "Epoch [72000/100000], Loss: 0.0000\n",
      "Epoch [72100/100000], Loss: 0.0000\n",
      "Epoch [72200/100000], Loss: 0.0000\n",
      "Epoch [72300/100000], Loss: 0.0000\n",
      "Epoch [72400/100000], Loss: 0.0000\n",
      "Epoch [72500/100000], Loss: 0.0000\n",
      "Epoch [72600/100000], Loss: 0.0000\n",
      "Epoch [72700/100000], Loss: 0.0000\n",
      "Epoch [72800/100000], Loss: 0.0000\n",
      "Epoch [72900/100000], Loss: 0.0000\n",
      "Epoch [73000/100000], Loss: 0.0000\n",
      "Epoch [73100/100000], Loss: 0.0000\n",
      "Epoch [73200/100000], Loss: 0.0000\n",
      "Epoch [73300/100000], Loss: 0.0000\n",
      "Epoch [73400/100000], Loss: 0.0000\n",
      "Epoch [73500/100000], Loss: 0.0000\n",
      "Epoch [73600/100000], Loss: 0.0000\n",
      "Epoch [73700/100000], Loss: 0.0000\n",
      "Epoch [73800/100000], Loss: 0.0000\n",
      "Epoch [73900/100000], Loss: 0.0000\n",
      "Epoch [74000/100000], Loss: 0.0000\n",
      "Epoch [74100/100000], Loss: 0.0000\n",
      "Epoch [74200/100000], Loss: 0.0000\n",
      "Epoch [74300/100000], Loss: 0.0000\n",
      "Epoch [74400/100000], Loss: 0.0000\n",
      "Epoch [74500/100000], Loss: 0.0000\n",
      "Epoch [74600/100000], Loss: 0.0000\n",
      "Epoch [74700/100000], Loss: 0.0000\n",
      "Epoch [74800/100000], Loss: 0.0000\n",
      "Epoch [74900/100000], Loss: 0.0000\n",
      "Epoch [75000/100000], Loss: 0.0000\n",
      "Epoch [75100/100000], Loss: 0.0000\n",
      "Epoch [75200/100000], Loss: 0.0000\n",
      "Epoch [75300/100000], Loss: 0.0000\n",
      "Epoch [75400/100000], Loss: 0.0000\n",
      "Epoch [75500/100000], Loss: 0.0000\n",
      "Epoch [75600/100000], Loss: 0.0000\n",
      "Epoch [75700/100000], Loss: 0.0000\n",
      "Epoch [75800/100000], Loss: 0.0000\n",
      "Epoch [75900/100000], Loss: 0.0000\n",
      "Epoch [76000/100000], Loss: 0.0000\n",
      "Epoch [76100/100000], Loss: 0.0000\n",
      "Epoch [76200/100000], Loss: 0.0000\n",
      "Epoch [76300/100000], Loss: 0.0000\n",
      "Epoch [76400/100000], Loss: 0.0000\n",
      "Epoch [76500/100000], Loss: 0.0000\n",
      "Epoch [76600/100000], Loss: 0.0000\n",
      "Epoch [76700/100000], Loss: 0.0000\n",
      "Epoch [76800/100000], Loss: 0.0000\n",
      "Epoch [76900/100000], Loss: 0.0000\n",
      "Epoch [77000/100000], Loss: 0.0000\n",
      "Epoch [77100/100000], Loss: 0.0000\n",
      "Epoch [77200/100000], Loss: 0.0000\n",
      "Epoch [77300/100000], Loss: 0.0000\n",
      "Epoch [77400/100000], Loss: 0.0000\n",
      "Epoch [77500/100000], Loss: 0.0000\n",
      "Epoch [77600/100000], Loss: 0.0000\n",
      "Epoch [77700/100000], Loss: 0.0000\n",
      "Epoch [77800/100000], Loss: 0.0000\n",
      "Epoch [77900/100000], Loss: 0.0000\n",
      "Epoch [78000/100000], Loss: 0.0000\n",
      "Epoch [78100/100000], Loss: 0.0000\n",
      "Epoch [78200/100000], Loss: 0.0000\n",
      "Epoch [78300/100000], Loss: 0.0000\n",
      "Epoch [78400/100000], Loss: 0.0000\n",
      "Epoch [78500/100000], Loss: 0.0000\n",
      "Epoch [78600/100000], Loss: 0.0000\n",
      "Epoch [78700/100000], Loss: 0.0000\n",
      "Epoch [78800/100000], Loss: 0.0000\n",
      "Epoch [78900/100000], Loss: 0.0000\n",
      "Epoch [79000/100000], Loss: 0.0000\n",
      "Epoch [79100/100000], Loss: 0.0000\n",
      "Epoch [79200/100000], Loss: 0.0000\n",
      "Epoch [79300/100000], Loss: 0.0000\n",
      "Epoch [79400/100000], Loss: 0.0000\n",
      "Epoch [79500/100000], Loss: 0.0000\n",
      "Epoch [79600/100000], Loss: 0.0000\n",
      "Epoch [79700/100000], Loss: 0.0000\n",
      "Epoch [79800/100000], Loss: 0.0000\n",
      "Epoch [79900/100000], Loss: 0.0000\n",
      "Epoch [80000/100000], Loss: 0.0000\n",
      "Epoch [80100/100000], Loss: 0.0000\n",
      "Epoch [80200/100000], Loss: 0.0000\n",
      "Epoch [80300/100000], Loss: 0.0000\n",
      "Epoch [80400/100000], Loss: 0.0000\n",
      "Epoch [80500/100000], Loss: 0.0000\n",
      "Epoch [80600/100000], Loss: 0.0000\n",
      "Epoch [80700/100000], Loss: 0.0000\n",
      "Epoch [80800/100000], Loss: 0.0000\n",
      "Epoch [80900/100000], Loss: 0.0000\n",
      "Epoch [81000/100000], Loss: 0.0000\n",
      "Epoch [81100/100000], Loss: 0.0000\n",
      "Epoch [81200/100000], Loss: 0.0000\n",
      "Epoch [81300/100000], Loss: 0.0000\n",
      "Epoch [81400/100000], Loss: 0.0000\n",
      "Epoch [81500/100000], Loss: 0.0000\n",
      "Epoch [81600/100000], Loss: 0.0000\n",
      "Epoch [81700/100000], Loss: 0.0000\n",
      "Epoch [81800/100000], Loss: 0.0000\n",
      "Epoch [81900/100000], Loss: 0.0000\n",
      "Epoch [82000/100000], Loss: 0.0000\n",
      "Epoch [82100/100000], Loss: 0.0000\n",
      "Epoch [82200/100000], Loss: 0.0000\n",
      "Epoch [82300/100000], Loss: 0.0000\n",
      "Epoch [82400/100000], Loss: 0.0000\n",
      "Epoch [82500/100000], Loss: 0.0000\n",
      "Epoch [82600/100000], Loss: 0.0000\n",
      "Epoch [82700/100000], Loss: 0.0000\n",
      "Epoch [82800/100000], Loss: 0.0000\n",
      "Epoch [82900/100000], Loss: 0.0000\n",
      "Epoch [83000/100000], Loss: 0.0000\n",
      "Epoch [83100/100000], Loss: 0.0000\n",
      "Epoch [83200/100000], Loss: 0.0000\n",
      "Epoch [83300/100000], Loss: 0.0000\n",
      "Epoch [83400/100000], Loss: 0.0000\n",
      "Epoch [83500/100000], Loss: 0.0000\n",
      "Epoch [83600/100000], Loss: 0.0000\n",
      "Epoch [83700/100000], Loss: 0.0000\n",
      "Epoch [83800/100000], Loss: 0.0000\n",
      "Epoch [83900/100000], Loss: 0.0000\n",
      "Epoch [84000/100000], Loss: 0.0000\n",
      "Epoch [84100/100000], Loss: 0.0000\n",
      "Epoch [84200/100000], Loss: 0.0000\n",
      "Epoch [84300/100000], Loss: 0.0000\n",
      "Epoch [84400/100000], Loss: 0.0000\n",
      "Epoch [84500/100000], Loss: 0.0000\n",
      "Epoch [84600/100000], Loss: 0.0000\n",
      "Epoch [84700/100000], Loss: 0.0000\n",
      "Epoch [84800/100000], Loss: 0.0000\n",
      "Epoch [84900/100000], Loss: 0.0000\n",
      "Epoch [85000/100000], Loss: 0.0000\n",
      "Epoch [85100/100000], Loss: 0.0000\n",
      "Epoch [85200/100000], Loss: 0.0000\n",
      "Epoch [85300/100000], Loss: 0.0000\n",
      "Epoch [85400/100000], Loss: 0.0000\n",
      "Epoch [85500/100000], Loss: 0.0000\n",
      "Epoch [85600/100000], Loss: 0.0000\n",
      "Epoch [85700/100000], Loss: 0.0000\n",
      "Epoch [85800/100000], Loss: 0.0000\n",
      "Epoch [85900/100000], Loss: 0.0000\n",
      "Epoch [86000/100000], Loss: 0.0000\n",
      "Epoch [86100/100000], Loss: 0.0000\n",
      "Epoch [86200/100000], Loss: 0.0000\n",
      "Epoch [86300/100000], Loss: 0.0000\n",
      "Epoch [86400/100000], Loss: 0.0000\n",
      "Epoch [86500/100000], Loss: 0.0000\n",
      "Epoch [86600/100000], Loss: 0.0000\n",
      "Epoch [86700/100000], Loss: 0.0000\n",
      "Epoch [86800/100000], Loss: 0.0000\n",
      "Epoch [86900/100000], Loss: 0.0000\n",
      "Epoch [87000/100000], Loss: 0.0000\n",
      "Epoch [87100/100000], Loss: 0.0000\n",
      "Epoch [87200/100000], Loss: 0.0000\n",
      "Epoch [87300/100000], Loss: 0.0000\n",
      "Epoch [87400/100000], Loss: 0.0000\n",
      "Epoch [87500/100000], Loss: 0.0000\n",
      "Epoch [87600/100000], Loss: 0.0000\n",
      "Epoch [87700/100000], Loss: 0.0000\n",
      "Epoch [87800/100000], Loss: 0.0000\n",
      "Epoch [87900/100000], Loss: 0.0000\n",
      "Epoch [88000/100000], Loss: 0.0000\n",
      "Epoch [88100/100000], Loss: 0.0000\n",
      "Epoch [88200/100000], Loss: 0.0000\n",
      "Epoch [88300/100000], Loss: 0.0000\n",
      "Epoch [88400/100000], Loss: 0.0000\n",
      "Epoch [88500/100000], Loss: 0.0000\n",
      "Epoch [88600/100000], Loss: 0.0000\n",
      "Epoch [88700/100000], Loss: 0.0000\n",
      "Epoch [88800/100000], Loss: 0.0000\n",
      "Epoch [88900/100000], Loss: 0.0000\n",
      "Epoch [89000/100000], Loss: 0.0000\n",
      "Epoch [89100/100000], Loss: 0.0000\n",
      "Epoch [89200/100000], Loss: 0.0000\n",
      "Epoch [89300/100000], Loss: 0.0000\n",
      "Epoch [89400/100000], Loss: 0.0000\n",
      "Epoch [89500/100000], Loss: 0.0000\n",
      "Epoch [89600/100000], Loss: 0.0000\n",
      "Epoch [89700/100000], Loss: 0.0000\n",
      "Epoch [89800/100000], Loss: 0.0000\n",
      "Epoch [89900/100000], Loss: 0.0000\n",
      "Epoch [90000/100000], Loss: 0.0000\n",
      "Epoch [90100/100000], Loss: 0.0000\n",
      "Epoch [90200/100000], Loss: 0.0000\n",
      "Epoch [90300/100000], Loss: 0.0000\n",
      "Epoch [90400/100000], Loss: 0.0000\n",
      "Epoch [90500/100000], Loss: 0.0000\n",
      "Epoch [90600/100000], Loss: 0.0000\n",
      "Epoch [90700/100000], Loss: 0.0000\n",
      "Epoch [90800/100000], Loss: 0.0000\n",
      "Epoch [90900/100000], Loss: 0.0000\n",
      "Epoch [91000/100000], Loss: 0.0000\n",
      "Epoch [91100/100000], Loss: 0.0000\n",
      "Epoch [91200/100000], Loss: 0.0000\n",
      "Epoch [91300/100000], Loss: 0.0000\n",
      "Epoch [91400/100000], Loss: 0.0000\n",
      "Epoch [91500/100000], Loss: 0.0000\n",
      "Epoch [91600/100000], Loss: 0.0000\n",
      "Epoch [91700/100000], Loss: 0.0000\n",
      "Epoch [91800/100000], Loss: 0.0000\n",
      "Epoch [91900/100000], Loss: 0.0000\n",
      "Epoch [92000/100000], Loss: 0.0000\n",
      "Epoch [92100/100000], Loss: 0.0000\n",
      "Epoch [92200/100000], Loss: 0.0000\n",
      "Epoch [92300/100000], Loss: 0.0000\n",
      "Epoch [92400/100000], Loss: 0.0000\n",
      "Epoch [92500/100000], Loss: 0.0000\n",
      "Epoch [92600/100000], Loss: 0.0000\n",
      "Epoch [92700/100000], Loss: 0.0000\n",
      "Epoch [92800/100000], Loss: 0.0000\n",
      "Epoch [92900/100000], Loss: 0.0000\n",
      "Epoch [93000/100000], Loss: 0.0000\n",
      "Epoch [93100/100000], Loss: 0.0000\n",
      "Epoch [93200/100000], Loss: 0.0000\n",
      "Epoch [93300/100000], Loss: 0.0000\n",
      "Epoch [93400/100000], Loss: 0.0000\n",
      "Epoch [93500/100000], Loss: 0.0000\n",
      "Epoch [93600/100000], Loss: 0.0000\n",
      "Epoch [93700/100000], Loss: 0.0000\n",
      "Epoch [93800/100000], Loss: 0.0000\n",
      "Epoch [93900/100000], Loss: 0.0000\n",
      "Epoch [94000/100000], Loss: 0.0000\n",
      "Epoch [94100/100000], Loss: 0.0000\n",
      "Epoch [94200/100000], Loss: 0.0000\n",
      "Epoch [94300/100000], Loss: 0.0000\n",
      "Epoch [94400/100000], Loss: 0.0000\n",
      "Epoch [94500/100000], Loss: 0.0000\n",
      "Epoch [94600/100000], Loss: 0.0000\n",
      "Epoch [94700/100000], Loss: 0.0000\n",
      "Epoch [94800/100000], Loss: 0.0000\n",
      "Epoch [94900/100000], Loss: 0.0000\n",
      "Epoch [95000/100000], Loss: 0.0000\n",
      "Epoch [95100/100000], Loss: 0.0000\n",
      "Epoch [95200/100000], Loss: 0.0000\n",
      "Epoch [95300/100000], Loss: 0.0000\n",
      "Epoch [95400/100000], Loss: 0.0000\n",
      "Epoch [95500/100000], Loss: 0.0000\n",
      "Epoch [95600/100000], Loss: 0.0000\n",
      "Epoch [95700/100000], Loss: 0.0000\n",
      "Epoch [95800/100000], Loss: 0.0000\n",
      "Epoch [95900/100000], Loss: 0.0000\n",
      "Epoch [96000/100000], Loss: 0.0000\n",
      "Epoch [96100/100000], Loss: 0.0000\n",
      "Epoch [96200/100000], Loss: 0.0000\n",
      "Epoch [96300/100000], Loss: 0.0000\n",
      "Epoch [96400/100000], Loss: 0.0000\n",
      "Epoch [96500/100000], Loss: 0.0000\n",
      "Epoch [96600/100000], Loss: 0.0000\n",
      "Epoch [96700/100000], Loss: 0.0000\n",
      "Epoch [96800/100000], Loss: 0.0000\n",
      "Epoch [96900/100000], Loss: 0.0000\n",
      "Epoch [97000/100000], Loss: 0.0000\n",
      "Epoch [97100/100000], Loss: 0.0000\n",
      "Epoch [97200/100000], Loss: 0.0000\n",
      "Epoch [97300/100000], Loss: 0.0000\n",
      "Epoch [97400/100000], Loss: 0.0000\n",
      "Epoch [97500/100000], Loss: 0.0000\n",
      "Epoch [97600/100000], Loss: 0.0000\n",
      "Epoch [97700/100000], Loss: 0.0000\n",
      "Epoch [97800/100000], Loss: 0.0000\n",
      "Epoch [97900/100000], Loss: 0.0000\n",
      "Epoch [98000/100000], Loss: 0.0000\n",
      "Epoch [98100/100000], Loss: 0.0000\n",
      "Epoch [98200/100000], Loss: 0.0000\n",
      "Epoch [98300/100000], Loss: 0.0000\n",
      "Epoch [98400/100000], Loss: 0.0000\n",
      "Epoch [98500/100000], Loss: 0.0000\n",
      "Epoch [98600/100000], Loss: 0.0000\n",
      "Epoch [98700/100000], Loss: 0.0000\n",
      "Epoch [98800/100000], Loss: 0.0000\n",
      "Epoch [98900/100000], Loss: 0.0000\n",
      "Epoch [99000/100000], Loss: 0.0000\n",
      "Epoch [99100/100000], Loss: 0.0000\n",
      "Epoch [99200/100000], Loss: 0.0000\n",
      "Epoch [99300/100000], Loss: 0.0000\n",
      "Epoch [99400/100000], Loss: 0.0000\n",
      "Epoch [99500/100000], Loss: 0.0000\n",
      "Epoch [99600/100000], Loss: 0.0000\n",
      "Epoch [99700/100000], Loss: 0.0000\n",
      "Epoch [99800/100000], Loss: 0.0000\n",
      "Epoch [99900/100000], Loss: 0.0000\n",
      "Ma trận B đã học sau khi huấn luyện:\n",
      "[[1.       2.      ]\n",
      " [2.999992 4.      ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Định nghĩa ma trận A (không thay đổi)\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n",
    "\n",
    "# Định nghĩa ma trận B, khởi tạo bằng phương pháp Xavier\n",
    "class XavierMatrix(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(XavierMatrix, self).__init__()\n",
    "        self.B = nn.Parameter(torch.empty(size))  # Ma trận B là tham số học\n",
    "        nn.init.normal_(self.B, mean=0, std=1)  # Khởi tạo với phân phối chuẩn\n",
    "\n",
    "    def forward(self):\n",
    "        return self.B\n",
    "\n",
    "# Khoảng cách Frobenius giữa 2 ma trận\n",
    "def frobenius_distance(A, B):\n",
    "    return torch.norm(A - B, p='fro')  # Frobenius norm\n",
    "\n",
    "# Định nghĩa mô hình\n",
    "class WassersteinModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WassersteinModel, self).__init__()\n",
    "        self.xavier_matrix = XavierMatrix(size=(2, 2))  # Khởi tạo ma trận B (cùng kích thước với A)\n",
    "\n",
    "    def forward(self):\n",
    "        B = self.xavier_matrix()\n",
    "        return frobenius_distance(A, B)\n",
    "\n",
    "# Tạo mô hình\n",
    "model = WassersteinModel()\n",
    "\n",
    "# Định nghĩa hàm tối ưu (Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Giảm learning rate\n",
    "\n",
    "# Tiến hành tối ưu\n",
    "num_epochs = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    # Tiến hành cập nhật các tham số của ma trận B\n",
    "    optimizer.zero_grad()\n",
    "    loss = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # In thông tin loss sau mỗi 100 epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Sau khi huấn luyện, lấy ma trận B đã học\n",
    "B_learned = model.xavier_matrix.B.detach().numpy()  # Dùng .detach() để tránh tính toán gradient\n",
    "print(\"Ma trận B đã học sau khi huấn luyện:\")\n",
    "print(B_learned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_learned = model.xavier_matrix.B.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.35883 ,  9.771715],\n",
       "       [11.24564 ,  9.942004]], dtype=float32)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.1181)"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def\n",
    "def loss_test(test_y_pred):\n",
    "    return -torch.log(test_y_pred).sum()/len(test_y_pred)\n",
    "\n",
    "y_pred = torch.tensor([1,1e-4,1e-3])\n",
    "- torch.log(y_pred).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_TI(p_wi, true_y):\n",
    "    loss_TI = 0.0\n",
    "    for i in range(len(true_y)):\n",
    "        # mask padding token\n",
    "        loss_TI += -torch.dot(true_y[i].float(), torch.log(p_wi[i])) - torch.dot(\n",
    "            (1 - true_y[i].float()), torch.log(1 - p_wi[i])\n",
    "        )\n",
    "\n",
    "    return loss_TI / len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wi[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0873)"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_TI(p_wi[0].unsqueeze(1), true_y[0].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_Task(pi_star, y_true):\n",
    "    # print('----in compute_loss_task---')\n",
    "    # print(f\"Pi_star requires_grad: {pi_star[0].requires_grad}\")\n",
    "    # print(f\"Y_true requires_grad: {y_true[0].requires_grad}\")\n",
    "    \n",
    "    loss_Task = 0.0\n",
    "    batch_size = len(pi_star)\n",
    "    for i in range(batch_size):\n",
    "        sentence_loss = -torch.log(\n",
    "            pi_star[i].gather(1, y_true[i].unsqueeze(1))\n",
    "        ).sum() / len(pi_star[i])\n",
    "        loss_Task += sentence_loss\n",
    "    return loss_Task / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([[1,3,3],[0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 3],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_star=torch.softmax(pi_star,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0358)"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_Task(pi_star,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2141, 0.3775, 0.2026, 0.2058],\n",
       "         [0.2708, 0.1549, 0.2592, 0.3151],\n",
       "         [0.2085, 0.2324, 0.1675, 0.3916]],\n",
       "\n",
       "        [[0.3726, 0.2001, 0.2512, 0.1761],\n",
       "         [0.2238, 0.3398, 0.1926, 0.2439],\n",
       "         [0.3391, 0.1758, 0.1359, 0.3492]]])"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7318],\n",
       "        [0.8467],\n",
       "        [0.9114]])"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[0].gather(1,y_true[0].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_star.requires_grad_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 7.4992\n",
      "Epoch [2/10000], Loss: 4.0422\n",
      "Epoch [3/10000], Loss: 3.7831\n",
      "Epoch [4/10000], Loss: 3.6453\n",
      "Epoch [5/10000], Loss: 3.5541\n",
      "Epoch [6/10000], Loss: 3.4871\n",
      "Epoch [7/10000], Loss: 3.4350\n",
      "Epoch [8/10000], Loss: 3.3929\n",
      "Epoch [9/10000], Loss: 3.3579\n",
      "Epoch [10/10000], Loss: 3.3283\n",
      "Epoch [11/10000], Loss: 3.3028\n",
      "Epoch [12/10000], Loss: 3.2805\n",
      "Epoch [13/10000], Loss: 3.2610\n",
      "Epoch [14/10000], Loss: 3.2436\n",
      "Epoch [15/10000], Loss: 3.2281\n",
      "Epoch [16/10000], Loss: 3.2142\n",
      "Epoch [17/10000], Loss: 3.2016\n",
      "Epoch [18/10000], Loss: 3.1901\n",
      "Epoch [19/10000], Loss: 3.1797\n",
      "Epoch [20/10000], Loss: 3.1701\n",
      "Epoch [21/10000], Loss: 3.1614\n",
      "Epoch [22/10000], Loss: 3.1533\n",
      "Epoch [23/10000], Loss: 3.1458\n",
      "Epoch [24/10000], Loss: 3.1389\n",
      "Epoch [25/10000], Loss: 3.1324\n",
      "Epoch [26/10000], Loss: 3.1264\n",
      "Epoch [27/10000], Loss: 3.1208\n",
      "Epoch [28/10000], Loss: 3.1156\n",
      "Epoch [29/10000], Loss: 3.1107\n",
      "Epoch [30/10000], Loss: 3.1060\n",
      "Epoch [31/10000], Loss: 3.1017\n",
      "Epoch [32/10000], Loss: 3.0975\n",
      "Epoch [33/10000], Loss: 3.0936\n",
      "Epoch [34/10000], Loss: 3.0899\n",
      "Epoch [35/10000], Loss: 3.0864\n",
      "Epoch [36/10000], Loss: 3.0831\n",
      "Epoch [37/10000], Loss: 3.0798\n",
      "Epoch [38/10000], Loss: 3.0768\n",
      "Epoch [39/10000], Loss: 3.0739\n",
      "Epoch [40/10000], Loss: 3.0710\n",
      "Epoch [41/10000], Loss: 3.0683\n",
      "Epoch [42/10000], Loss: 3.0657\n",
      "Epoch [43/10000], Loss: 3.0632\n",
      "Epoch [44/10000], Loss: 3.0608\n",
      "Epoch [45/10000], Loss: 3.0584\n",
      "Epoch [46/10000], Loss: 3.0561\n",
      "Epoch [47/10000], Loss: 3.0539\n",
      "Epoch [48/10000], Loss: 3.0517\n",
      "Epoch [49/10000], Loss: 3.0496\n",
      "Epoch [50/10000], Loss: 3.0476\n",
      "Epoch [51/10000], Loss: 3.0456\n",
      "Epoch [52/10000], Loss: 3.0436\n",
      "Epoch [53/10000], Loss: 3.0417\n",
      "Epoch [54/10000], Loss: 3.0399\n",
      "Epoch [55/10000], Loss: 3.0380\n",
      "Epoch [56/10000], Loss: 3.0362\n",
      "Epoch [57/10000], Loss: 3.0345\n",
      "Epoch [58/10000], Loss: 3.0327\n",
      "Epoch [59/10000], Loss: 3.0310\n",
      "Epoch [60/10000], Loss: 3.0293\n",
      "Epoch [61/10000], Loss: 3.0277\n",
      "Epoch [62/10000], Loss: 3.0261\n",
      "Epoch [63/10000], Loss: 3.0244\n",
      "Epoch [64/10000], Loss: 3.0229\n",
      "Epoch [65/10000], Loss: 3.0213\n",
      "Epoch [66/10000], Loss: 3.0197\n",
      "Epoch [67/10000], Loss: 3.0182\n",
      "Epoch [68/10000], Loss: 3.0167\n",
      "Epoch [69/10000], Loss: 3.0152\n",
      "Epoch [70/10000], Loss: 3.0137\n",
      "Epoch [71/10000], Loss: 3.0122\n",
      "Epoch [72/10000], Loss: 3.0108\n",
      "Epoch [73/10000], Loss: 3.0093\n",
      "Epoch [74/10000], Loss: 3.0079\n",
      "Epoch [75/10000], Loss: 3.0065\n",
      "Epoch [76/10000], Loss: 3.0051\n",
      "Epoch [77/10000], Loss: 3.0037\n",
      "Epoch [78/10000], Loss: 3.0023\n",
      "Epoch [79/10000], Loss: 3.0009\n",
      "Epoch [80/10000], Loss: 2.9996\n",
      "Epoch [81/10000], Loss: 2.9982\n",
      "Epoch [82/10000], Loss: 2.9969\n",
      "Epoch [83/10000], Loss: 2.9956\n",
      "Epoch [84/10000], Loss: 2.9942\n",
      "Epoch [85/10000], Loss: 2.9929\n",
      "Epoch [86/10000], Loss: 2.9916\n",
      "Epoch [87/10000], Loss: 2.9903\n",
      "Epoch [88/10000], Loss: 2.9890\n",
      "Epoch [89/10000], Loss: 2.9878\n",
      "Epoch [90/10000], Loss: 2.9865\n",
      "Epoch [91/10000], Loss: 2.9852\n",
      "Epoch [92/10000], Loss: 2.9840\n",
      "Epoch [93/10000], Loss: 2.9827\n",
      "Epoch [94/10000], Loss: 2.9815\n",
      "Epoch [95/10000], Loss: 2.9803\n",
      "Epoch [96/10000], Loss: 2.9790\n",
      "Epoch [97/10000], Loss: 2.9778\n",
      "Epoch [98/10000], Loss: 2.9766\n",
      "Epoch [99/10000], Loss: 2.9754\n",
      "Epoch [100/10000], Loss: 2.9742\n",
      "Epoch [101/10000], Loss: 2.9730\n",
      "Epoch [102/10000], Loss: 2.9718\n",
      "Epoch [103/10000], Loss: 2.9706\n",
      "Epoch [104/10000], Loss: 2.9695\n",
      "Epoch [105/10000], Loss: 2.9683\n",
      "Epoch [106/10000], Loss: 2.9671\n",
      "Epoch [107/10000], Loss: 2.9660\n",
      "Epoch [108/10000], Loss: 2.9648\n",
      "Epoch [109/10000], Loss: 2.9637\n",
      "Epoch [110/10000], Loss: 2.9625\n",
      "Epoch [111/10000], Loss: 2.9614\n",
      "Epoch [112/10000], Loss: 2.9603\n",
      "Epoch [113/10000], Loss: 2.9591\n",
      "Epoch [114/10000], Loss: 2.9580\n",
      "Epoch [115/10000], Loss: 2.9569\n",
      "Epoch [116/10000], Loss: 2.9558\n",
      "Epoch [117/10000], Loss: 2.9547\n",
      "Epoch [118/10000], Loss: 2.9536\n",
      "Epoch [119/10000], Loss: 2.9525\n",
      "Epoch [120/10000], Loss: 2.9514\n",
      "Epoch [121/10000], Loss: 2.9503\n",
      "Epoch [122/10000], Loss: 2.9492\n",
      "Epoch [123/10000], Loss: 2.9481\n",
      "Epoch [124/10000], Loss: 2.9471\n",
      "Epoch [125/10000], Loss: 2.9460\n",
      "Epoch [126/10000], Loss: 2.9449\n",
      "Epoch [127/10000], Loss: 2.9439\n",
      "Epoch [128/10000], Loss: 2.9428\n",
      "Epoch [129/10000], Loss: 2.9418\n",
      "Epoch [130/10000], Loss: 2.9407\n",
      "Epoch [131/10000], Loss: 2.9397\n",
      "Epoch [132/10000], Loss: 2.9386\n",
      "Epoch [133/10000], Loss: 2.9376\n",
      "Epoch [134/10000], Loss: 2.9366\n",
      "Epoch [135/10000], Loss: 2.9356\n",
      "Epoch [136/10000], Loss: 2.9345\n",
      "Epoch [137/10000], Loss: 2.9335\n",
      "Epoch [138/10000], Loss: 2.9325\n",
      "Epoch [139/10000], Loss: 2.9315\n",
      "Epoch [140/10000], Loss: 2.9305\n",
      "Epoch [141/10000], Loss: 2.9295\n",
      "Epoch [142/10000], Loss: 2.9285\n",
      "Epoch [143/10000], Loss: 2.9275\n",
      "Epoch [144/10000], Loss: 2.9265\n",
      "Epoch [145/10000], Loss: 2.9255\n",
      "Epoch [146/10000], Loss: 2.9245\n",
      "Epoch [147/10000], Loss: 2.9235\n",
      "Epoch [148/10000], Loss: 2.9225\n",
      "Epoch [149/10000], Loss: 2.9216\n",
      "Epoch [150/10000], Loss: 2.9206\n",
      "Epoch [151/10000], Loss: 2.9196\n",
      "Epoch [152/10000], Loss: 2.9187\n",
      "Epoch [153/10000], Loss: 2.9177\n",
      "Epoch [154/10000], Loss: 2.9167\n",
      "Epoch [155/10000], Loss: 2.9158\n",
      "Epoch [156/10000], Loss: 2.9148\n",
      "Epoch [157/10000], Loss: 2.9139\n",
      "Epoch [158/10000], Loss: 2.9129\n",
      "Epoch [159/10000], Loss: 2.9120\n",
      "Epoch [160/10000], Loss: 2.9111\n",
      "Epoch [161/10000], Loss: 2.9101\n",
      "Epoch [162/10000], Loss: 2.9092\n",
      "Epoch [163/10000], Loss: 2.9082\n",
      "Epoch [164/10000], Loss: 2.9073\n",
      "Epoch [165/10000], Loss: 2.9064\n",
      "Epoch [166/10000], Loss: 2.9055\n",
      "Epoch [167/10000], Loss: 2.9045\n",
      "Epoch [168/10000], Loss: 2.9036\n",
      "Epoch [169/10000], Loss: 2.9027\n",
      "Epoch [170/10000], Loss: 2.9018\n",
      "Epoch [171/10000], Loss: 2.9009\n",
      "Epoch [172/10000], Loss: 2.9000\n",
      "Epoch [173/10000], Loss: 2.8991\n",
      "Epoch [174/10000], Loss: 2.8982\n",
      "Epoch [175/10000], Loss: 2.8973\n",
      "Epoch [176/10000], Loss: 2.8964\n",
      "Epoch [177/10000], Loss: 2.8955\n",
      "Epoch [178/10000], Loss: 2.8946\n",
      "Epoch [179/10000], Loss: 2.8937\n",
      "Epoch [180/10000], Loss: 2.8928\n",
      "Epoch [181/10000], Loss: 2.8919\n",
      "Epoch [182/10000], Loss: 2.8910\n",
      "Epoch [183/10000], Loss: 2.8902\n",
      "Epoch [184/10000], Loss: 2.8893\n",
      "Epoch [185/10000], Loss: 2.8884\n",
      "Epoch [186/10000], Loss: 2.8875\n",
      "Epoch [187/10000], Loss: 2.8867\n",
      "Epoch [188/10000], Loss: 2.8858\n",
      "Epoch [189/10000], Loss: 2.8849\n",
      "Epoch [190/10000], Loss: 2.8841\n",
      "Epoch [191/10000], Loss: 2.8832\n",
      "Epoch [192/10000], Loss: 2.8824\n",
      "Epoch [193/10000], Loss: 2.8815\n",
      "Epoch [194/10000], Loss: 2.8806\n",
      "Epoch [195/10000], Loss: 2.8798\n",
      "Epoch [196/10000], Loss: 2.8789\n",
      "Epoch [197/10000], Loss: 2.8781\n",
      "Epoch [198/10000], Loss: 2.8773\n",
      "Epoch [199/10000], Loss: 2.8764\n",
      "Epoch [200/10000], Loss: 2.8756\n",
      "Epoch [201/10000], Loss: 2.8747\n",
      "Epoch [202/10000], Loss: 2.8739\n",
      "Epoch [203/10000], Loss: 2.8731\n",
      "Epoch [204/10000], Loss: 2.8722\n",
      "Epoch [205/10000], Loss: 2.8714\n",
      "Epoch [206/10000], Loss: 2.8706\n",
      "Epoch [207/10000], Loss: 2.8697\n",
      "Epoch [208/10000], Loss: 2.8689\n",
      "Epoch [209/10000], Loss: 2.8681\n",
      "Epoch [210/10000], Loss: 2.8673\n",
      "Epoch [211/10000], Loss: 2.8664\n",
      "Epoch [212/10000], Loss: 2.8656\n",
      "Epoch [213/10000], Loss: 2.8648\n",
      "Epoch [214/10000], Loss: 2.8640\n",
      "Epoch [215/10000], Loss: 2.8632\n",
      "Epoch [216/10000], Loss: 2.8624\n",
      "Epoch [217/10000], Loss: 2.8616\n",
      "Epoch [218/10000], Loss: 2.8608\n",
      "Epoch [219/10000], Loss: 2.8599\n",
      "Epoch [220/10000], Loss: 2.8591\n",
      "Epoch [221/10000], Loss: 2.8583\n",
      "Epoch [222/10000], Loss: 2.8575\n",
      "Epoch [223/10000], Loss: 2.8567\n",
      "Epoch [224/10000], Loss: 2.8559\n",
      "Epoch [225/10000], Loss: 2.8552\n",
      "Epoch [226/10000], Loss: 2.8544\n",
      "Epoch [227/10000], Loss: 2.8536\n",
      "Epoch [228/10000], Loss: 2.8528\n",
      "Epoch [229/10000], Loss: 2.8520\n",
      "Epoch [230/10000], Loss: 2.8512\n",
      "Epoch [231/10000], Loss: 2.8504\n",
      "Epoch [232/10000], Loss: 2.8496\n",
      "Epoch [233/10000], Loss: 2.8489\n",
      "Epoch [234/10000], Loss: 2.8481\n",
      "Epoch [235/10000], Loss: 2.8473\n",
      "Epoch [236/10000], Loss: 2.8465\n",
      "Epoch [237/10000], Loss: 2.8457\n",
      "Epoch [238/10000], Loss: 2.8450\n",
      "Epoch [239/10000], Loss: 2.8442\n",
      "Epoch [240/10000], Loss: 2.8434\n",
      "Epoch [241/10000], Loss: 2.8427\n",
      "Epoch [242/10000], Loss: 2.8419\n",
      "Epoch [243/10000], Loss: 2.8411\n",
      "Epoch [244/10000], Loss: 2.8404\n",
      "Epoch [245/10000], Loss: 2.8396\n",
      "Epoch [246/10000], Loss: 2.8388\n",
      "Epoch [247/10000], Loss: 2.8381\n",
      "Epoch [248/10000], Loss: 2.8373\n",
      "Epoch [249/10000], Loss: 2.8366\n",
      "Epoch [250/10000], Loss: 2.8358\n",
      "Epoch [251/10000], Loss: 2.8350\n",
      "Epoch [252/10000], Loss: 2.8343\n",
      "Epoch [253/10000], Loss: 2.8335\n",
      "Epoch [254/10000], Loss: 2.8328\n",
      "Epoch [255/10000], Loss: 2.8320\n",
      "Epoch [256/10000], Loss: 2.8313\n",
      "Epoch [257/10000], Loss: 2.8305\n",
      "Epoch [258/10000], Loss: 2.8298\n",
      "Epoch [259/10000], Loss: 2.8291\n",
      "Epoch [260/10000], Loss: 2.8283\n",
      "Epoch [261/10000], Loss: 2.8276\n",
      "Epoch [262/10000], Loss: 2.8268\n",
      "Epoch [263/10000], Loss: 2.8261\n",
      "Epoch [264/10000], Loss: 2.8254\n",
      "Epoch [265/10000], Loss: 2.8246\n",
      "Epoch [266/10000], Loss: 2.8239\n",
      "Epoch [267/10000], Loss: 2.8232\n",
      "Epoch [268/10000], Loss: 2.8224\n",
      "Epoch [269/10000], Loss: 2.8217\n",
      "Epoch [270/10000], Loss: 2.8210\n",
      "Epoch [271/10000], Loss: 2.8202\n",
      "Epoch [272/10000], Loss: 2.8195\n",
      "Epoch [273/10000], Loss: 2.8188\n",
      "Epoch [274/10000], Loss: 2.8181\n",
      "Epoch [275/10000], Loss: 2.8173\n",
      "Epoch [276/10000], Loss: 2.8166\n",
      "Epoch [277/10000], Loss: 2.8159\n",
      "Epoch [278/10000], Loss: 2.8152\n",
      "Epoch [279/10000], Loss: 2.8144\n",
      "Epoch [280/10000], Loss: 2.8137\n",
      "Epoch [281/10000], Loss: 2.8130\n",
      "Epoch [282/10000], Loss: 2.8123\n",
      "Epoch [283/10000], Loss: 2.8116\n",
      "Epoch [284/10000], Loss: 2.8109\n",
      "Epoch [285/10000], Loss: 2.8102\n",
      "Epoch [286/10000], Loss: 2.8094\n",
      "Epoch [287/10000], Loss: 2.8087\n",
      "Epoch [288/10000], Loss: 2.8080\n",
      "Epoch [289/10000], Loss: 2.8073\n",
      "Epoch [290/10000], Loss: 2.8066\n",
      "Epoch [291/10000], Loss: 2.8059\n",
      "Epoch [292/10000], Loss: 2.8052\n",
      "Epoch [293/10000], Loss: 2.8045\n",
      "Epoch [294/10000], Loss: 2.8038\n",
      "Epoch [295/10000], Loss: 2.8031\n",
      "Epoch [296/10000], Loss: 2.8024\n",
      "Epoch [297/10000], Loss: 2.8017\n",
      "Epoch [298/10000], Loss: 2.8010\n",
      "Epoch [299/10000], Loss: 2.8003\n",
      "Epoch [300/10000], Loss: 2.7996\n",
      "Epoch [301/10000], Loss: 2.7989\n",
      "Epoch [302/10000], Loss: 2.7982\n",
      "Epoch [303/10000], Loss: 2.7975\n",
      "Epoch [304/10000], Loss: 2.7968\n",
      "Epoch [305/10000], Loss: 2.7961\n",
      "Epoch [306/10000], Loss: 2.7955\n",
      "Epoch [307/10000], Loss: 2.7948\n",
      "Epoch [308/10000], Loss: 2.7941\n",
      "Epoch [309/10000], Loss: 2.7934\n",
      "Epoch [310/10000], Loss: 2.7927\n",
      "Epoch [311/10000], Loss: 2.7920\n",
      "Epoch [312/10000], Loss: 2.7913\n",
      "Epoch [313/10000], Loss: 2.7907\n",
      "Epoch [314/10000], Loss: 2.7900\n",
      "Epoch [315/10000], Loss: 2.7893\n",
      "Epoch [316/10000], Loss: 2.7886\n",
      "Epoch [317/10000], Loss: 2.7879\n",
      "Epoch [318/10000], Loss: 2.7873\n",
      "Epoch [319/10000], Loss: 2.7866\n",
      "Epoch [320/10000], Loss: 2.7859\n",
      "Epoch [321/10000], Loss: 2.7852\n",
      "Epoch [322/10000], Loss: 2.7846\n",
      "Epoch [323/10000], Loss: 2.7839\n",
      "Epoch [324/10000], Loss: 2.7832\n",
      "Epoch [325/10000], Loss: 2.7825\n",
      "Epoch [326/10000], Loss: 2.7819\n",
      "Epoch [327/10000], Loss: 2.7812\n",
      "Epoch [328/10000], Loss: 2.7805\n",
      "Epoch [329/10000], Loss: 2.7799\n",
      "Epoch [330/10000], Loss: 2.7792\n",
      "Epoch [331/10000], Loss: 2.7785\n",
      "Epoch [332/10000], Loss: 2.7779\n",
      "Epoch [333/10000], Loss: 2.7772\n",
      "Epoch [334/10000], Loss: 2.7765\n",
      "Epoch [335/10000], Loss: 2.7759\n",
      "Epoch [336/10000], Loss: 2.7752\n",
      "Epoch [337/10000], Loss: 2.7746\n",
      "Epoch [338/10000], Loss: 2.7739\n",
      "Epoch [339/10000], Loss: 2.7732\n",
      "Epoch [340/10000], Loss: 2.7726\n",
      "Epoch [341/10000], Loss: 2.7719\n",
      "Epoch [342/10000], Loss: 2.7713\n",
      "Epoch [343/10000], Loss: 2.7706\n",
      "Epoch [344/10000], Loss: 2.7699\n",
      "Epoch [345/10000], Loss: 2.7693\n",
      "Epoch [346/10000], Loss: 2.7686\n",
      "Epoch [347/10000], Loss: 2.7680\n",
      "Epoch [348/10000], Loss: 2.7673\n",
      "Epoch [349/10000], Loss: 2.7667\n",
      "Epoch [350/10000], Loss: 2.7660\n",
      "Epoch [351/10000], Loss: 2.7654\n",
      "Epoch [352/10000], Loss: 2.7647\n",
      "Epoch [353/10000], Loss: 2.7641\n",
      "Epoch [354/10000], Loss: 2.7634\n",
      "Epoch [355/10000], Loss: 2.7628\n",
      "Epoch [356/10000], Loss: 2.7621\n",
      "Epoch [357/10000], Loss: 2.7615\n",
      "Epoch [358/10000], Loss: 2.7609\n",
      "Epoch [359/10000], Loss: 2.7602\n",
      "Epoch [360/10000], Loss: 2.7596\n",
      "Epoch [361/10000], Loss: 2.7589\n",
      "Epoch [362/10000], Loss: 2.7583\n",
      "Epoch [363/10000], Loss: 2.7577\n",
      "Epoch [364/10000], Loss: 2.7570\n",
      "Epoch [365/10000], Loss: 2.7564\n",
      "Epoch [366/10000], Loss: 2.7557\n",
      "Epoch [367/10000], Loss: 2.7551\n",
      "Epoch [368/10000], Loss: 2.7545\n",
      "Epoch [369/10000], Loss: 2.7538\n",
      "Epoch [370/10000], Loss: 2.7532\n",
      "Epoch [371/10000], Loss: 2.7526\n",
      "Epoch [372/10000], Loss: 2.7519\n",
      "Epoch [373/10000], Loss: 2.7513\n",
      "Epoch [374/10000], Loss: 2.7507\n",
      "Epoch [375/10000], Loss: 2.7500\n",
      "Epoch [376/10000], Loss: 2.7494\n",
      "Epoch [377/10000], Loss: 2.7488\n",
      "Epoch [378/10000], Loss: 2.7481\n",
      "Epoch [379/10000], Loss: 2.7475\n",
      "Epoch [380/10000], Loss: 2.7469\n",
      "Epoch [381/10000], Loss: 2.7463\n",
      "Epoch [382/10000], Loss: 2.7456\n",
      "Epoch [383/10000], Loss: 2.7450\n",
      "Epoch [384/10000], Loss: 2.7444\n",
      "Epoch [385/10000], Loss: 2.7438\n",
      "Epoch [386/10000], Loss: 2.7431\n",
      "Epoch [387/10000], Loss: 2.7425\n",
      "Epoch [388/10000], Loss: 2.7419\n",
      "Epoch [389/10000], Loss: 2.7413\n",
      "Epoch [390/10000], Loss: 2.7406\n",
      "Epoch [391/10000], Loss: 2.7400\n",
      "Epoch [392/10000], Loss: 2.7394\n",
      "Epoch [393/10000], Loss: 2.7388\n",
      "Epoch [394/10000], Loss: 2.7382\n",
      "Epoch [395/10000], Loss: 2.7375\n",
      "Epoch [396/10000], Loss: 2.7369\n",
      "Epoch [397/10000], Loss: 2.7363\n",
      "Epoch [398/10000], Loss: 2.7357\n",
      "Epoch [399/10000], Loss: 2.7351\n",
      "Epoch [400/10000], Loss: 2.7345\n",
      "Epoch [401/10000], Loss: 2.7338\n",
      "Epoch [402/10000], Loss: 2.7332\n",
      "Epoch [403/10000], Loss: 2.7326\n",
      "Epoch [404/10000], Loss: 2.7320\n",
      "Epoch [405/10000], Loss: 2.7314\n",
      "Epoch [406/10000], Loss: 2.7308\n",
      "Epoch [407/10000], Loss: 2.7302\n",
      "Epoch [408/10000], Loss: 2.7296\n",
      "Epoch [409/10000], Loss: 2.7290\n",
      "Epoch [410/10000], Loss: 2.7283\n",
      "Epoch [411/10000], Loss: 2.7277\n",
      "Epoch [412/10000], Loss: 2.7271\n",
      "Epoch [413/10000], Loss: 2.7265\n",
      "Epoch [414/10000], Loss: 2.7259\n",
      "Epoch [415/10000], Loss: 2.7253\n",
      "Epoch [416/10000], Loss: 2.7247\n",
      "Epoch [417/10000], Loss: 2.7241\n",
      "Epoch [418/10000], Loss: 2.7235\n",
      "Epoch [419/10000], Loss: 2.7229\n",
      "Epoch [420/10000], Loss: 2.7223\n",
      "Epoch [421/10000], Loss: 2.7217\n",
      "Epoch [422/10000], Loss: 2.7211\n",
      "Epoch [423/10000], Loss: 2.7205\n",
      "Epoch [424/10000], Loss: 2.7199\n",
      "Epoch [425/10000], Loss: 2.7193\n",
      "Epoch [426/10000], Loss: 2.7187\n",
      "Epoch [427/10000], Loss: 2.7181\n",
      "Epoch [428/10000], Loss: 2.7175\n",
      "Epoch [429/10000], Loss: 2.7169\n",
      "Epoch [430/10000], Loss: 2.7163\n",
      "Epoch [431/10000], Loss: 2.7157\n",
      "Epoch [432/10000], Loss: 2.7151\n",
      "Epoch [433/10000], Loss: 2.7145\n",
      "Epoch [434/10000], Loss: 2.7139\n",
      "Epoch [435/10000], Loss: 2.7133\n",
      "Epoch [436/10000], Loss: 2.7127\n",
      "Epoch [437/10000], Loss: 2.7121\n",
      "Epoch [438/10000], Loss: 2.7116\n",
      "Epoch [439/10000], Loss: 2.7110\n",
      "Epoch [440/10000], Loss: 2.7104\n",
      "Epoch [441/10000], Loss: 2.7098\n",
      "Epoch [442/10000], Loss: 2.7092\n",
      "Epoch [443/10000], Loss: 2.7086\n",
      "Epoch [444/10000], Loss: 2.7080\n",
      "Epoch [445/10000], Loss: 2.7074\n",
      "Epoch [446/10000], Loss: 2.7068\n",
      "Epoch [447/10000], Loss: 2.7063\n",
      "Epoch [448/10000], Loss: 2.7057\n",
      "Epoch [449/10000], Loss: 2.7051\n",
      "Epoch [450/10000], Loss: 2.7045\n",
      "Epoch [451/10000], Loss: 2.7039\n",
      "Epoch [452/10000], Loss: 2.7033\n",
      "Epoch [453/10000], Loss: 2.7027\n",
      "Epoch [454/10000], Loss: 2.7022\n",
      "Epoch [455/10000], Loss: 2.7016\n",
      "Epoch [456/10000], Loss: 2.7010\n",
      "Epoch [457/10000], Loss: 2.7004\n",
      "Epoch [458/10000], Loss: 2.6998\n",
      "Epoch [459/10000], Loss: 2.6992\n",
      "Epoch [460/10000], Loss: 2.6987\n",
      "Epoch [461/10000], Loss: 2.6981\n",
      "Epoch [462/10000], Loss: 2.6975\n",
      "Epoch [463/10000], Loss: 2.6969\n",
      "Epoch [464/10000], Loss: 2.6963\n",
      "Epoch [465/10000], Loss: 2.6958\n",
      "Epoch [466/10000], Loss: 2.6952\n",
      "Epoch [467/10000], Loss: 2.6946\n",
      "Epoch [468/10000], Loss: 2.6940\n",
      "Epoch [469/10000], Loss: 2.6935\n",
      "Epoch [470/10000], Loss: 2.6929\n",
      "Epoch [471/10000], Loss: 2.6923\n",
      "Epoch [472/10000], Loss: 2.6917\n",
      "Epoch [473/10000], Loss: 2.6912\n",
      "Epoch [474/10000], Loss: 2.6906\n",
      "Epoch [475/10000], Loss: 2.6900\n",
      "Epoch [476/10000], Loss: 2.6894\n",
      "Epoch [477/10000], Loss: 2.6889\n",
      "Epoch [478/10000], Loss: 2.6883\n",
      "Epoch [479/10000], Loss: 2.6877\n",
      "Epoch [480/10000], Loss: 2.6872\n",
      "Epoch [481/10000], Loss: 2.6866\n",
      "Epoch [482/10000], Loss: 2.6860\n",
      "Epoch [483/10000], Loss: 2.6855\n",
      "Epoch [484/10000], Loss: 2.6849\n",
      "Epoch [485/10000], Loss: 2.6843\n",
      "Epoch [486/10000], Loss: 2.6837\n",
      "Epoch [487/10000], Loss: 2.6832\n",
      "Epoch [488/10000], Loss: 2.6826\n",
      "Epoch [489/10000], Loss: 2.6820\n",
      "Epoch [490/10000], Loss: 2.6815\n",
      "Epoch [491/10000], Loss: 2.6809\n",
      "Epoch [492/10000], Loss: 2.6804\n",
      "Epoch [493/10000], Loss: 2.6798\n",
      "Epoch [494/10000], Loss: 2.6792\n",
      "Epoch [495/10000], Loss: 2.6787\n",
      "Epoch [496/10000], Loss: 2.6781\n",
      "Epoch [497/10000], Loss: 2.6775\n",
      "Epoch [498/10000], Loss: 2.6770\n",
      "Epoch [499/10000], Loss: 2.6764\n",
      "Epoch [500/10000], Loss: 2.6759\n",
      "Epoch [501/10000], Loss: 2.6753\n",
      "Epoch [502/10000], Loss: 2.6747\n",
      "Epoch [503/10000], Loss: 2.6742\n",
      "Epoch [504/10000], Loss: 2.6736\n",
      "Epoch [505/10000], Loss: 2.6731\n",
      "Epoch [506/10000], Loss: 2.6725\n",
      "Epoch [507/10000], Loss: 2.6719\n",
      "Epoch [508/10000], Loss: 2.6714\n",
      "Epoch [509/10000], Loss: 2.6708\n",
      "Epoch [510/10000], Loss: 2.6703\n",
      "Epoch [511/10000], Loss: 2.6697\n",
      "Epoch [512/10000], Loss: 2.6692\n",
      "Epoch [513/10000], Loss: 2.6686\n",
      "Epoch [514/10000], Loss: 2.6680\n",
      "Epoch [515/10000], Loss: 2.6675\n",
      "Epoch [516/10000], Loss: 2.6669\n",
      "Epoch [517/10000], Loss: 2.6664\n",
      "Epoch [518/10000], Loss: 2.6658\n",
      "Epoch [519/10000], Loss: 2.6653\n",
      "Epoch [520/10000], Loss: 2.6647\n",
      "Epoch [521/10000], Loss: 2.6642\n",
      "Epoch [522/10000], Loss: 2.6636\n",
      "Epoch [523/10000], Loss: 2.6631\n",
      "Epoch [524/10000], Loss: 2.6625\n",
      "Epoch [525/10000], Loss: 2.6620\n",
      "Epoch [526/10000], Loss: 2.6614\n",
      "Epoch [527/10000], Loss: 2.6609\n",
      "Epoch [528/10000], Loss: 2.6603\n",
      "Epoch [529/10000], Loss: 2.6598\n",
      "Epoch [530/10000], Loss: 2.6592\n",
      "Epoch [531/10000], Loss: 2.6587\n",
      "Epoch [532/10000], Loss: 2.6581\n",
      "Epoch [533/10000], Loss: 2.6576\n",
      "Epoch [534/10000], Loss: 2.6570\n",
      "Epoch [535/10000], Loss: 2.6565\n",
      "Epoch [536/10000], Loss: 2.6559\n",
      "Epoch [537/10000], Loss: 2.6554\n",
      "Epoch [538/10000], Loss: 2.6548\n",
      "Epoch [539/10000], Loss: 2.6543\n",
      "Epoch [540/10000], Loss: 2.6538\n",
      "Epoch [541/10000], Loss: 2.6532\n",
      "Epoch [542/10000], Loss: 2.6527\n",
      "Epoch [543/10000], Loss: 2.6521\n",
      "Epoch [544/10000], Loss: 2.6516\n",
      "Epoch [545/10000], Loss: 2.6510\n",
      "Epoch [546/10000], Loss: 2.6505\n",
      "Epoch [547/10000], Loss: 2.6500\n",
      "Epoch [548/10000], Loss: 2.6494\n",
      "Epoch [549/10000], Loss: 2.6489\n",
      "Epoch [550/10000], Loss: 2.6483\n",
      "Epoch [551/10000], Loss: 2.6478\n",
      "Epoch [552/10000], Loss: 2.6473\n",
      "Epoch [553/10000], Loss: 2.6467\n",
      "Epoch [554/10000], Loss: 2.6462\n",
      "Epoch [555/10000], Loss: 2.6456\n",
      "Epoch [556/10000], Loss: 2.6451\n",
      "Epoch [557/10000], Loss: 2.6446\n",
      "Epoch [558/10000], Loss: 2.6440\n",
      "Epoch [559/10000], Loss: 2.6435\n",
      "Epoch [560/10000], Loss: 2.6430\n",
      "Epoch [561/10000], Loss: 2.6424\n",
      "Epoch [562/10000], Loss: 2.6419\n",
      "Epoch [563/10000], Loss: 2.6414\n",
      "Epoch [564/10000], Loss: 2.6408\n",
      "Epoch [565/10000], Loss: 2.6403\n",
      "Epoch [566/10000], Loss: 2.6398\n",
      "Epoch [567/10000], Loss: 2.6392\n",
      "Epoch [568/10000], Loss: 2.6387\n",
      "Epoch [569/10000], Loss: 2.6382\n",
      "Epoch [570/10000], Loss: 2.6376\n",
      "Epoch [571/10000], Loss: 2.6371\n",
      "Epoch [572/10000], Loss: 2.6366\n",
      "Epoch [573/10000], Loss: 2.6360\n",
      "Epoch [574/10000], Loss: 2.6355\n",
      "Epoch [575/10000], Loss: 2.6350\n",
      "Epoch [576/10000], Loss: 2.6344\n",
      "Epoch [577/10000], Loss: 2.6339\n",
      "Epoch [578/10000], Loss: 2.6334\n",
      "Epoch [579/10000], Loss: 2.6328\n",
      "Epoch [580/10000], Loss: 2.6323\n",
      "Epoch [581/10000], Loss: 2.6318\n",
      "Epoch [582/10000], Loss: 2.6313\n",
      "Epoch [583/10000], Loss: 2.6307\n",
      "Epoch [584/10000], Loss: 2.6302\n",
      "Epoch [585/10000], Loss: 2.6297\n",
      "Epoch [586/10000], Loss: 2.6292\n",
      "Epoch [587/10000], Loss: 2.6286\n",
      "Epoch [588/10000], Loss: 2.6281\n",
      "Epoch [589/10000], Loss: 2.6276\n",
      "Epoch [590/10000], Loss: 2.6271\n",
      "Epoch [591/10000], Loss: 2.6265\n",
      "Epoch [592/10000], Loss: 2.6260\n",
      "Epoch [593/10000], Loss: 2.6255\n",
      "Epoch [594/10000], Loss: 2.6250\n",
      "Epoch [595/10000], Loss: 2.6244\n",
      "Epoch [596/10000], Loss: 2.6239\n",
      "Epoch [597/10000], Loss: 2.6234\n",
      "Epoch [598/10000], Loss: 2.6229\n",
      "Epoch [599/10000], Loss: 2.6223\n",
      "Epoch [600/10000], Loss: 2.6218\n",
      "Epoch [601/10000], Loss: 2.6213\n",
      "Epoch [602/10000], Loss: 2.6208\n",
      "Epoch [603/10000], Loss: 2.6203\n",
      "Epoch [604/10000], Loss: 2.6197\n",
      "Epoch [605/10000], Loss: 2.6192\n",
      "Epoch [606/10000], Loss: 2.6187\n",
      "Epoch [607/10000], Loss: 2.6182\n",
      "Epoch [608/10000], Loss: 2.6177\n",
      "Epoch [609/10000], Loss: 2.6171\n",
      "Epoch [610/10000], Loss: 2.6166\n",
      "Epoch [611/10000], Loss: 2.6161\n",
      "Epoch [612/10000], Loss: 2.6156\n",
      "Epoch [613/10000], Loss: 2.6151\n",
      "Epoch [614/10000], Loss: 2.6146\n",
      "Epoch [615/10000], Loss: 2.6140\n",
      "Epoch [616/10000], Loss: 2.6135\n",
      "Epoch [617/10000], Loss: 2.6130\n",
      "Epoch [618/10000], Loss: 2.6125\n",
      "Epoch [619/10000], Loss: 2.6120\n",
      "Epoch [620/10000], Loss: 2.6115\n",
      "Epoch [621/10000], Loss: 2.6109\n",
      "Epoch [622/10000], Loss: 2.6104\n",
      "Epoch [623/10000], Loss: 2.6099\n",
      "Epoch [624/10000], Loss: 2.6094\n",
      "Epoch [625/10000], Loss: 2.6089\n",
      "Epoch [626/10000], Loss: 2.6084\n",
      "Epoch [627/10000], Loss: 2.6079\n",
      "Epoch [628/10000], Loss: 2.6074\n",
      "Epoch [629/10000], Loss: 2.6068\n",
      "Epoch [630/10000], Loss: 2.6063\n",
      "Epoch [631/10000], Loss: 2.6058\n",
      "Epoch [632/10000], Loss: 2.6053\n",
      "Epoch [633/10000], Loss: 2.6048\n",
      "Epoch [634/10000], Loss: 2.6043\n",
      "Epoch [635/10000], Loss: 2.6038\n",
      "Epoch [636/10000], Loss: 2.6033\n",
      "Epoch [637/10000], Loss: 2.6028\n",
      "Epoch [638/10000], Loss: 2.6023\n",
      "Epoch [639/10000], Loss: 2.6017\n",
      "Epoch [640/10000], Loss: 2.6012\n",
      "Epoch [641/10000], Loss: 2.6007\n",
      "Epoch [642/10000], Loss: 2.6002\n",
      "Epoch [643/10000], Loss: 2.5997\n",
      "Epoch [644/10000], Loss: 2.5992\n",
      "Epoch [645/10000], Loss: 2.5987\n",
      "Epoch [646/10000], Loss: 2.5982\n",
      "Epoch [647/10000], Loss: 2.5977\n",
      "Epoch [648/10000], Loss: 2.5972\n",
      "Epoch [649/10000], Loss: 2.5967\n",
      "Epoch [650/10000], Loss: 2.5962\n",
      "Epoch [651/10000], Loss: 2.5957\n",
      "Epoch [652/10000], Loss: 2.5952\n",
      "Epoch [653/10000], Loss: 2.5947\n",
      "Epoch [654/10000], Loss: 2.5942\n",
      "Epoch [655/10000], Loss: 2.5937\n",
      "Epoch [656/10000], Loss: 2.5931\n",
      "Epoch [657/10000], Loss: 2.5926\n",
      "Epoch [658/10000], Loss: 2.5921\n",
      "Epoch [659/10000], Loss: 2.5916\n",
      "Epoch [660/10000], Loss: 2.5911\n",
      "Epoch [661/10000], Loss: 2.5906\n",
      "Epoch [662/10000], Loss: 2.5901\n",
      "Epoch [663/10000], Loss: 2.5896\n",
      "Epoch [664/10000], Loss: 2.5891\n",
      "Epoch [665/10000], Loss: 2.5886\n",
      "Epoch [666/10000], Loss: 2.5881\n",
      "Epoch [667/10000], Loss: 2.5876\n",
      "Epoch [668/10000], Loss: 2.5871\n",
      "Epoch [669/10000], Loss: 2.5866\n",
      "Epoch [670/10000], Loss: 2.5861\n",
      "Epoch [671/10000], Loss: 2.5856\n",
      "Epoch [672/10000], Loss: 2.5851\n",
      "Epoch [673/10000], Loss: 2.5846\n",
      "Epoch [674/10000], Loss: 2.5841\n",
      "Epoch [675/10000], Loss: 2.5836\n",
      "Epoch [676/10000], Loss: 2.5832\n",
      "Epoch [677/10000], Loss: 2.5827\n",
      "Epoch [678/10000], Loss: 2.5822\n",
      "Epoch [679/10000], Loss: 2.5817\n",
      "Epoch [680/10000], Loss: 2.5812\n",
      "Epoch [681/10000], Loss: 2.5807\n",
      "Epoch [682/10000], Loss: 2.5802\n",
      "Epoch [683/10000], Loss: 2.5797\n",
      "Epoch [684/10000], Loss: 2.5792\n",
      "Epoch [685/10000], Loss: 2.5787\n",
      "Epoch [686/10000], Loss: 2.5782\n",
      "Epoch [687/10000], Loss: 2.5777\n",
      "Epoch [688/10000], Loss: 2.5772\n",
      "Epoch [689/10000], Loss: 2.5767\n",
      "Epoch [690/10000], Loss: 2.5762\n",
      "Epoch [691/10000], Loss: 2.5757\n",
      "Epoch [692/10000], Loss: 2.5752\n",
      "Epoch [693/10000], Loss: 2.5747\n",
      "Epoch [694/10000], Loss: 2.5743\n",
      "Epoch [695/10000], Loss: 2.5738\n",
      "Epoch [696/10000], Loss: 2.5733\n",
      "Epoch [697/10000], Loss: 2.5728\n",
      "Epoch [698/10000], Loss: 2.5723\n",
      "Epoch [699/10000], Loss: 2.5718\n",
      "Epoch [700/10000], Loss: 2.5713\n",
      "Epoch [701/10000], Loss: 2.5708\n",
      "Epoch [702/10000], Loss: 2.5703\n",
      "Epoch [703/10000], Loss: 2.5698\n",
      "Epoch [704/10000], Loss: 2.5694\n",
      "Epoch [705/10000], Loss: 2.5689\n",
      "Epoch [706/10000], Loss: 2.5684\n",
      "Epoch [707/10000], Loss: 2.5679\n",
      "Epoch [708/10000], Loss: 2.5674\n",
      "Epoch [709/10000], Loss: 2.5669\n",
      "Epoch [710/10000], Loss: 2.5664\n",
      "Epoch [711/10000], Loss: 2.5659\n",
      "Epoch [712/10000], Loss: 2.5655\n",
      "Epoch [713/10000], Loss: 2.5650\n",
      "Epoch [714/10000], Loss: 2.5645\n",
      "Epoch [715/10000], Loss: 2.5640\n",
      "Epoch [716/10000], Loss: 2.5635\n",
      "Epoch [717/10000], Loss: 2.5630\n",
      "Epoch [718/10000], Loss: 2.5625\n",
      "Epoch [719/10000], Loss: 2.5621\n",
      "Epoch [720/10000], Loss: 2.5616\n",
      "Epoch [721/10000], Loss: 2.5611\n",
      "Epoch [722/10000], Loss: 2.5606\n",
      "Epoch [723/10000], Loss: 2.5601\n",
      "Epoch [724/10000], Loss: 2.5596\n",
      "Epoch [725/10000], Loss: 2.5592\n",
      "Epoch [726/10000], Loss: 2.5587\n",
      "Epoch [727/10000], Loss: 2.5582\n",
      "Epoch [728/10000], Loss: 2.5577\n",
      "Epoch [729/10000], Loss: 2.5572\n",
      "Epoch [730/10000], Loss: 2.5567\n",
      "Epoch [731/10000], Loss: 2.5563\n",
      "Epoch [732/10000], Loss: 2.5558\n",
      "Epoch [733/10000], Loss: 2.5553\n",
      "Epoch [734/10000], Loss: 2.5548\n",
      "Epoch [735/10000], Loss: 2.5543\n",
      "Epoch [736/10000], Loss: 2.5539\n",
      "Epoch [737/10000], Loss: 2.5534\n",
      "Epoch [738/10000], Loss: 2.5529\n",
      "Epoch [739/10000], Loss: 2.5524\n",
      "Epoch [740/10000], Loss: 2.5519\n",
      "Epoch [741/10000], Loss: 2.5515\n",
      "Epoch [742/10000], Loss: 2.5510\n",
      "Epoch [743/10000], Loss: 2.5505\n",
      "Epoch [744/10000], Loss: 2.5500\n",
      "Epoch [745/10000], Loss: 2.5496\n",
      "Epoch [746/10000], Loss: 2.5491\n",
      "Epoch [747/10000], Loss: 2.5486\n",
      "Epoch [748/10000], Loss: 2.5481\n",
      "Epoch [749/10000], Loss: 2.5476\n",
      "Epoch [750/10000], Loss: 2.5472\n",
      "Epoch [751/10000], Loss: 2.5467\n",
      "Epoch [752/10000], Loss: 2.5462\n",
      "Epoch [753/10000], Loss: 2.5457\n",
      "Epoch [754/10000], Loss: 2.5453\n",
      "Epoch [755/10000], Loss: 2.5448\n",
      "Epoch [756/10000], Loss: 2.5443\n",
      "Epoch [757/10000], Loss: 2.5438\n",
      "Epoch [758/10000], Loss: 2.5434\n",
      "Epoch [759/10000], Loss: 2.5429\n",
      "Epoch [760/10000], Loss: 2.5424\n",
      "Epoch [761/10000], Loss: 2.5420\n",
      "Epoch [762/10000], Loss: 2.5415\n",
      "Epoch [763/10000], Loss: 2.5410\n",
      "Epoch [764/10000], Loss: 2.5405\n",
      "Epoch [765/10000], Loss: 2.5401\n",
      "Epoch [766/10000], Loss: 2.5396\n",
      "Epoch [767/10000], Loss: 2.5391\n",
      "Epoch [768/10000], Loss: 2.5386\n",
      "Epoch [769/10000], Loss: 2.5382\n",
      "Epoch [770/10000], Loss: 2.5377\n",
      "Epoch [771/10000], Loss: 2.5372\n",
      "Epoch [772/10000], Loss: 2.5368\n",
      "Epoch [773/10000], Loss: 2.5363\n",
      "Epoch [774/10000], Loss: 2.5358\n",
      "Epoch [775/10000], Loss: 2.5354\n",
      "Epoch [776/10000], Loss: 2.5349\n",
      "Epoch [777/10000], Loss: 2.5344\n",
      "Epoch [778/10000], Loss: 2.5339\n",
      "Epoch [779/10000], Loss: 2.5335\n",
      "Epoch [780/10000], Loss: 2.5330\n",
      "Epoch [781/10000], Loss: 2.5325\n",
      "Epoch [782/10000], Loss: 2.5321\n",
      "Epoch [783/10000], Loss: 2.5316\n",
      "Epoch [784/10000], Loss: 2.5311\n",
      "Epoch [785/10000], Loss: 2.5307\n",
      "Epoch [786/10000], Loss: 2.5302\n",
      "Epoch [787/10000], Loss: 2.5297\n",
      "Epoch [788/10000], Loss: 2.5293\n",
      "Epoch [789/10000], Loss: 2.5288\n",
      "Epoch [790/10000], Loss: 2.5283\n",
      "Epoch [791/10000], Loss: 2.5279\n",
      "Epoch [792/10000], Loss: 2.5274\n",
      "Epoch [793/10000], Loss: 2.5269\n",
      "Epoch [794/10000], Loss: 2.5265\n",
      "Epoch [795/10000], Loss: 2.5260\n",
      "Epoch [796/10000], Loss: 2.5256\n",
      "Epoch [797/10000], Loss: 2.5251\n",
      "Epoch [798/10000], Loss: 2.5246\n",
      "Epoch [799/10000], Loss: 2.5242\n",
      "Epoch [800/10000], Loss: 2.5237\n",
      "Epoch [801/10000], Loss: 2.5232\n",
      "Epoch [802/10000], Loss: 2.5228\n",
      "Epoch [803/10000], Loss: 2.5223\n",
      "Epoch [804/10000], Loss: 2.5218\n",
      "Epoch [805/10000], Loss: 2.5214\n",
      "Epoch [806/10000], Loss: 2.5209\n",
      "Epoch [807/10000], Loss: 2.5205\n",
      "Epoch [808/10000], Loss: 2.5200\n",
      "Epoch [809/10000], Loss: 2.5195\n",
      "Epoch [810/10000], Loss: 2.5191\n",
      "Epoch [811/10000], Loss: 2.5186\n",
      "Epoch [812/10000], Loss: 2.5182\n",
      "Epoch [813/10000], Loss: 2.5177\n",
      "Epoch [814/10000], Loss: 2.5172\n",
      "Epoch [815/10000], Loss: 2.5168\n",
      "Epoch [816/10000], Loss: 2.5163\n",
      "Epoch [817/10000], Loss: 2.5159\n",
      "Epoch [818/10000], Loss: 2.5154\n",
      "Epoch [819/10000], Loss: 2.5149\n",
      "Epoch [820/10000], Loss: 2.5145\n",
      "Epoch [821/10000], Loss: 2.5140\n",
      "Epoch [822/10000], Loss: 2.5136\n",
      "Epoch [823/10000], Loss: 2.5131\n",
      "Epoch [824/10000], Loss: 2.5126\n",
      "Epoch [825/10000], Loss: 2.5122\n",
      "Epoch [826/10000], Loss: 2.5117\n",
      "Epoch [827/10000], Loss: 2.5113\n",
      "Epoch [828/10000], Loss: 2.5108\n",
      "Epoch [829/10000], Loss: 2.5104\n",
      "Epoch [830/10000], Loss: 2.5099\n",
      "Epoch [831/10000], Loss: 2.5094\n",
      "Epoch [832/10000], Loss: 2.5090\n",
      "Epoch [833/10000], Loss: 2.5085\n",
      "Epoch [834/10000], Loss: 2.5081\n",
      "Epoch [835/10000], Loss: 2.5076\n",
      "Epoch [836/10000], Loss: 2.5072\n",
      "Epoch [837/10000], Loss: 2.5067\n",
      "Epoch [838/10000], Loss: 2.5063\n",
      "Epoch [839/10000], Loss: 2.5058\n",
      "Epoch [840/10000], Loss: 2.5054\n",
      "Epoch [841/10000], Loss: 2.5049\n",
      "Epoch [842/10000], Loss: 2.5044\n",
      "Epoch [843/10000], Loss: 2.5040\n",
      "Epoch [844/10000], Loss: 2.5035\n",
      "Epoch [845/10000], Loss: 2.5031\n",
      "Epoch [846/10000], Loss: 2.5026\n",
      "Epoch [847/10000], Loss: 2.5022\n",
      "Epoch [848/10000], Loss: 2.5017\n",
      "Epoch [849/10000], Loss: 2.5013\n",
      "Epoch [850/10000], Loss: 2.5008\n",
      "Epoch [851/10000], Loss: 2.5004\n",
      "Epoch [852/10000], Loss: 2.4999\n",
      "Epoch [853/10000], Loss: 2.4995\n",
      "Epoch [854/10000], Loss: 2.4990\n",
      "Epoch [855/10000], Loss: 2.4986\n",
      "Epoch [856/10000], Loss: 2.4981\n",
      "Epoch [857/10000], Loss: 2.4977\n",
      "Epoch [858/10000], Loss: 2.4972\n",
      "Epoch [859/10000], Loss: 2.4968\n",
      "Epoch [860/10000], Loss: 2.4963\n",
      "Epoch [861/10000], Loss: 2.4959\n",
      "Epoch [862/10000], Loss: 2.4954\n",
      "Epoch [863/10000], Loss: 2.4950\n",
      "Epoch [864/10000], Loss: 2.4945\n",
      "Epoch [865/10000], Loss: 2.4941\n",
      "Epoch [866/10000], Loss: 2.4936\n",
      "Epoch [867/10000], Loss: 2.4932\n",
      "Epoch [868/10000], Loss: 2.4927\n",
      "Epoch [869/10000], Loss: 2.4923\n",
      "Epoch [870/10000], Loss: 2.4918\n",
      "Epoch [871/10000], Loss: 2.4914\n",
      "Epoch [872/10000], Loss: 2.4910\n",
      "Epoch [873/10000], Loss: 2.4905\n",
      "Epoch [874/10000], Loss: 2.4901\n",
      "Epoch [875/10000], Loss: 2.4896\n",
      "Epoch [876/10000], Loss: 2.4892\n",
      "Epoch [877/10000], Loss: 2.4887\n",
      "Epoch [878/10000], Loss: 2.4883\n",
      "Epoch [879/10000], Loss: 2.4878\n",
      "Epoch [880/10000], Loss: 2.4874\n",
      "Epoch [881/10000], Loss: 2.4869\n",
      "Epoch [882/10000], Loss: 2.4865\n",
      "Epoch [883/10000], Loss: 2.4861\n",
      "Epoch [884/10000], Loss: 2.4856\n",
      "Epoch [885/10000], Loss: 2.4852\n",
      "Epoch [886/10000], Loss: 2.4847\n",
      "Epoch [887/10000], Loss: 2.4843\n",
      "Epoch [888/10000], Loss: 2.4838\n",
      "Epoch [889/10000], Loss: 2.4834\n",
      "Epoch [890/10000], Loss: 2.4829\n",
      "Epoch [891/10000], Loss: 2.4825\n",
      "Epoch [892/10000], Loss: 2.4821\n",
      "Epoch [893/10000], Loss: 2.4816\n",
      "Epoch [894/10000], Loss: 2.4812\n",
      "Epoch [895/10000], Loss: 2.4807\n",
      "Epoch [896/10000], Loss: 2.4803\n",
      "Epoch [897/10000], Loss: 2.4799\n",
      "Epoch [898/10000], Loss: 2.4794\n",
      "Epoch [899/10000], Loss: 2.4790\n",
      "Epoch [900/10000], Loss: 2.4785\n",
      "Epoch [901/10000], Loss: 2.4781\n",
      "Epoch [902/10000], Loss: 2.4777\n",
      "Epoch [903/10000], Loss: 2.4772\n",
      "Epoch [904/10000], Loss: 2.4768\n",
      "Epoch [905/10000], Loss: 2.4763\n",
      "Epoch [906/10000], Loss: 2.4759\n",
      "Epoch [907/10000], Loss: 2.4755\n",
      "Epoch [908/10000], Loss: 2.4750\n",
      "Epoch [909/10000], Loss: 2.4746\n",
      "Epoch [910/10000], Loss: 2.4741\n",
      "Epoch [911/10000], Loss: 2.4737\n",
      "Epoch [912/10000], Loss: 2.4733\n",
      "Epoch [913/10000], Loss: 2.4728\n",
      "Epoch [914/10000], Loss: 2.4724\n",
      "Epoch [915/10000], Loss: 2.4720\n",
      "Epoch [916/10000], Loss: 2.4715\n",
      "Epoch [917/10000], Loss: 2.4711\n",
      "Epoch [918/10000], Loss: 2.4706\n",
      "Epoch [919/10000], Loss: 2.4702\n",
      "Epoch [920/10000], Loss: 2.4698\n",
      "Epoch [921/10000], Loss: 2.4693\n",
      "Epoch [922/10000], Loss: 2.4689\n",
      "Epoch [923/10000], Loss: 2.4685\n",
      "Epoch [924/10000], Loss: 2.4680\n",
      "Epoch [925/10000], Loss: 2.4676\n",
      "Epoch [926/10000], Loss: 2.4672\n",
      "Epoch [927/10000], Loss: 2.4667\n",
      "Epoch [928/10000], Loss: 2.4663\n",
      "Epoch [929/10000], Loss: 2.4659\n",
      "Epoch [930/10000], Loss: 2.4654\n",
      "Epoch [931/10000], Loss: 2.4650\n",
      "Epoch [932/10000], Loss: 2.4645\n",
      "Epoch [933/10000], Loss: 2.4641\n",
      "Epoch [934/10000], Loss: 2.4637\n",
      "Epoch [935/10000], Loss: 2.4632\n",
      "Epoch [936/10000], Loss: 2.4628\n",
      "Epoch [937/10000], Loss: 2.4624\n",
      "Epoch [938/10000], Loss: 2.4620\n",
      "Epoch [939/10000], Loss: 2.4615\n",
      "Epoch [940/10000], Loss: 2.4611\n",
      "Epoch [941/10000], Loss: 2.4607\n",
      "Epoch [942/10000], Loss: 2.4602\n",
      "Epoch [943/10000], Loss: 2.4598\n",
      "Epoch [944/10000], Loss: 2.4594\n",
      "Epoch [945/10000], Loss: 2.4589\n",
      "Epoch [946/10000], Loss: 2.4585\n",
      "Epoch [947/10000], Loss: 2.4581\n",
      "Epoch [948/10000], Loss: 2.4576\n",
      "Epoch [949/10000], Loss: 2.4572\n",
      "Epoch [950/10000], Loss: 2.4568\n",
      "Epoch [951/10000], Loss: 2.4563\n",
      "Epoch [952/10000], Loss: 2.4559\n",
      "Epoch [953/10000], Loss: 2.4555\n",
      "Epoch [954/10000], Loss: 2.4551\n",
      "Epoch [955/10000], Loss: 2.4546\n",
      "Epoch [956/10000], Loss: 2.4542\n",
      "Epoch [957/10000], Loss: 2.4538\n",
      "Epoch [958/10000], Loss: 2.4533\n",
      "Epoch [959/10000], Loss: 2.4529\n",
      "Epoch [960/10000], Loss: 2.4525\n",
      "Epoch [961/10000], Loss: 2.4521\n",
      "Epoch [962/10000], Loss: 2.4516\n",
      "Epoch [963/10000], Loss: 2.4512\n",
      "Epoch [964/10000], Loss: 2.4508\n",
      "Epoch [965/10000], Loss: 2.4503\n",
      "Epoch [966/10000], Loss: 2.4499\n",
      "Epoch [967/10000], Loss: 2.4495\n",
      "Epoch [968/10000], Loss: 2.4491\n",
      "Epoch [969/10000], Loss: 2.4486\n",
      "Epoch [970/10000], Loss: 2.4482\n",
      "Epoch [971/10000], Loss: 2.4478\n",
      "Epoch [972/10000], Loss: 2.4474\n",
      "Epoch [973/10000], Loss: 2.4469\n",
      "Epoch [974/10000], Loss: 2.4465\n",
      "Epoch [975/10000], Loss: 2.4461\n",
      "Epoch [976/10000], Loss: 2.4457\n",
      "Epoch [977/10000], Loss: 2.4452\n",
      "Epoch [978/10000], Loss: 2.4448\n",
      "Epoch [979/10000], Loss: 2.4444\n",
      "Epoch [980/10000], Loss: 2.4440\n",
      "Epoch [981/10000], Loss: 2.4435\n",
      "Epoch [982/10000], Loss: 2.4431\n",
      "Epoch [983/10000], Loss: 2.4427\n",
      "Epoch [984/10000], Loss: 2.4423\n",
      "Epoch [985/10000], Loss: 2.4418\n",
      "Epoch [986/10000], Loss: 2.4414\n",
      "Epoch [987/10000], Loss: 2.4410\n",
      "Epoch [988/10000], Loss: 2.4406\n",
      "Epoch [989/10000], Loss: 2.4402\n",
      "Epoch [990/10000], Loss: 2.4397\n",
      "Epoch [991/10000], Loss: 2.4393\n",
      "Epoch [992/10000], Loss: 2.4389\n",
      "Epoch [993/10000], Loss: 2.4385\n",
      "Epoch [994/10000], Loss: 2.4380\n",
      "Epoch [995/10000], Loss: 2.4376\n",
      "Epoch [996/10000], Loss: 2.4372\n",
      "Epoch [997/10000], Loss: 2.4368\n",
      "Epoch [998/10000], Loss: 2.4364\n",
      "Epoch [999/10000], Loss: 2.4359\n",
      "Epoch [1000/10000], Loss: 2.4355\n",
      "Epoch [1001/10000], Loss: 2.4351\n",
      "Epoch [1002/10000], Loss: 2.4347\n",
      "Epoch [1003/10000], Loss: 2.4343\n",
      "Epoch [1004/10000], Loss: 2.4338\n",
      "Epoch [1005/10000], Loss: 2.4334\n",
      "Epoch [1006/10000], Loss: 2.4330\n",
      "Epoch [1007/10000], Loss: 2.4326\n",
      "Epoch [1008/10000], Loss: 2.4322\n",
      "Epoch [1009/10000], Loss: 2.4317\n",
      "Epoch [1010/10000], Loss: 2.4313\n",
      "Epoch [1011/10000], Loss: 2.4309\n",
      "Epoch [1012/10000], Loss: 2.4305\n",
      "Epoch [1013/10000], Loss: 2.4301\n",
      "Epoch [1014/10000], Loss: 2.4297\n",
      "Epoch [1015/10000], Loss: 2.4292\n",
      "Epoch [1016/10000], Loss: 2.4288\n",
      "Epoch [1017/10000], Loss: 2.4284\n",
      "Epoch [1018/10000], Loss: 2.4280\n",
      "Epoch [1019/10000], Loss: 2.4276\n",
      "Epoch [1020/10000], Loss: 2.4272\n",
      "Epoch [1021/10000], Loss: 2.4267\n",
      "Epoch [1022/10000], Loss: 2.4263\n",
      "Epoch [1023/10000], Loss: 2.4259\n",
      "Epoch [1024/10000], Loss: 2.4255\n",
      "Epoch [1025/10000], Loss: 2.4251\n",
      "Epoch [1026/10000], Loss: 2.4247\n",
      "Epoch [1027/10000], Loss: 2.4242\n",
      "Epoch [1028/10000], Loss: 2.4238\n",
      "Epoch [1029/10000], Loss: 2.4234\n",
      "Epoch [1030/10000], Loss: 2.4230\n",
      "Epoch [1031/10000], Loss: 2.4226\n",
      "Epoch [1032/10000], Loss: 2.4222\n",
      "Epoch [1033/10000], Loss: 2.4218\n",
      "Epoch [1034/10000], Loss: 2.4213\n",
      "Epoch [1035/10000], Loss: 2.4209\n",
      "Epoch [1036/10000], Loss: 2.4205\n",
      "Epoch [1037/10000], Loss: 2.4201\n",
      "Epoch [1038/10000], Loss: 2.4197\n",
      "Epoch [1039/10000], Loss: 2.4193\n",
      "Epoch [1040/10000], Loss: 2.4189\n",
      "Epoch [1041/10000], Loss: 2.4184\n",
      "Epoch [1042/10000], Loss: 2.4180\n",
      "Epoch [1043/10000], Loss: 2.4176\n",
      "Epoch [1044/10000], Loss: 2.4172\n",
      "Epoch [1045/10000], Loss: 2.4168\n",
      "Epoch [1046/10000], Loss: 2.4164\n",
      "Epoch [1047/10000], Loss: 2.4160\n",
      "Epoch [1048/10000], Loss: 2.4156\n",
      "Epoch [1049/10000], Loss: 2.4152\n",
      "Epoch [1050/10000], Loss: 2.4147\n",
      "Epoch [1051/10000], Loss: 2.4143\n",
      "Epoch [1052/10000], Loss: 2.4139\n",
      "Epoch [1053/10000], Loss: 2.4135\n",
      "Epoch [1054/10000], Loss: 2.4131\n",
      "Epoch [1055/10000], Loss: 2.4127\n",
      "Epoch [1056/10000], Loss: 2.4123\n",
      "Epoch [1057/10000], Loss: 2.4119\n",
      "Epoch [1058/10000], Loss: 2.4115\n",
      "Epoch [1059/10000], Loss: 2.4110\n",
      "Epoch [1060/10000], Loss: 2.4106\n",
      "Epoch [1061/10000], Loss: 2.4102\n",
      "Epoch [1062/10000], Loss: 2.4098\n",
      "Epoch [1063/10000], Loss: 2.4094\n",
      "Epoch [1064/10000], Loss: 2.4090\n",
      "Epoch [1065/10000], Loss: 2.4086\n",
      "Epoch [1066/10000], Loss: 2.4082\n",
      "Epoch [1067/10000], Loss: 2.4078\n",
      "Epoch [1068/10000], Loss: 2.4074\n",
      "Epoch [1069/10000], Loss: 2.4070\n",
      "Epoch [1070/10000], Loss: 2.4066\n",
      "Epoch [1071/10000], Loss: 2.4061\n",
      "Epoch [1072/10000], Loss: 2.4057\n",
      "Epoch [1073/10000], Loss: 2.4053\n",
      "Epoch [1074/10000], Loss: 2.4049\n",
      "Epoch [1075/10000], Loss: 2.4045\n",
      "Epoch [1076/10000], Loss: 2.4041\n",
      "Epoch [1077/10000], Loss: 2.4037\n",
      "Epoch [1078/10000], Loss: 2.4033\n",
      "Epoch [1079/10000], Loss: 2.4029\n",
      "Epoch [1080/10000], Loss: 2.4025\n",
      "Epoch [1081/10000], Loss: 2.4021\n",
      "Epoch [1082/10000], Loss: 2.4017\n",
      "Epoch [1083/10000], Loss: 2.4013\n",
      "Epoch [1084/10000], Loss: 2.4009\n",
      "Epoch [1085/10000], Loss: 2.4005\n",
      "Epoch [1086/10000], Loss: 2.4001\n",
      "Epoch [1087/10000], Loss: 2.3997\n",
      "Epoch [1088/10000], Loss: 2.3993\n",
      "Epoch [1089/10000], Loss: 2.3988\n",
      "Epoch [1090/10000], Loss: 2.3984\n",
      "Epoch [1091/10000], Loss: 2.3980\n",
      "Epoch [1092/10000], Loss: 2.3976\n",
      "Epoch [1093/10000], Loss: 2.3972\n",
      "Epoch [1094/10000], Loss: 2.3968\n",
      "Epoch [1095/10000], Loss: 2.3964\n",
      "Epoch [1096/10000], Loss: 2.3960\n",
      "Epoch [1097/10000], Loss: 2.3956\n",
      "Epoch [1098/10000], Loss: 2.3952\n",
      "Epoch [1099/10000], Loss: 2.3948\n",
      "Epoch [1100/10000], Loss: 2.3944\n",
      "Epoch [1101/10000], Loss: 2.3940\n",
      "Epoch [1102/10000], Loss: 2.3936\n",
      "Epoch [1103/10000], Loss: 2.3932\n",
      "Epoch [1104/10000], Loss: 2.3928\n",
      "Epoch [1105/10000], Loss: 2.3924\n",
      "Epoch [1106/10000], Loss: 2.3920\n",
      "Epoch [1107/10000], Loss: 2.3916\n",
      "Epoch [1108/10000], Loss: 2.3912\n",
      "Epoch [1109/10000], Loss: 2.3908\n",
      "Epoch [1110/10000], Loss: 2.3904\n",
      "Epoch [1111/10000], Loss: 2.3900\n",
      "Epoch [1112/10000], Loss: 2.3896\n",
      "Epoch [1113/10000], Loss: 2.3892\n",
      "Epoch [1114/10000], Loss: 2.3888\n",
      "Epoch [1115/10000], Loss: 2.3884\n",
      "Epoch [1116/10000], Loss: 2.3880\n",
      "Epoch [1117/10000], Loss: 2.3876\n",
      "Epoch [1118/10000], Loss: 2.3872\n",
      "Epoch [1119/10000], Loss: 2.3868\n",
      "Epoch [1120/10000], Loss: 2.3864\n",
      "Epoch [1121/10000], Loss: 2.3860\n",
      "Epoch [1122/10000], Loss: 2.3856\n",
      "Epoch [1123/10000], Loss: 2.3852\n",
      "Epoch [1124/10000], Loss: 2.3848\n",
      "Epoch [1125/10000], Loss: 2.3844\n",
      "Epoch [1126/10000], Loss: 2.3840\n",
      "Epoch [1127/10000], Loss: 2.3836\n",
      "Epoch [1128/10000], Loss: 2.3832\n",
      "Epoch [1129/10000], Loss: 2.3828\n",
      "Epoch [1130/10000], Loss: 2.3824\n",
      "Epoch [1131/10000], Loss: 2.3820\n",
      "Epoch [1132/10000], Loss: 2.3816\n",
      "Epoch [1133/10000], Loss: 2.3812\n",
      "Epoch [1134/10000], Loss: 2.3808\n",
      "Epoch [1135/10000], Loss: 2.3804\n",
      "Epoch [1136/10000], Loss: 2.3800\n",
      "Epoch [1137/10000], Loss: 2.3796\n",
      "Epoch [1138/10000], Loss: 2.3792\n",
      "Epoch [1139/10000], Loss: 2.3789\n",
      "Epoch [1140/10000], Loss: 2.3785\n",
      "Epoch [1141/10000], Loss: 2.3781\n",
      "Epoch [1142/10000], Loss: 2.3777\n",
      "Epoch [1143/10000], Loss: 2.3773\n",
      "Epoch [1144/10000], Loss: 2.3769\n",
      "Epoch [1145/10000], Loss: 2.3765\n",
      "Epoch [1146/10000], Loss: 2.3761\n",
      "Epoch [1147/10000], Loss: 2.3757\n",
      "Epoch [1148/10000], Loss: 2.3753\n",
      "Epoch [1149/10000], Loss: 2.3749\n",
      "Epoch [1150/10000], Loss: 2.3745\n",
      "Epoch [1151/10000], Loss: 2.3741\n",
      "Epoch [1152/10000], Loss: 2.3737\n",
      "Epoch [1153/10000], Loss: 2.3733\n",
      "Epoch [1154/10000], Loss: 2.3729\n",
      "Epoch [1155/10000], Loss: 2.3725\n",
      "Epoch [1156/10000], Loss: 2.3721\n",
      "Epoch [1157/10000], Loss: 2.3718\n",
      "Epoch [1158/10000], Loss: 2.3714\n",
      "Epoch [1159/10000], Loss: 2.3710\n",
      "Epoch [1160/10000], Loss: 2.3706\n",
      "Epoch [1161/10000], Loss: 2.3702\n",
      "Epoch [1162/10000], Loss: 2.3698\n",
      "Epoch [1163/10000], Loss: 2.3694\n",
      "Epoch [1164/10000], Loss: 2.3690\n",
      "Epoch [1165/10000], Loss: 2.3686\n",
      "Epoch [1166/10000], Loss: 2.3682\n",
      "Epoch [1167/10000], Loss: 2.3678\n",
      "Epoch [1168/10000], Loss: 2.3674\n",
      "Epoch [1169/10000], Loss: 2.3671\n",
      "Epoch [1170/10000], Loss: 2.3667\n",
      "Epoch [1171/10000], Loss: 2.3663\n",
      "Epoch [1172/10000], Loss: 2.3659\n",
      "Epoch [1173/10000], Loss: 2.3655\n",
      "Epoch [1174/10000], Loss: 2.3651\n",
      "Epoch [1175/10000], Loss: 2.3647\n",
      "Epoch [1176/10000], Loss: 2.3643\n",
      "Epoch [1177/10000], Loss: 2.3639\n",
      "Epoch [1178/10000], Loss: 2.3635\n",
      "Epoch [1179/10000], Loss: 2.3632\n",
      "Epoch [1180/10000], Loss: 2.3628\n",
      "Epoch [1181/10000], Loss: 2.3624\n",
      "Epoch [1182/10000], Loss: 2.3620\n",
      "Epoch [1183/10000], Loss: 2.3616\n",
      "Epoch [1184/10000], Loss: 2.3612\n",
      "Epoch [1185/10000], Loss: 2.3608\n",
      "Epoch [1186/10000], Loss: 2.3604\n",
      "Epoch [1187/10000], Loss: 2.3600\n",
      "Epoch [1188/10000], Loss: 2.3597\n",
      "Epoch [1189/10000], Loss: 2.3593\n",
      "Epoch [1190/10000], Loss: 2.3589\n",
      "Epoch [1191/10000], Loss: 2.3585\n",
      "Epoch [1192/10000], Loss: 2.3581\n",
      "Epoch [1193/10000], Loss: 2.3577\n",
      "Epoch [1194/10000], Loss: 2.3573\n",
      "Epoch [1195/10000], Loss: 2.3569\n",
      "Epoch [1196/10000], Loss: 2.3566\n",
      "Epoch [1197/10000], Loss: 2.3562\n",
      "Epoch [1198/10000], Loss: 2.3558\n",
      "Epoch [1199/10000], Loss: 2.3554\n",
      "Epoch [1200/10000], Loss: 2.3550\n",
      "Epoch [1201/10000], Loss: 2.3546\n",
      "Epoch [1202/10000], Loss: 2.3542\n",
      "Epoch [1203/10000], Loss: 2.3539\n",
      "Epoch [1204/10000], Loss: 2.3535\n",
      "Epoch [1205/10000], Loss: 2.3531\n",
      "Epoch [1206/10000], Loss: 2.3527\n",
      "Epoch [1207/10000], Loss: 2.3523\n",
      "Epoch [1208/10000], Loss: 2.3519\n",
      "Epoch [1209/10000], Loss: 2.3515\n",
      "Epoch [1210/10000], Loss: 2.3512\n",
      "Epoch [1211/10000], Loss: 2.3508\n",
      "Epoch [1212/10000], Loss: 2.3504\n",
      "Epoch [1213/10000], Loss: 2.3500\n",
      "Epoch [1214/10000], Loss: 2.3496\n",
      "Epoch [1215/10000], Loss: 2.3492\n",
      "Epoch [1216/10000], Loss: 2.3489\n",
      "Epoch [1217/10000], Loss: 2.3485\n",
      "Epoch [1218/10000], Loss: 2.3481\n",
      "Epoch [1219/10000], Loss: 2.3477\n",
      "Epoch [1220/10000], Loss: 2.3473\n",
      "Epoch [1221/10000], Loss: 2.3469\n",
      "Epoch [1222/10000], Loss: 2.3466\n",
      "Epoch [1223/10000], Loss: 2.3462\n",
      "Epoch [1224/10000], Loss: 2.3458\n",
      "Epoch [1225/10000], Loss: 2.3454\n",
      "Epoch [1226/10000], Loss: 2.3450\n",
      "Epoch [1227/10000], Loss: 2.3446\n",
      "Epoch [1228/10000], Loss: 2.3443\n",
      "Epoch [1229/10000], Loss: 2.3439\n",
      "Epoch [1230/10000], Loss: 2.3435\n",
      "Epoch [1231/10000], Loss: 2.3431\n",
      "Epoch [1232/10000], Loss: 2.3427\n",
      "Epoch [1233/10000], Loss: 2.3424\n",
      "Epoch [1234/10000], Loss: 2.3420\n",
      "Epoch [1235/10000], Loss: 2.3416\n",
      "Epoch [1236/10000], Loss: 2.3412\n",
      "Epoch [1237/10000], Loss: 2.3408\n",
      "Epoch [1238/10000], Loss: 2.3404\n",
      "Epoch [1239/10000], Loss: 2.3401\n",
      "Epoch [1240/10000], Loss: 2.3397\n",
      "Epoch [1241/10000], Loss: 2.3393\n",
      "Epoch [1242/10000], Loss: 2.3389\n",
      "Epoch [1243/10000], Loss: 2.3386\n",
      "Epoch [1244/10000], Loss: 2.3382\n",
      "Epoch [1245/10000], Loss: 2.3378\n",
      "Epoch [1246/10000], Loss: 2.3374\n",
      "Epoch [1247/10000], Loss: 2.3370\n",
      "Epoch [1248/10000], Loss: 2.3367\n",
      "Epoch [1249/10000], Loss: 2.3363\n",
      "Epoch [1250/10000], Loss: 2.3359\n",
      "Epoch [1251/10000], Loss: 2.3355\n",
      "Epoch [1252/10000], Loss: 2.3351\n",
      "Epoch [1253/10000], Loss: 2.3348\n",
      "Epoch [1254/10000], Loss: 2.3344\n",
      "Epoch [1255/10000], Loss: 2.3340\n",
      "Epoch [1256/10000], Loss: 2.3336\n",
      "Epoch [1257/10000], Loss: 2.3333\n",
      "Epoch [1258/10000], Loss: 2.3329\n",
      "Epoch [1259/10000], Loss: 2.3325\n",
      "Epoch [1260/10000], Loss: 2.3321\n",
      "Epoch [1261/10000], Loss: 2.3317\n",
      "Epoch [1262/10000], Loss: 2.3314\n",
      "Epoch [1263/10000], Loss: 2.3310\n",
      "Epoch [1264/10000], Loss: 2.3306\n",
      "Epoch [1265/10000], Loss: 2.3302\n",
      "Epoch [1266/10000], Loss: 2.3299\n",
      "Epoch [1267/10000], Loss: 2.3295\n",
      "Epoch [1268/10000], Loss: 2.3291\n",
      "Epoch [1269/10000], Loss: 2.3287\n",
      "Epoch [1270/10000], Loss: 2.3284\n",
      "Epoch [1271/10000], Loss: 2.3280\n",
      "Epoch [1272/10000], Loss: 2.3276\n",
      "Epoch [1273/10000], Loss: 2.3272\n",
      "Epoch [1274/10000], Loss: 2.3269\n",
      "Epoch [1275/10000], Loss: 2.3265\n",
      "Epoch [1276/10000], Loss: 2.3261\n",
      "Epoch [1277/10000], Loss: 2.3257\n",
      "Epoch [1278/10000], Loss: 2.3254\n",
      "Epoch [1279/10000], Loss: 2.3250\n",
      "Epoch [1280/10000], Loss: 2.3246\n",
      "Epoch [1281/10000], Loss: 2.3242\n",
      "Epoch [1282/10000], Loss: 2.3239\n",
      "Epoch [1283/10000], Loss: 2.3235\n",
      "Epoch [1284/10000], Loss: 2.3231\n",
      "Epoch [1285/10000], Loss: 2.3227\n",
      "Epoch [1286/10000], Loss: 2.3224\n",
      "Epoch [1287/10000], Loss: 2.3220\n",
      "Epoch [1288/10000], Loss: 2.3216\n",
      "Epoch [1289/10000], Loss: 2.3212\n",
      "Epoch [1290/10000], Loss: 2.3209\n",
      "Epoch [1291/10000], Loss: 2.3205\n",
      "Epoch [1292/10000], Loss: 2.3201\n",
      "Epoch [1293/10000], Loss: 2.3198\n",
      "Epoch [1294/10000], Loss: 2.3194\n",
      "Epoch [1295/10000], Loss: 2.3190\n",
      "Epoch [1296/10000], Loss: 2.3186\n",
      "Epoch [1297/10000], Loss: 2.3183\n",
      "Epoch [1298/10000], Loss: 2.3179\n",
      "Epoch [1299/10000], Loss: 2.3175\n",
      "Epoch [1300/10000], Loss: 2.3172\n",
      "Epoch [1301/10000], Loss: 2.3168\n",
      "Epoch [1302/10000], Loss: 2.3164\n",
      "Epoch [1303/10000], Loss: 2.3160\n",
      "Epoch [1304/10000], Loss: 2.3157\n",
      "Epoch [1305/10000], Loss: 2.3153\n",
      "Epoch [1306/10000], Loss: 2.3149\n",
      "Epoch [1307/10000], Loss: 2.3146\n",
      "Epoch [1308/10000], Loss: 2.3142\n",
      "Epoch [1309/10000], Loss: 2.3138\n",
      "Epoch [1310/10000], Loss: 2.3134\n",
      "Epoch [1311/10000], Loss: 2.3131\n",
      "Epoch [1312/10000], Loss: 2.3127\n",
      "Epoch [1313/10000], Loss: 2.3123\n",
      "Epoch [1314/10000], Loss: 2.3120\n",
      "Epoch [1315/10000], Loss: 2.3116\n",
      "Epoch [1316/10000], Loss: 2.3112\n",
      "Epoch [1317/10000], Loss: 2.3109\n",
      "Epoch [1318/10000], Loss: 2.3105\n",
      "Epoch [1319/10000], Loss: 2.3101\n",
      "Epoch [1320/10000], Loss: 2.3098\n",
      "Epoch [1321/10000], Loss: 2.3094\n",
      "Epoch [1322/10000], Loss: 2.3090\n",
      "Epoch [1323/10000], Loss: 2.3086\n",
      "Epoch [1324/10000], Loss: 2.3083\n",
      "Epoch [1325/10000], Loss: 2.3079\n",
      "Epoch [1326/10000], Loss: 2.3075\n",
      "Epoch [1327/10000], Loss: 2.3072\n",
      "Epoch [1328/10000], Loss: 2.3068\n",
      "Epoch [1329/10000], Loss: 2.3064\n",
      "Epoch [1330/10000], Loss: 2.3061\n",
      "Epoch [1331/10000], Loss: 2.3057\n",
      "Epoch [1332/10000], Loss: 2.3053\n",
      "Epoch [1333/10000], Loss: 2.3050\n",
      "Epoch [1334/10000], Loss: 2.3046\n",
      "Epoch [1335/10000], Loss: 2.3042\n",
      "Epoch [1336/10000], Loss: 2.3039\n",
      "Epoch [1337/10000], Loss: 2.3035\n",
      "Epoch [1338/10000], Loss: 2.3031\n",
      "Epoch [1339/10000], Loss: 2.3028\n",
      "Epoch [1340/10000], Loss: 2.3024\n",
      "Epoch [1341/10000], Loss: 2.3020\n",
      "Epoch [1342/10000], Loss: 2.3017\n",
      "Epoch [1343/10000], Loss: 2.3013\n",
      "Epoch [1344/10000], Loss: 2.3009\n",
      "Epoch [1345/10000], Loss: 2.3006\n",
      "Epoch [1346/10000], Loss: 2.3002\n",
      "Epoch [1347/10000], Loss: 2.2999\n",
      "Epoch [1348/10000], Loss: 2.2995\n",
      "Epoch [1349/10000], Loss: 2.2991\n",
      "Epoch [1350/10000], Loss: 2.2988\n",
      "Epoch [1351/10000], Loss: 2.2984\n",
      "Epoch [1352/10000], Loss: 2.2980\n",
      "Epoch [1353/10000], Loss: 2.2977\n",
      "Epoch [1354/10000], Loss: 2.2973\n",
      "Epoch [1355/10000], Loss: 2.2969\n",
      "Epoch [1356/10000], Loss: 2.2966\n",
      "Epoch [1357/10000], Loss: 2.2962\n",
      "Epoch [1358/10000], Loss: 2.2958\n",
      "Epoch [1359/10000], Loss: 2.2955\n",
      "Epoch [1360/10000], Loss: 2.2951\n",
      "Epoch [1361/10000], Loss: 2.2948\n",
      "Epoch [1362/10000], Loss: 2.2944\n",
      "Epoch [1363/10000], Loss: 2.2940\n",
      "Epoch [1364/10000], Loss: 2.2937\n",
      "Epoch [1365/10000], Loss: 2.2933\n",
      "Epoch [1366/10000], Loss: 2.2929\n",
      "Epoch [1367/10000], Loss: 2.2926\n",
      "Epoch [1368/10000], Loss: 2.2922\n",
      "Epoch [1369/10000], Loss: 2.2919\n",
      "Epoch [1370/10000], Loss: 2.2915\n",
      "Epoch [1371/10000], Loss: 2.2911\n",
      "Epoch [1372/10000], Loss: 2.2908\n",
      "Epoch [1373/10000], Loss: 2.2904\n",
      "Epoch [1374/10000], Loss: 2.2900\n",
      "Epoch [1375/10000], Loss: 2.2897\n",
      "Epoch [1376/10000], Loss: 2.2893\n",
      "Epoch [1377/10000], Loss: 2.2890\n",
      "Epoch [1378/10000], Loss: 2.2886\n",
      "Epoch [1379/10000], Loss: 2.2882\n",
      "Epoch [1380/10000], Loss: 2.2879\n",
      "Epoch [1381/10000], Loss: 2.2875\n",
      "Epoch [1382/10000], Loss: 2.2872\n",
      "Epoch [1383/10000], Loss: 2.2868\n",
      "Epoch [1384/10000], Loss: 2.2864\n",
      "Epoch [1385/10000], Loss: 2.2861\n",
      "Epoch [1386/10000], Loss: 2.2857\n",
      "Epoch [1387/10000], Loss: 2.2854\n",
      "Epoch [1388/10000], Loss: 2.2850\n",
      "Epoch [1389/10000], Loss: 2.2846\n",
      "Epoch [1390/10000], Loss: 2.2843\n",
      "Epoch [1391/10000], Loss: 2.2839\n",
      "Epoch [1392/10000], Loss: 2.2836\n",
      "Epoch [1393/10000], Loss: 2.2832\n",
      "Epoch [1394/10000], Loss: 2.2828\n",
      "Epoch [1395/10000], Loss: 2.2825\n",
      "Epoch [1396/10000], Loss: 2.2821\n",
      "Epoch [1397/10000], Loss: 2.2818\n",
      "Epoch [1398/10000], Loss: 2.2814\n",
      "Epoch [1399/10000], Loss: 2.2811\n",
      "Epoch [1400/10000], Loss: 2.2807\n",
      "Epoch [1401/10000], Loss: 2.2803\n",
      "Epoch [1402/10000], Loss: 2.2800\n",
      "Epoch [1403/10000], Loss: 2.2796\n",
      "Epoch [1404/10000], Loss: 2.2793\n",
      "Epoch [1405/10000], Loss: 2.2789\n",
      "Epoch [1406/10000], Loss: 2.2786\n",
      "Epoch [1407/10000], Loss: 2.2782\n",
      "Epoch [1408/10000], Loss: 2.2778\n",
      "Epoch [1409/10000], Loss: 2.2775\n",
      "Epoch [1410/10000], Loss: 2.2771\n",
      "Epoch [1411/10000], Loss: 2.2768\n",
      "Epoch [1412/10000], Loss: 2.2764\n",
      "Epoch [1413/10000], Loss: 2.2761\n",
      "Epoch [1414/10000], Loss: 2.2757\n",
      "Epoch [1415/10000], Loss: 2.2753\n",
      "Epoch [1416/10000], Loss: 2.2750\n",
      "Epoch [1417/10000], Loss: 2.2746\n",
      "Epoch [1418/10000], Loss: 2.2743\n",
      "Epoch [1419/10000], Loss: 2.2739\n",
      "Epoch [1420/10000], Loss: 2.2736\n",
      "Epoch [1421/10000], Loss: 2.2732\n",
      "Epoch [1422/10000], Loss: 2.2729\n",
      "Epoch [1423/10000], Loss: 2.2725\n",
      "Epoch [1424/10000], Loss: 2.2721\n",
      "Epoch [1425/10000], Loss: 2.2718\n",
      "Epoch [1426/10000], Loss: 2.2714\n",
      "Epoch [1427/10000], Loss: 2.2711\n",
      "Epoch [1428/10000], Loss: 2.2707\n",
      "Epoch [1429/10000], Loss: 2.2704\n",
      "Epoch [1430/10000], Loss: 2.2700\n",
      "Epoch [1431/10000], Loss: 2.2697\n",
      "Epoch [1432/10000], Loss: 2.2693\n",
      "Epoch [1433/10000], Loss: 2.2690\n",
      "Epoch [1434/10000], Loss: 2.2686\n",
      "Epoch [1435/10000], Loss: 2.2682\n",
      "Epoch [1436/10000], Loss: 2.2679\n",
      "Epoch [1437/10000], Loss: 2.2675\n",
      "Epoch [1438/10000], Loss: 2.2672\n",
      "Epoch [1439/10000], Loss: 2.2668\n",
      "Epoch [1440/10000], Loss: 2.2665\n",
      "Epoch [1441/10000], Loss: 2.2661\n",
      "Epoch [1442/10000], Loss: 2.2658\n",
      "Epoch [1443/10000], Loss: 2.2654\n",
      "Epoch [1444/10000], Loss: 2.2651\n",
      "Epoch [1445/10000], Loss: 2.2647\n",
      "Epoch [1446/10000], Loss: 2.2644\n",
      "Epoch [1447/10000], Loss: 2.2640\n",
      "Epoch [1448/10000], Loss: 2.2637\n",
      "Epoch [1449/10000], Loss: 2.2633\n",
      "Epoch [1450/10000], Loss: 2.2630\n",
      "Epoch [1451/10000], Loss: 2.2626\n",
      "Epoch [1452/10000], Loss: 2.2623\n",
      "Epoch [1453/10000], Loss: 2.2619\n",
      "Epoch [1454/10000], Loss: 2.2616\n",
      "Epoch [1455/10000], Loss: 2.2612\n",
      "Epoch [1456/10000], Loss: 2.2608\n",
      "Epoch [1457/10000], Loss: 2.2605\n",
      "Epoch [1458/10000], Loss: 2.2601\n",
      "Epoch [1459/10000], Loss: 2.2598\n",
      "Epoch [1460/10000], Loss: 2.2594\n",
      "Epoch [1461/10000], Loss: 2.2591\n",
      "Epoch [1462/10000], Loss: 2.2587\n",
      "Epoch [1463/10000], Loss: 2.2584\n",
      "Epoch [1464/10000], Loss: 2.2580\n",
      "Epoch [1465/10000], Loss: 2.2577\n",
      "Epoch [1466/10000], Loss: 2.2573\n",
      "Epoch [1467/10000], Loss: 2.2570\n",
      "Epoch [1468/10000], Loss: 2.2566\n",
      "Epoch [1469/10000], Loss: 2.2563\n",
      "Epoch [1470/10000], Loss: 2.2559\n",
      "Epoch [1471/10000], Loss: 2.2556\n",
      "Epoch [1472/10000], Loss: 2.2552\n",
      "Epoch [1473/10000], Loss: 2.2549\n",
      "Epoch [1474/10000], Loss: 2.2546\n",
      "Epoch [1475/10000], Loss: 2.2542\n",
      "Epoch [1476/10000], Loss: 2.2539\n",
      "Epoch [1477/10000], Loss: 2.2535\n",
      "Epoch [1478/10000], Loss: 2.2532\n",
      "Epoch [1479/10000], Loss: 2.2528\n",
      "Epoch [1480/10000], Loss: 2.2525\n",
      "Epoch [1481/10000], Loss: 2.2521\n",
      "Epoch [1482/10000], Loss: 2.2518\n",
      "Epoch [1483/10000], Loss: 2.2514\n",
      "Epoch [1484/10000], Loss: 2.2511\n",
      "Epoch [1485/10000], Loss: 2.2507\n",
      "Epoch [1486/10000], Loss: 2.2504\n",
      "Epoch [1487/10000], Loss: 2.2500\n",
      "Epoch [1488/10000], Loss: 2.2497\n",
      "Epoch [1489/10000], Loss: 2.2493\n",
      "Epoch [1490/10000], Loss: 2.2490\n",
      "Epoch [1491/10000], Loss: 2.2486\n",
      "Epoch [1492/10000], Loss: 2.2483\n",
      "Epoch [1493/10000], Loss: 2.2479\n",
      "Epoch [1494/10000], Loss: 2.2476\n",
      "Epoch [1495/10000], Loss: 2.2473\n",
      "Epoch [1496/10000], Loss: 2.2469\n",
      "Epoch [1497/10000], Loss: 2.2466\n",
      "Epoch [1498/10000], Loss: 2.2462\n",
      "Epoch [1499/10000], Loss: 2.2459\n",
      "Epoch [1500/10000], Loss: 2.2455\n",
      "Epoch [1501/10000], Loss: 2.2452\n",
      "Epoch [1502/10000], Loss: 2.2448\n",
      "Epoch [1503/10000], Loss: 2.2445\n",
      "Epoch [1504/10000], Loss: 2.2441\n",
      "Epoch [1505/10000], Loss: 2.2438\n",
      "Epoch [1506/10000], Loss: 2.2434\n",
      "Epoch [1507/10000], Loss: 2.2431\n",
      "Epoch [1508/10000], Loss: 2.2428\n",
      "Epoch [1509/10000], Loss: 2.2424\n",
      "Epoch [1510/10000], Loss: 2.2421\n",
      "Epoch [1511/10000], Loss: 2.2417\n",
      "Epoch [1512/10000], Loss: 2.2414\n",
      "Epoch [1513/10000], Loss: 2.2410\n",
      "Epoch [1514/10000], Loss: 2.2407\n",
      "Epoch [1515/10000], Loss: 2.2403\n",
      "Epoch [1516/10000], Loss: 2.2400\n",
      "Epoch [1517/10000], Loss: 2.2397\n",
      "Epoch [1518/10000], Loss: 2.2393\n",
      "Epoch [1519/10000], Loss: 2.2390\n",
      "Epoch [1520/10000], Loss: 2.2386\n",
      "Epoch [1521/10000], Loss: 2.2383\n",
      "Epoch [1522/10000], Loss: 2.2379\n",
      "Epoch [1523/10000], Loss: 2.2376\n",
      "Epoch [1524/10000], Loss: 2.2373\n",
      "Epoch [1525/10000], Loss: 2.2369\n",
      "Epoch [1526/10000], Loss: 2.2366\n",
      "Epoch [1527/10000], Loss: 2.2362\n",
      "Epoch [1528/10000], Loss: 2.2359\n",
      "Epoch [1529/10000], Loss: 2.2355\n",
      "Epoch [1530/10000], Loss: 2.2352\n",
      "Epoch [1531/10000], Loss: 2.2349\n",
      "Epoch [1532/10000], Loss: 2.2345\n",
      "Epoch [1533/10000], Loss: 2.2342\n",
      "Epoch [1534/10000], Loss: 2.2338\n",
      "Epoch [1535/10000], Loss: 2.2335\n",
      "Epoch [1536/10000], Loss: 2.2332\n",
      "Epoch [1537/10000], Loss: 2.2328\n",
      "Epoch [1538/10000], Loss: 2.2325\n",
      "Epoch [1539/10000], Loss: 2.2321\n",
      "Epoch [1540/10000], Loss: 2.2318\n",
      "Epoch [1541/10000], Loss: 2.2314\n",
      "Epoch [1542/10000], Loss: 2.2311\n",
      "Epoch [1543/10000], Loss: 2.2308\n",
      "Epoch [1544/10000], Loss: 2.2304\n",
      "Epoch [1545/10000], Loss: 2.2301\n",
      "Epoch [1546/10000], Loss: 2.2297\n",
      "Epoch [1547/10000], Loss: 2.2294\n",
      "Epoch [1548/10000], Loss: 2.2291\n",
      "Epoch [1549/10000], Loss: 2.2287\n",
      "Epoch [1550/10000], Loss: 2.2284\n",
      "Epoch [1551/10000], Loss: 2.2280\n",
      "Epoch [1552/10000], Loss: 2.2277\n",
      "Epoch [1553/10000], Loss: 2.2274\n",
      "Epoch [1554/10000], Loss: 2.2270\n",
      "Epoch [1555/10000], Loss: 2.2267\n",
      "Epoch [1556/10000], Loss: 2.2263\n",
      "Epoch [1557/10000], Loss: 2.2260\n",
      "Epoch [1558/10000], Loss: 2.2257\n",
      "Epoch [1559/10000], Loss: 2.2253\n",
      "Epoch [1560/10000], Loss: 2.2250\n",
      "Epoch [1561/10000], Loss: 2.2246\n",
      "Epoch [1562/10000], Loss: 2.2243\n",
      "Epoch [1563/10000], Loss: 2.2240\n",
      "Epoch [1564/10000], Loss: 2.2236\n",
      "Epoch [1565/10000], Loss: 2.2233\n",
      "Epoch [1566/10000], Loss: 2.2230\n",
      "Epoch [1567/10000], Loss: 2.2226\n",
      "Epoch [1568/10000], Loss: 2.2223\n",
      "Epoch [1569/10000], Loss: 2.2219\n",
      "Epoch [1570/10000], Loss: 2.2216\n",
      "Epoch [1571/10000], Loss: 2.2213\n",
      "Epoch [1572/10000], Loss: 2.2209\n",
      "Epoch [1573/10000], Loss: 2.2206\n",
      "Epoch [1574/10000], Loss: 2.2203\n",
      "Epoch [1575/10000], Loss: 2.2199\n",
      "Epoch [1576/10000], Loss: 2.2196\n",
      "Epoch [1577/10000], Loss: 2.2192\n",
      "Epoch [1578/10000], Loss: 2.2189\n",
      "Epoch [1579/10000], Loss: 2.2186\n",
      "Epoch [1580/10000], Loss: 2.2182\n",
      "Epoch [1581/10000], Loss: 2.2179\n",
      "Epoch [1582/10000], Loss: 2.2176\n",
      "Epoch [1583/10000], Loss: 2.2172\n",
      "Epoch [1584/10000], Loss: 2.2169\n",
      "Epoch [1585/10000], Loss: 2.2165\n",
      "Epoch [1586/10000], Loss: 2.2162\n",
      "Epoch [1587/10000], Loss: 2.2159\n",
      "Epoch [1588/10000], Loss: 2.2155\n",
      "Epoch [1589/10000], Loss: 2.2152\n",
      "Epoch [1590/10000], Loss: 2.2149\n",
      "Epoch [1591/10000], Loss: 2.2145\n",
      "Epoch [1592/10000], Loss: 2.2142\n",
      "Epoch [1593/10000], Loss: 2.2139\n",
      "Epoch [1594/10000], Loss: 2.2135\n",
      "Epoch [1595/10000], Loss: 2.2132\n",
      "Epoch [1596/10000], Loss: 2.2129\n",
      "Epoch [1597/10000], Loss: 2.2125\n",
      "Epoch [1598/10000], Loss: 2.2122\n",
      "Epoch [1599/10000], Loss: 2.2119\n",
      "Epoch [1600/10000], Loss: 2.2115\n",
      "Epoch [1601/10000], Loss: 2.2112\n",
      "Epoch [1602/10000], Loss: 2.2108\n",
      "Epoch [1603/10000], Loss: 2.2105\n",
      "Epoch [1604/10000], Loss: 2.2102\n",
      "Epoch [1605/10000], Loss: 2.2098\n",
      "Epoch [1606/10000], Loss: 2.2095\n",
      "Epoch [1607/10000], Loss: 2.2092\n",
      "Epoch [1608/10000], Loss: 2.2088\n",
      "Epoch [1609/10000], Loss: 2.2085\n",
      "Epoch [1610/10000], Loss: 2.2082\n",
      "Epoch [1611/10000], Loss: 2.2078\n",
      "Epoch [1612/10000], Loss: 2.2075\n",
      "Epoch [1613/10000], Loss: 2.2072\n",
      "Epoch [1614/10000], Loss: 2.2068\n",
      "Epoch [1615/10000], Loss: 2.2065\n",
      "Epoch [1616/10000], Loss: 2.2062\n",
      "Epoch [1617/10000], Loss: 2.2058\n",
      "Epoch [1618/10000], Loss: 2.2055\n",
      "Epoch [1619/10000], Loss: 2.2052\n",
      "Epoch [1620/10000], Loss: 2.2048\n",
      "Epoch [1621/10000], Loss: 2.2045\n",
      "Epoch [1622/10000], Loss: 2.2042\n",
      "Epoch [1623/10000], Loss: 2.2039\n",
      "Epoch [1624/10000], Loss: 2.2035\n",
      "Epoch [1625/10000], Loss: 2.2032\n",
      "Epoch [1626/10000], Loss: 2.2029\n",
      "Epoch [1627/10000], Loss: 2.2025\n",
      "Epoch [1628/10000], Loss: 2.2022\n",
      "Epoch [1629/10000], Loss: 2.2019\n",
      "Epoch [1630/10000], Loss: 2.2015\n",
      "Epoch [1631/10000], Loss: 2.2012\n",
      "Epoch [1632/10000], Loss: 2.2009\n",
      "Epoch [1633/10000], Loss: 2.2005\n",
      "Epoch [1634/10000], Loss: 2.2002\n",
      "Epoch [1635/10000], Loss: 2.1999\n",
      "Epoch [1636/10000], Loss: 2.1995\n",
      "Epoch [1637/10000], Loss: 2.1992\n",
      "Epoch [1638/10000], Loss: 2.1989\n",
      "Epoch [1639/10000], Loss: 2.1986\n",
      "Epoch [1640/10000], Loss: 2.1982\n",
      "Epoch [1641/10000], Loss: 2.1979\n",
      "Epoch [1642/10000], Loss: 2.1976\n",
      "Epoch [1643/10000], Loss: 2.1972\n",
      "Epoch [1644/10000], Loss: 2.1969\n",
      "Epoch [1645/10000], Loss: 2.1966\n",
      "Epoch [1646/10000], Loss: 2.1962\n",
      "Epoch [1647/10000], Loss: 2.1959\n",
      "Epoch [1648/10000], Loss: 2.1956\n",
      "Epoch [1649/10000], Loss: 2.1953\n",
      "Epoch [1650/10000], Loss: 2.1949\n",
      "Epoch [1651/10000], Loss: 2.1946\n",
      "Epoch [1652/10000], Loss: 2.1943\n",
      "Epoch [1653/10000], Loss: 2.1939\n",
      "Epoch [1654/10000], Loss: 2.1936\n",
      "Epoch [1655/10000], Loss: 2.1933\n",
      "Epoch [1656/10000], Loss: 2.1930\n",
      "Epoch [1657/10000], Loss: 2.1926\n",
      "Epoch [1658/10000], Loss: 2.1923\n",
      "Epoch [1659/10000], Loss: 2.1920\n",
      "Epoch [1660/10000], Loss: 2.1916\n",
      "Epoch [1661/10000], Loss: 2.1913\n",
      "Epoch [1662/10000], Loss: 2.1910\n",
      "Epoch [1663/10000], Loss: 2.1907\n",
      "Epoch [1664/10000], Loss: 2.1903\n",
      "Epoch [1665/10000], Loss: 2.1901\n",
      "Epoch [1666/10000], Loss: 2.1899\n",
      "Epoch [1667/10000], Loss: 2.1897\n",
      "Epoch [1668/10000], Loss: 2.1895\n",
      "Epoch [1669/10000], Loss: 2.1893\n",
      "Epoch [1670/10000], Loss: 2.1891\n",
      "Epoch [1671/10000], Loss: 2.1889\n",
      "Epoch [1672/10000], Loss: 2.1887\n",
      "Epoch [1673/10000], Loss: 2.1885\n",
      "Epoch [1674/10000], Loss: 2.1883\n",
      "Epoch [1675/10000], Loss: 2.1880\n",
      "Epoch [1676/10000], Loss: 2.1878\n",
      "Epoch [1677/10000], Loss: 2.1876\n",
      "Epoch [1678/10000], Loss: 2.1874\n",
      "Epoch [1679/10000], Loss: 2.1872\n",
      "Epoch [1680/10000], Loss: 2.1870\n",
      "Epoch [1681/10000], Loss: 2.1868\n",
      "Epoch [1682/10000], Loss: 2.1866\n",
      "Epoch [1683/10000], Loss: 2.1864\n",
      "Epoch [1684/10000], Loss: 2.1862\n",
      "Epoch [1685/10000], Loss: 2.1860\n",
      "Epoch [1686/10000], Loss: 2.1857\n",
      "Epoch [1687/10000], Loss: 2.1855\n",
      "Epoch [1688/10000], Loss: 2.1853\n",
      "Epoch [1689/10000], Loss: 2.1851\n",
      "Epoch [1690/10000], Loss: 2.1849\n",
      "Epoch [1691/10000], Loss: 2.1847\n",
      "Epoch [1692/10000], Loss: 2.1845\n",
      "Epoch [1693/10000], Loss: 2.1843\n",
      "Epoch [1694/10000], Loss: 2.1841\n",
      "Epoch [1695/10000], Loss: 2.1839\n",
      "Epoch [1696/10000], Loss: 2.1837\n",
      "Epoch [1697/10000], Loss: 2.1835\n",
      "Epoch [1698/10000], Loss: 2.1832\n",
      "Epoch [1699/10000], Loss: 2.1830\n",
      "Epoch [1700/10000], Loss: 2.1828\n",
      "Epoch [1701/10000], Loss: 2.1826\n",
      "Epoch [1702/10000], Loss: 2.1824\n",
      "Epoch [1703/10000], Loss: 2.1822\n",
      "Epoch [1704/10000], Loss: 2.1820\n",
      "Epoch [1705/10000], Loss: 2.1818\n",
      "Epoch [1706/10000], Loss: 2.1816\n",
      "Epoch [1707/10000], Loss: 2.1814\n",
      "Epoch [1708/10000], Loss: 2.1812\n",
      "Epoch [1709/10000], Loss: 2.1810\n",
      "Epoch [1710/10000], Loss: 2.1808\n",
      "Epoch [1711/10000], Loss: 2.1805\n",
      "Epoch [1712/10000], Loss: 2.1803\n",
      "Epoch [1713/10000], Loss: 2.1801\n",
      "Epoch [1714/10000], Loss: 2.1799\n",
      "Epoch [1715/10000], Loss: 2.1797\n",
      "Epoch [1716/10000], Loss: 2.1795\n",
      "Epoch [1717/10000], Loss: 2.1793\n",
      "Epoch [1718/10000], Loss: 2.1791\n",
      "Epoch [1719/10000], Loss: 2.1789\n",
      "Epoch [1720/10000], Loss: 2.1787\n",
      "Epoch [1721/10000], Loss: 2.1785\n",
      "Epoch [1722/10000], Loss: 2.1783\n",
      "Epoch [1723/10000], Loss: 2.1781\n",
      "Epoch [1724/10000], Loss: 2.1779\n",
      "Epoch [1725/10000], Loss: 2.1776\n",
      "Epoch [1726/10000], Loss: 2.1774\n",
      "Epoch [1727/10000], Loss: 2.1772\n",
      "Epoch [1728/10000], Loss: 2.1770\n",
      "Epoch [1729/10000], Loss: 2.1768\n",
      "Epoch [1730/10000], Loss: 2.1766\n",
      "Epoch [1731/10000], Loss: 2.1764\n",
      "Epoch [1732/10000], Loss: 2.1762\n",
      "Epoch [1733/10000], Loss: 2.1760\n",
      "Epoch [1734/10000], Loss: 2.1758\n",
      "Epoch [1735/10000], Loss: 2.1756\n",
      "Epoch [1736/10000], Loss: 2.1754\n",
      "Epoch [1737/10000], Loss: 2.1752\n",
      "Epoch [1738/10000], Loss: 2.1750\n",
      "Epoch [1739/10000], Loss: 2.1748\n",
      "Epoch [1740/10000], Loss: 2.1746\n",
      "Epoch [1741/10000], Loss: 2.1743\n",
      "Epoch [1742/10000], Loss: 2.1741\n",
      "Epoch [1743/10000], Loss: 2.1739\n",
      "Epoch [1744/10000], Loss: 2.1737\n",
      "Epoch [1745/10000], Loss: 2.1735\n",
      "Epoch [1746/10000], Loss: 2.1733\n",
      "Epoch [1747/10000], Loss: 2.1731\n",
      "Epoch [1748/10000], Loss: 2.1729\n",
      "Epoch [1749/10000], Loss: 2.1727\n",
      "Epoch [1750/10000], Loss: 2.1725\n",
      "Epoch [1751/10000], Loss: 2.1723\n",
      "Epoch [1752/10000], Loss: 2.1721\n",
      "Epoch [1753/10000], Loss: 2.1719\n",
      "Epoch [1754/10000], Loss: 2.1717\n",
      "Epoch [1755/10000], Loss: 2.1715\n",
      "Epoch [1756/10000], Loss: 2.1713\n",
      "Epoch [1757/10000], Loss: 2.1711\n",
      "Epoch [1758/10000], Loss: 2.1709\n",
      "Epoch [1759/10000], Loss: 2.1706\n",
      "Epoch [1760/10000], Loss: 2.1704\n",
      "Epoch [1761/10000], Loss: 2.1702\n",
      "Epoch [1762/10000], Loss: 2.1700\n",
      "Epoch [1763/10000], Loss: 2.1698\n",
      "Epoch [1764/10000], Loss: 2.1696\n",
      "Epoch [1765/10000], Loss: 2.1694\n",
      "Epoch [1766/10000], Loss: 2.1692\n",
      "Epoch [1767/10000], Loss: 2.1690\n",
      "Epoch [1768/10000], Loss: 2.1688\n",
      "Epoch [1769/10000], Loss: 2.1686\n",
      "Epoch [1770/10000], Loss: 2.1684\n",
      "Epoch [1771/10000], Loss: 2.1682\n",
      "Epoch [1772/10000], Loss: 2.1680\n",
      "Epoch [1773/10000], Loss: 2.1678\n",
      "Epoch [1774/10000], Loss: 2.1676\n",
      "Epoch [1775/10000], Loss: 2.1674\n",
      "Epoch [1776/10000], Loss: 2.1672\n",
      "Epoch [1777/10000], Loss: 2.1670\n",
      "Epoch [1778/10000], Loss: 2.1668\n",
      "Epoch [1779/10000], Loss: 2.1665\n",
      "Epoch [1780/10000], Loss: 2.1663\n",
      "Epoch [1781/10000], Loss: 2.1661\n",
      "Epoch [1782/10000], Loss: 2.1659\n",
      "Epoch [1783/10000], Loss: 2.1657\n",
      "Epoch [1784/10000], Loss: 2.1655\n",
      "Epoch [1785/10000], Loss: 2.1653\n",
      "Epoch [1786/10000], Loss: 2.1651\n",
      "Epoch [1787/10000], Loss: 2.1649\n",
      "Epoch [1788/10000], Loss: 2.1647\n",
      "Epoch [1789/10000], Loss: 2.1645\n",
      "Epoch [1790/10000], Loss: 2.1643\n",
      "Epoch [1791/10000], Loss: 2.1641\n",
      "Epoch [1792/10000], Loss: 2.1639\n",
      "Epoch [1793/10000], Loss: 2.1637\n",
      "Epoch [1794/10000], Loss: 2.1635\n",
      "Epoch [1795/10000], Loss: 2.1633\n",
      "Epoch [1796/10000], Loss: 2.1631\n",
      "Epoch [1797/10000], Loss: 2.1629\n",
      "Epoch [1798/10000], Loss: 2.1627\n",
      "Epoch [1799/10000], Loss: 2.1625\n",
      "Epoch [1800/10000], Loss: 2.1623\n",
      "Epoch [1801/10000], Loss: 2.1621\n",
      "Epoch [1802/10000], Loss: 2.1619\n",
      "Epoch [1803/10000], Loss: 2.1617\n",
      "Epoch [1804/10000], Loss: 2.1615\n",
      "Epoch [1805/10000], Loss: 2.1613\n",
      "Epoch [1806/10000], Loss: 2.1610\n",
      "Epoch [1807/10000], Loss: 2.1608\n",
      "Epoch [1808/10000], Loss: 2.1606\n",
      "Epoch [1809/10000], Loss: 2.1604\n",
      "Epoch [1810/10000], Loss: 2.1602\n",
      "Epoch [1811/10000], Loss: 2.1600\n",
      "Epoch [1812/10000], Loss: 2.1598\n",
      "Epoch [1813/10000], Loss: 2.1596\n",
      "Epoch [1814/10000], Loss: 2.1594\n",
      "Epoch [1815/10000], Loss: 2.1592\n",
      "Epoch [1816/10000], Loss: 2.1590\n",
      "Epoch [1817/10000], Loss: 2.1588\n",
      "Epoch [1818/10000], Loss: 2.1586\n",
      "Epoch [1819/10000], Loss: 2.1584\n",
      "Epoch [1820/10000], Loss: 2.1582\n",
      "Epoch [1821/10000], Loss: 2.1580\n",
      "Epoch [1822/10000], Loss: 2.1578\n",
      "Epoch [1823/10000], Loss: 2.1576\n",
      "Epoch [1824/10000], Loss: 2.1574\n",
      "Epoch [1825/10000], Loss: 2.1572\n",
      "Epoch [1826/10000], Loss: 2.1570\n",
      "Epoch [1827/10000], Loss: 2.1568\n",
      "Epoch [1828/10000], Loss: 2.1566\n",
      "Epoch [1829/10000], Loss: 2.1564\n",
      "Epoch [1830/10000], Loss: 2.1562\n",
      "Epoch [1831/10000], Loss: 2.1560\n",
      "Epoch [1832/10000], Loss: 2.1558\n",
      "Epoch [1833/10000], Loss: 2.1556\n",
      "Epoch [1834/10000], Loss: 2.1554\n",
      "Epoch [1835/10000], Loss: 2.1552\n",
      "Epoch [1836/10000], Loss: 2.1550\n",
      "Epoch [1837/10000], Loss: 2.1548\n",
      "Epoch [1838/10000], Loss: 2.1546\n",
      "Epoch [1839/10000], Loss: 2.1544\n",
      "Epoch [1840/10000], Loss: 2.1542\n",
      "Epoch [1841/10000], Loss: 2.1540\n",
      "Epoch [1842/10000], Loss: 2.1538\n",
      "Epoch [1843/10000], Loss: 2.1536\n",
      "Epoch [1844/10000], Loss: 2.1534\n",
      "Epoch [1845/10000], Loss: 2.1532\n",
      "Epoch [1846/10000], Loss: 2.1530\n",
      "Epoch [1847/10000], Loss: 2.1528\n",
      "Epoch [1848/10000], Loss: 2.1526\n",
      "Epoch [1849/10000], Loss: 2.1524\n",
      "Epoch [1850/10000], Loss: 2.1522\n",
      "Epoch [1851/10000], Loss: 2.1519\n",
      "Epoch [1852/10000], Loss: 2.1517\n",
      "Epoch [1853/10000], Loss: 2.1515\n",
      "Epoch [1854/10000], Loss: 2.1513\n",
      "Epoch [1855/10000], Loss: 2.1511\n",
      "Epoch [1856/10000], Loss: 2.1509\n",
      "Epoch [1857/10000], Loss: 2.1507\n",
      "Epoch [1858/10000], Loss: 2.1505\n",
      "Epoch [1859/10000], Loss: 2.1503\n",
      "Epoch [1860/10000], Loss: 2.1501\n",
      "Epoch [1861/10000], Loss: 2.1499\n",
      "Epoch [1862/10000], Loss: 2.1497\n",
      "Epoch [1863/10000], Loss: 2.1495\n",
      "Epoch [1864/10000], Loss: 2.1493\n",
      "Epoch [1865/10000], Loss: 2.1491\n",
      "Epoch [1866/10000], Loss: 2.1489\n",
      "Epoch [1867/10000], Loss: 2.1487\n",
      "Epoch [1868/10000], Loss: 2.1485\n",
      "Epoch [1869/10000], Loss: 2.1483\n",
      "Epoch [1870/10000], Loss: 2.1481\n",
      "Epoch [1871/10000], Loss: 2.1479\n",
      "Epoch [1872/10000], Loss: 2.1477\n",
      "Epoch [1873/10000], Loss: 2.1475\n",
      "Epoch [1874/10000], Loss: 2.1473\n",
      "Epoch [1875/10000], Loss: 2.1471\n",
      "Epoch [1876/10000], Loss: 2.1469\n",
      "Epoch [1877/10000], Loss: 2.1467\n",
      "Epoch [1878/10000], Loss: 2.1465\n",
      "Epoch [1879/10000], Loss: 2.1463\n",
      "Epoch [1880/10000], Loss: 2.1461\n",
      "Epoch [1881/10000], Loss: 2.1459\n",
      "Epoch [1882/10000], Loss: 2.1457\n",
      "Epoch [1883/10000], Loss: 2.1455\n",
      "Epoch [1884/10000], Loss: 2.1453\n",
      "Epoch [1885/10000], Loss: 2.1451\n",
      "Epoch [1886/10000], Loss: 2.1449\n",
      "Epoch [1887/10000], Loss: 2.1447\n",
      "Epoch [1888/10000], Loss: 2.1445\n",
      "Epoch [1889/10000], Loss: 2.1443\n",
      "Epoch [1890/10000], Loss: 2.1441\n",
      "Epoch [1891/10000], Loss: 2.1439\n",
      "Epoch [1892/10000], Loss: 2.1437\n",
      "Epoch [1893/10000], Loss: 2.1435\n",
      "Epoch [1894/10000], Loss: 2.1433\n",
      "Epoch [1895/10000], Loss: 2.1431\n",
      "Epoch [1896/10000], Loss: 2.1429\n",
      "Epoch [1897/10000], Loss: 2.1427\n",
      "Epoch [1898/10000], Loss: 2.1425\n",
      "Epoch [1899/10000], Loss: 2.1423\n",
      "Epoch [1900/10000], Loss: 2.1421\n",
      "Epoch [1901/10000], Loss: 2.1419\n",
      "Epoch [1902/10000], Loss: 2.1417\n",
      "Epoch [1903/10000], Loss: 2.1415\n",
      "Epoch [1904/10000], Loss: 2.1413\n",
      "Epoch [1905/10000], Loss: 2.1411\n",
      "Epoch [1906/10000], Loss: 2.1409\n",
      "Epoch [1907/10000], Loss: 2.1407\n",
      "Epoch [1908/10000], Loss: 2.1405\n",
      "Epoch [1909/10000], Loss: 2.1404\n",
      "Epoch [1910/10000], Loss: 2.1402\n",
      "Epoch [1911/10000], Loss: 2.1400\n",
      "Epoch [1912/10000], Loss: 2.1398\n",
      "Epoch [1913/10000], Loss: 2.1396\n",
      "Epoch [1914/10000], Loss: 2.1394\n",
      "Epoch [1915/10000], Loss: 2.1392\n",
      "Epoch [1916/10000], Loss: 2.1390\n",
      "Epoch [1917/10000], Loss: 2.1388\n",
      "Epoch [1918/10000], Loss: 2.1386\n",
      "Epoch [1919/10000], Loss: 2.1384\n",
      "Epoch [1920/10000], Loss: 2.1382\n",
      "Epoch [1921/10000], Loss: 2.1380\n",
      "Epoch [1922/10000], Loss: 2.1378\n",
      "Epoch [1923/10000], Loss: 2.1376\n",
      "Epoch [1924/10000], Loss: 2.1374\n",
      "Epoch [1925/10000], Loss: 2.1372\n",
      "Epoch [1926/10000], Loss: 2.1370\n",
      "Epoch [1927/10000], Loss: 2.1368\n",
      "Epoch [1928/10000], Loss: 2.1366\n",
      "Epoch [1929/10000], Loss: 2.1364\n",
      "Epoch [1930/10000], Loss: 2.1362\n",
      "Epoch [1931/10000], Loss: 2.1360\n",
      "Epoch [1932/10000], Loss: 2.1358\n",
      "Epoch [1933/10000], Loss: 2.1356\n",
      "Epoch [1934/10000], Loss: 2.1354\n",
      "Epoch [1935/10000], Loss: 2.1352\n",
      "Epoch [1936/10000], Loss: 2.1350\n",
      "Epoch [1937/10000], Loss: 2.1348\n",
      "Epoch [1938/10000], Loss: 2.1346\n",
      "Epoch [1939/10000], Loss: 2.1344\n",
      "Epoch [1940/10000], Loss: 2.1342\n",
      "Epoch [1941/10000], Loss: 2.1340\n",
      "Epoch [1942/10000], Loss: 2.1338\n",
      "Epoch [1943/10000], Loss: 2.1336\n",
      "Epoch [1944/10000], Loss: 2.1334\n",
      "Epoch [1945/10000], Loss: 2.1332\n",
      "Epoch [1946/10000], Loss: 2.1330\n",
      "Epoch [1947/10000], Loss: 2.1328\n",
      "Epoch [1948/10000], Loss: 2.1326\n",
      "Epoch [1949/10000], Loss: 2.1324\n",
      "Epoch [1950/10000], Loss: 2.1322\n",
      "Epoch [1951/10000], Loss: 2.1320\n",
      "Epoch [1952/10000], Loss: 2.1318\n",
      "Epoch [1953/10000], Loss: 2.1316\n",
      "Epoch [1954/10000], Loss: 2.1314\n",
      "Epoch [1955/10000], Loss: 2.1312\n",
      "Epoch [1956/10000], Loss: 2.1311\n",
      "Epoch [1957/10000], Loss: 2.1309\n",
      "Epoch [1958/10000], Loss: 2.1307\n",
      "Epoch [1959/10000], Loss: 2.1305\n",
      "Epoch [1960/10000], Loss: 2.1303\n",
      "Epoch [1961/10000], Loss: 2.1301\n",
      "Epoch [1962/10000], Loss: 2.1299\n",
      "Epoch [1963/10000], Loss: 2.1297\n",
      "Epoch [1964/10000], Loss: 2.1295\n",
      "Epoch [1965/10000], Loss: 2.1293\n",
      "Epoch [1966/10000], Loss: 2.1291\n",
      "Epoch [1967/10000], Loss: 2.1289\n",
      "Epoch [1968/10000], Loss: 2.1287\n",
      "Epoch [1969/10000], Loss: 2.1285\n",
      "Epoch [1970/10000], Loss: 2.1283\n",
      "Epoch [1971/10000], Loss: 2.1281\n",
      "Epoch [1972/10000], Loss: 2.1279\n",
      "Epoch [1973/10000], Loss: 2.1277\n",
      "Epoch [1974/10000], Loss: 2.1275\n",
      "Epoch [1975/10000], Loss: 2.1273\n",
      "Epoch [1976/10000], Loss: 2.1271\n",
      "Epoch [1977/10000], Loss: 2.1269\n",
      "Epoch [1978/10000], Loss: 2.1267\n",
      "Epoch [1979/10000], Loss: 2.1265\n",
      "Epoch [1980/10000], Loss: 2.1263\n",
      "Epoch [1981/10000], Loss: 2.1261\n",
      "Epoch [1982/10000], Loss: 2.1259\n",
      "Epoch [1983/10000], Loss: 2.1258\n",
      "Epoch [1984/10000], Loss: 2.1256\n",
      "Epoch [1985/10000], Loss: 2.1254\n",
      "Epoch [1986/10000], Loss: 2.1252\n",
      "Epoch [1987/10000], Loss: 2.1250\n",
      "Epoch [1988/10000], Loss: 2.1248\n",
      "Epoch [1989/10000], Loss: 2.1246\n",
      "Epoch [1990/10000], Loss: 2.1244\n",
      "Epoch [1991/10000], Loss: 2.1242\n",
      "Epoch [1992/10000], Loss: 2.1240\n",
      "Epoch [1993/10000], Loss: 2.1238\n",
      "Epoch [1994/10000], Loss: 2.1236\n",
      "Epoch [1995/10000], Loss: 2.1234\n",
      "Epoch [1996/10000], Loss: 2.1232\n",
      "Epoch [1997/10000], Loss: 2.1230\n",
      "Epoch [1998/10000], Loss: 2.1228\n",
      "Epoch [1999/10000], Loss: 2.1226\n",
      "Epoch [2000/10000], Loss: 2.1224\n",
      "Epoch [2001/10000], Loss: 2.1222\n",
      "Epoch [2002/10000], Loss: 2.1220\n",
      "Epoch [2003/10000], Loss: 2.1218\n",
      "Epoch [2004/10000], Loss: 2.1216\n",
      "Epoch [2005/10000], Loss: 2.1215\n",
      "Epoch [2006/10000], Loss: 2.1213\n",
      "Epoch [2007/10000], Loss: 2.1211\n",
      "Epoch [2008/10000], Loss: 2.1209\n",
      "Epoch [2009/10000], Loss: 2.1207\n",
      "Epoch [2010/10000], Loss: 2.1205\n",
      "Epoch [2011/10000], Loss: 2.1203\n",
      "Epoch [2012/10000], Loss: 2.1201\n",
      "Epoch [2013/10000], Loss: 2.1199\n",
      "Epoch [2014/10000], Loss: 2.1197\n",
      "Epoch [2015/10000], Loss: 2.1195\n",
      "Epoch [2016/10000], Loss: 2.1193\n",
      "Epoch [2017/10000], Loss: 2.1191\n",
      "Epoch [2018/10000], Loss: 2.1189\n",
      "Epoch [2019/10000], Loss: 2.1187\n",
      "Epoch [2020/10000], Loss: 2.1185\n",
      "Epoch [2021/10000], Loss: 2.1183\n",
      "Epoch [2022/10000], Loss: 2.1181\n",
      "Epoch [2023/10000], Loss: 2.1180\n",
      "Epoch [2024/10000], Loss: 2.1178\n",
      "Epoch [2025/10000], Loss: 2.1176\n",
      "Epoch [2026/10000], Loss: 2.1174\n",
      "Epoch [2027/10000], Loss: 2.1172\n",
      "Epoch [2028/10000], Loss: 2.1170\n",
      "Epoch [2029/10000], Loss: 2.1168\n",
      "Epoch [2030/10000], Loss: 2.1166\n",
      "Epoch [2031/10000], Loss: 2.1164\n",
      "Epoch [2032/10000], Loss: 2.1162\n",
      "Epoch [2033/10000], Loss: 2.1160\n",
      "Epoch [2034/10000], Loss: 2.1158\n",
      "Epoch [2035/10000], Loss: 2.1156\n",
      "Epoch [2036/10000], Loss: 2.1154\n",
      "Epoch [2037/10000], Loss: 2.1152\n",
      "Epoch [2038/10000], Loss: 2.1150\n",
      "Epoch [2039/10000], Loss: 2.1149\n",
      "Epoch [2040/10000], Loss: 2.1147\n",
      "Epoch [2041/10000], Loss: 2.1145\n",
      "Epoch [2042/10000], Loss: 2.1143\n",
      "Epoch [2043/10000], Loss: 2.1141\n",
      "Epoch [2044/10000], Loss: 2.1139\n",
      "Epoch [2045/10000], Loss: 2.1137\n",
      "Epoch [2046/10000], Loss: 2.1135\n",
      "Epoch [2047/10000], Loss: 2.1133\n",
      "Epoch [2048/10000], Loss: 2.1131\n",
      "Epoch [2049/10000], Loss: 2.1129\n",
      "Epoch [2050/10000], Loss: 2.1127\n",
      "Epoch [2051/10000], Loss: 2.1125\n",
      "Epoch [2052/10000], Loss: 2.1123\n",
      "Epoch [2053/10000], Loss: 2.1121\n",
      "Epoch [2054/10000], Loss: 2.1120\n",
      "Epoch [2055/10000], Loss: 2.1118\n",
      "Epoch [2056/10000], Loss: 2.1116\n",
      "Epoch [2057/10000], Loss: 2.1114\n",
      "Epoch [2058/10000], Loss: 2.1112\n",
      "Epoch [2059/10000], Loss: 2.1110\n",
      "Epoch [2060/10000], Loss: 2.1108\n",
      "Epoch [2061/10000], Loss: 2.1106\n",
      "Epoch [2062/10000], Loss: 2.1104\n",
      "Epoch [2063/10000], Loss: 2.1102\n",
      "Epoch [2064/10000], Loss: 2.1100\n",
      "Epoch [2065/10000], Loss: 2.1098\n",
      "Epoch [2066/10000], Loss: 2.1096\n",
      "Epoch [2067/10000], Loss: 2.1094\n",
      "Epoch [2068/10000], Loss: 2.1093\n",
      "Epoch [2069/10000], Loss: 2.1091\n",
      "Epoch [2070/10000], Loss: 2.1089\n",
      "Epoch [2071/10000], Loss: 2.1087\n",
      "Epoch [2072/10000], Loss: 2.1085\n",
      "Epoch [2073/10000], Loss: 2.1083\n",
      "Epoch [2074/10000], Loss: 2.1081\n",
      "Epoch [2075/10000], Loss: 2.1079\n",
      "Epoch [2076/10000], Loss: 2.1077\n",
      "Epoch [2077/10000], Loss: 2.1075\n",
      "Epoch [2078/10000], Loss: 2.1073\n",
      "Epoch [2079/10000], Loss: 2.1071\n",
      "Epoch [2080/10000], Loss: 2.1069\n",
      "Epoch [2081/10000], Loss: 2.1068\n",
      "Epoch [2082/10000], Loss: 2.1066\n",
      "Epoch [2083/10000], Loss: 2.1064\n",
      "Epoch [2084/10000], Loss: 2.1062\n",
      "Epoch [2085/10000], Loss: 2.1060\n",
      "Epoch [2086/10000], Loss: 2.1058\n",
      "Epoch [2087/10000], Loss: 2.1056\n",
      "Epoch [2088/10000], Loss: 2.1054\n",
      "Epoch [2089/10000], Loss: 2.1052\n",
      "Epoch [2090/10000], Loss: 2.1050\n",
      "Epoch [2091/10000], Loss: 2.1048\n",
      "Epoch [2092/10000], Loss: 2.1046\n",
      "Epoch [2093/10000], Loss: 2.1045\n",
      "Epoch [2094/10000], Loss: 2.1043\n",
      "Epoch [2095/10000], Loss: 2.1041\n",
      "Epoch [2096/10000], Loss: 2.1039\n",
      "Epoch [2097/10000], Loss: 2.1037\n",
      "Epoch [2098/10000], Loss: 2.1035\n",
      "Epoch [2099/10000], Loss: 2.1033\n",
      "Epoch [2100/10000], Loss: 2.1031\n",
      "Epoch [2101/10000], Loss: 2.1029\n",
      "Epoch [2102/10000], Loss: 2.1027\n",
      "Epoch [2103/10000], Loss: 2.1025\n",
      "Epoch [2104/10000], Loss: 2.1024\n",
      "Epoch [2105/10000], Loss: 2.1022\n",
      "Epoch [2106/10000], Loss: 2.1020\n",
      "Epoch [2107/10000], Loss: 2.1018\n",
      "Epoch [2108/10000], Loss: 2.1016\n",
      "Epoch [2109/10000], Loss: 2.1014\n",
      "Epoch [2110/10000], Loss: 2.1012\n",
      "Epoch [2111/10000], Loss: 2.1010\n",
      "Epoch [2112/10000], Loss: 2.1008\n",
      "Epoch [2113/10000], Loss: 2.1006\n",
      "Epoch [2114/10000], Loss: 2.1004\n",
      "Epoch [2115/10000], Loss: 2.1003\n",
      "Epoch [2116/10000], Loss: 2.1001\n",
      "Epoch [2117/10000], Loss: 2.0999\n",
      "Epoch [2118/10000], Loss: 2.0997\n",
      "Epoch [2119/10000], Loss: 2.0995\n",
      "Epoch [2120/10000], Loss: 2.0993\n",
      "Epoch [2121/10000], Loss: 2.0991\n",
      "Epoch [2122/10000], Loss: 2.0989\n",
      "Epoch [2123/10000], Loss: 2.0987\n",
      "Epoch [2124/10000], Loss: 2.0985\n",
      "Epoch [2125/10000], Loss: 2.0983\n",
      "Epoch [2126/10000], Loss: 2.0982\n",
      "Epoch [2127/10000], Loss: 2.0980\n",
      "Epoch [2128/10000], Loss: 2.0978\n",
      "Epoch [2129/10000], Loss: 2.0976\n",
      "Epoch [2130/10000], Loss: 2.0974\n",
      "Epoch [2131/10000], Loss: 2.0972\n",
      "Epoch [2132/10000], Loss: 2.0970\n",
      "Epoch [2133/10000], Loss: 2.0968\n",
      "Epoch [2134/10000], Loss: 2.0966\n",
      "Epoch [2135/10000], Loss: 2.0964\n",
      "Epoch [2136/10000], Loss: 2.0963\n",
      "Epoch [2137/10000], Loss: 2.0961\n",
      "Epoch [2138/10000], Loss: 2.0959\n",
      "Epoch [2139/10000], Loss: 2.0957\n",
      "Epoch [2140/10000], Loss: 2.0955\n",
      "Epoch [2141/10000], Loss: 2.0953\n",
      "Epoch [2142/10000], Loss: 2.0951\n",
      "Epoch [2143/10000], Loss: 2.0949\n",
      "Epoch [2144/10000], Loss: 2.0947\n",
      "Epoch [2145/10000], Loss: 2.0945\n",
      "Epoch [2146/10000], Loss: 2.0944\n",
      "Epoch [2147/10000], Loss: 2.0942\n",
      "Epoch [2148/10000], Loss: 2.0940\n",
      "Epoch [2149/10000], Loss: 2.0938\n",
      "Epoch [2150/10000], Loss: 2.0936\n",
      "Epoch [2151/10000], Loss: 2.0934\n",
      "Epoch [2152/10000], Loss: 2.0932\n",
      "Epoch [2153/10000], Loss: 2.0930\n",
      "Epoch [2154/10000], Loss: 2.0928\n",
      "Epoch [2155/10000], Loss: 2.0927\n",
      "Epoch [2156/10000], Loss: 2.0925\n",
      "Epoch [2157/10000], Loss: 2.0923\n",
      "Epoch [2158/10000], Loss: 2.0921\n",
      "Epoch [2159/10000], Loss: 2.0919\n",
      "Epoch [2160/10000], Loss: 2.0917\n",
      "Epoch [2161/10000], Loss: 2.0915\n",
      "Epoch [2162/10000], Loss: 2.0913\n",
      "Epoch [2163/10000], Loss: 2.0911\n",
      "Epoch [2164/10000], Loss: 2.0910\n",
      "Epoch [2165/10000], Loss: 2.0908\n",
      "Epoch [2166/10000], Loss: 2.0906\n",
      "Epoch [2167/10000], Loss: 2.0904\n",
      "Epoch [2168/10000], Loss: 2.0902\n",
      "Epoch [2169/10000], Loss: 2.0900\n",
      "Epoch [2170/10000], Loss: 2.0898\n",
      "Epoch [2171/10000], Loss: 2.0896\n",
      "Epoch [2172/10000], Loss: 2.0894\n",
      "Epoch [2173/10000], Loss: 2.0893\n",
      "Epoch [2174/10000], Loss: 2.0891\n",
      "Epoch [2175/10000], Loss: 2.0889\n",
      "Epoch [2176/10000], Loss: 2.0887\n",
      "Epoch [2177/10000], Loss: 2.0885\n",
      "Epoch [2178/10000], Loss: 2.0883\n",
      "Epoch [2179/10000], Loss: 2.0881\n",
      "Epoch [2180/10000], Loss: 2.0879\n",
      "Epoch [2181/10000], Loss: 2.0877\n",
      "Epoch [2182/10000], Loss: 2.0876\n",
      "Epoch [2183/10000], Loss: 2.0874\n",
      "Epoch [2184/10000], Loss: 2.0872\n",
      "Epoch [2185/10000], Loss: 2.0870\n",
      "Epoch [2186/10000], Loss: 2.0868\n",
      "Epoch [2187/10000], Loss: 2.0866\n",
      "Epoch [2188/10000], Loss: 2.0864\n",
      "Epoch [2189/10000], Loss: 2.0862\n",
      "Epoch [2190/10000], Loss: 2.0861\n",
      "Epoch [2191/10000], Loss: 2.0859\n",
      "Epoch [2192/10000], Loss: 2.0857\n",
      "Epoch [2193/10000], Loss: 2.0855\n",
      "Epoch [2194/10000], Loss: 2.0853\n",
      "Epoch [2195/10000], Loss: 2.0851\n",
      "Epoch [2196/10000], Loss: 2.0849\n",
      "Epoch [2197/10000], Loss: 2.0847\n",
      "Epoch [2198/10000], Loss: 2.0845\n",
      "Epoch [2199/10000], Loss: 2.0844\n",
      "Epoch [2200/10000], Loss: 2.0842\n",
      "Epoch [2201/10000], Loss: 2.0840\n",
      "Epoch [2202/10000], Loss: 2.0838\n",
      "Epoch [2203/10000], Loss: 2.0836\n",
      "Epoch [2204/10000], Loss: 2.0834\n",
      "Epoch [2205/10000], Loss: 2.0832\n",
      "Epoch [2206/10000], Loss: 2.0830\n",
      "Epoch [2207/10000], Loss: 2.0829\n",
      "Epoch [2208/10000], Loss: 2.0827\n",
      "Epoch [2209/10000], Loss: 2.0825\n",
      "Epoch [2210/10000], Loss: 2.0823\n",
      "Epoch [2211/10000], Loss: 2.0821\n",
      "Epoch [2212/10000], Loss: 2.0819\n",
      "Epoch [2213/10000], Loss: 2.0817\n",
      "Epoch [2214/10000], Loss: 2.0816\n",
      "Epoch [2215/10000], Loss: 2.0814\n",
      "Epoch [2216/10000], Loss: 2.0812\n",
      "Epoch [2217/10000], Loss: 2.0810\n",
      "Epoch [2218/10000], Loss: 2.0808\n",
      "Epoch [2219/10000], Loss: 2.0806\n",
      "Epoch [2220/10000], Loss: 2.0804\n",
      "Epoch [2221/10000], Loss: 2.0802\n",
      "Epoch [2222/10000], Loss: 2.0801\n",
      "Epoch [2223/10000], Loss: 2.0799\n",
      "Epoch [2224/10000], Loss: 2.0797\n",
      "Epoch [2225/10000], Loss: 2.0795\n",
      "Epoch [2226/10000], Loss: 2.0793\n",
      "Epoch [2227/10000], Loss: 2.0791\n",
      "Epoch [2228/10000], Loss: 2.0789\n",
      "Epoch [2229/10000], Loss: 2.0787\n",
      "Epoch [2230/10000], Loss: 2.0786\n",
      "Epoch [2231/10000], Loss: 2.0784\n",
      "Epoch [2232/10000], Loss: 2.0782\n",
      "Epoch [2233/10000], Loss: 2.0780\n",
      "Epoch [2234/10000], Loss: 2.0778\n",
      "Epoch [2235/10000], Loss: 2.0776\n",
      "Epoch [2236/10000], Loss: 2.0774\n",
      "Epoch [2237/10000], Loss: 2.0773\n",
      "Epoch [2238/10000], Loss: 2.0771\n",
      "Epoch [2239/10000], Loss: 2.0769\n",
      "Epoch [2240/10000], Loss: 2.0767\n",
      "Epoch [2241/10000], Loss: 2.0765\n",
      "Epoch [2242/10000], Loss: 2.0763\n",
      "Epoch [2243/10000], Loss: 2.0761\n",
      "Epoch [2244/10000], Loss: 2.0760\n",
      "Epoch [2245/10000], Loss: 2.0758\n",
      "Epoch [2246/10000], Loss: 2.0756\n",
      "Epoch [2247/10000], Loss: 2.0754\n",
      "Epoch [2248/10000], Loss: 2.0752\n",
      "Epoch [2249/10000], Loss: 2.0750\n",
      "Epoch [2250/10000], Loss: 2.0748\n",
      "Epoch [2251/10000], Loss: 2.0747\n",
      "Epoch [2252/10000], Loss: 2.0745\n",
      "Epoch [2253/10000], Loss: 2.0743\n",
      "Epoch [2254/10000], Loss: 2.0741\n",
      "Epoch [2255/10000], Loss: 2.0739\n",
      "Epoch [2256/10000], Loss: 2.0737\n",
      "Epoch [2257/10000], Loss: 2.0735\n",
      "Epoch [2258/10000], Loss: 2.0734\n",
      "Epoch [2259/10000], Loss: 2.0732\n",
      "Epoch [2260/10000], Loss: 2.0730\n",
      "Epoch [2261/10000], Loss: 2.0728\n",
      "Epoch [2262/10000], Loss: 2.0726\n",
      "Epoch [2263/10000], Loss: 2.0724\n",
      "Epoch [2264/10000], Loss: 2.0722\n",
      "Epoch [2265/10000], Loss: 2.0721\n",
      "Epoch [2266/10000], Loss: 2.0719\n",
      "Epoch [2267/10000], Loss: 2.0717\n",
      "Epoch [2268/10000], Loss: 2.0715\n",
      "Epoch [2269/10000], Loss: 2.0713\n",
      "Epoch [2270/10000], Loss: 2.0711\n",
      "Epoch [2271/10000], Loss: 2.0709\n",
      "Epoch [2272/10000], Loss: 2.0708\n",
      "Epoch [2273/10000], Loss: 2.0706\n",
      "Epoch [2274/10000], Loss: 2.0704\n",
      "Epoch [2275/10000], Loss: 2.0702\n",
      "Epoch [2276/10000], Loss: 2.0700\n",
      "Epoch [2277/10000], Loss: 2.0698\n",
      "Epoch [2278/10000], Loss: 2.0696\n",
      "Epoch [2279/10000], Loss: 2.0695\n",
      "Epoch [2280/10000], Loss: 2.0693\n",
      "Epoch [2281/10000], Loss: 2.0691\n",
      "Epoch [2282/10000], Loss: 2.0689\n",
      "Epoch [2283/10000], Loss: 2.0687\n",
      "Epoch [2284/10000], Loss: 2.0685\n",
      "Epoch [2285/10000], Loss: 2.0684\n",
      "Epoch [2286/10000], Loss: 2.0682\n",
      "Epoch [2287/10000], Loss: 2.0680\n",
      "Epoch [2288/10000], Loss: 2.0678\n",
      "Epoch [2289/10000], Loss: 2.0676\n",
      "Epoch [2290/10000], Loss: 2.0674\n",
      "Epoch [2291/10000], Loss: 2.0672\n",
      "Epoch [2292/10000], Loss: 2.0671\n",
      "Epoch [2293/10000], Loss: 2.0669\n",
      "Epoch [2294/10000], Loss: 2.0667\n",
      "Epoch [2295/10000], Loss: 2.0665\n",
      "Epoch [2296/10000], Loss: 2.0663\n",
      "Epoch [2297/10000], Loss: 2.0661\n",
      "Epoch [2298/10000], Loss: 2.0660\n",
      "Epoch [2299/10000], Loss: 2.0658\n",
      "Epoch [2300/10000], Loss: 2.0656\n",
      "Epoch [2301/10000], Loss: 2.0654\n",
      "Epoch [2302/10000], Loss: 2.0652\n",
      "Epoch [2303/10000], Loss: 2.0650\n",
      "Epoch [2304/10000], Loss: 2.0649\n",
      "Epoch [2305/10000], Loss: 2.0647\n",
      "Epoch [2306/10000], Loss: 2.0645\n",
      "Epoch [2307/10000], Loss: 2.0643\n",
      "Epoch [2308/10000], Loss: 2.0641\n",
      "Epoch [2309/10000], Loss: 2.0639\n",
      "Epoch [2310/10000], Loss: 2.0637\n",
      "Epoch [2311/10000], Loss: 2.0636\n",
      "Epoch [2312/10000], Loss: 2.0634\n",
      "Epoch [2313/10000], Loss: 2.0632\n",
      "Epoch [2314/10000], Loss: 2.0630\n",
      "Epoch [2315/10000], Loss: 2.0628\n",
      "Epoch [2316/10000], Loss: 2.0626\n",
      "Epoch [2317/10000], Loss: 2.0625\n",
      "Epoch [2318/10000], Loss: 2.0623\n",
      "Epoch [2319/10000], Loss: 2.0621\n",
      "Epoch [2320/10000], Loss: 2.0619\n",
      "Epoch [2321/10000], Loss: 2.0617\n",
      "Epoch [2322/10000], Loss: 2.0615\n",
      "Epoch [2323/10000], Loss: 2.0614\n",
      "Epoch [2324/10000], Loss: 2.0612\n",
      "Epoch [2325/10000], Loss: 2.0610\n",
      "Epoch [2326/10000], Loss: 2.0608\n",
      "Epoch [2327/10000], Loss: 2.0606\n",
      "Epoch [2328/10000], Loss: 2.0604\n",
      "Epoch [2329/10000], Loss: 2.0603\n",
      "Epoch [2330/10000], Loss: 2.0601\n",
      "Epoch [2331/10000], Loss: 2.0599\n",
      "Epoch [2332/10000], Loss: 2.0597\n",
      "Epoch [2333/10000], Loss: 2.0595\n",
      "Epoch [2334/10000], Loss: 2.0593\n",
      "Epoch [2335/10000], Loss: 2.0592\n",
      "Epoch [2336/10000], Loss: 2.0590\n",
      "Epoch [2337/10000], Loss: 2.0588\n",
      "Epoch [2338/10000], Loss: 2.0586\n",
      "Epoch [2339/10000], Loss: 2.0584\n",
      "Epoch [2340/10000], Loss: 2.0582\n",
      "Epoch [2341/10000], Loss: 2.0581\n",
      "Epoch [2342/10000], Loss: 2.0579\n",
      "Epoch [2343/10000], Loss: 2.0577\n",
      "Epoch [2344/10000], Loss: 2.0575\n",
      "Epoch [2345/10000], Loss: 2.0573\n",
      "Epoch [2346/10000], Loss: 2.0572\n",
      "Epoch [2347/10000], Loss: 2.0570\n",
      "Epoch [2348/10000], Loss: 2.0568\n",
      "Epoch [2349/10000], Loss: 2.0566\n",
      "Epoch [2350/10000], Loss: 2.0564\n",
      "Epoch [2351/10000], Loss: 2.0562\n",
      "Epoch [2352/10000], Loss: 2.0561\n",
      "Epoch [2353/10000], Loss: 2.0559\n",
      "Epoch [2354/10000], Loss: 2.0557\n",
      "Epoch [2355/10000], Loss: 2.0555\n",
      "Epoch [2356/10000], Loss: 2.0553\n",
      "Epoch [2357/10000], Loss: 2.0551\n",
      "Epoch [2358/10000], Loss: 2.0550\n",
      "Epoch [2359/10000], Loss: 2.0548\n",
      "Epoch [2360/10000], Loss: 2.0546\n",
      "Epoch [2361/10000], Loss: 2.0544\n",
      "Epoch [2362/10000], Loss: 2.0542\n",
      "Epoch [2363/10000], Loss: 2.0541\n",
      "Epoch [2364/10000], Loss: 2.0539\n",
      "Epoch [2365/10000], Loss: 2.0537\n",
      "Epoch [2366/10000], Loss: 2.0535\n",
      "Epoch [2367/10000], Loss: 2.0533\n",
      "Epoch [2368/10000], Loss: 2.0531\n",
      "Epoch [2369/10000], Loss: 2.0530\n",
      "Epoch [2370/10000], Loss: 2.0528\n",
      "Epoch [2371/10000], Loss: 2.0526\n",
      "Epoch [2372/10000], Loss: 2.0524\n",
      "Epoch [2373/10000], Loss: 2.0522\n",
      "Epoch [2374/10000], Loss: 2.0520\n",
      "Epoch [2375/10000], Loss: 2.0519\n",
      "Epoch [2376/10000], Loss: 2.0517\n",
      "Epoch [2377/10000], Loss: 2.0515\n",
      "Epoch [2378/10000], Loss: 2.0513\n",
      "Epoch [2379/10000], Loss: 2.0511\n",
      "Epoch [2380/10000], Loss: 2.0510\n",
      "Epoch [2381/10000], Loss: 2.0508\n",
      "Epoch [2382/10000], Loss: 2.0506\n",
      "Epoch [2383/10000], Loss: 2.0504\n",
      "Epoch [2384/10000], Loss: 2.0502\n",
      "Epoch [2385/10000], Loss: 2.0501\n",
      "Epoch [2386/10000], Loss: 2.0499\n",
      "Epoch [2387/10000], Loss: 2.0497\n",
      "Epoch [2388/10000], Loss: 2.0495\n",
      "Epoch [2389/10000], Loss: 2.0493\n",
      "Epoch [2390/10000], Loss: 2.0491\n",
      "Epoch [2391/10000], Loss: 2.0490\n",
      "Epoch [2392/10000], Loss: 2.0488\n",
      "Epoch [2393/10000], Loss: 2.0486\n",
      "Epoch [2394/10000], Loss: 2.0484\n",
      "Epoch [2395/10000], Loss: 2.0482\n",
      "Epoch [2396/10000], Loss: 2.0481\n",
      "Epoch [2397/10000], Loss: 2.0479\n",
      "Epoch [2398/10000], Loss: 2.0477\n",
      "Epoch [2399/10000], Loss: 2.0475\n",
      "Epoch [2400/10000], Loss: 2.0473\n",
      "Epoch [2401/10000], Loss: 2.0472\n",
      "Epoch [2402/10000], Loss: 2.0470\n",
      "Epoch [2403/10000], Loss: 2.0468\n",
      "Epoch [2404/10000], Loss: 2.0466\n",
      "Epoch [2405/10000], Loss: 2.0464\n",
      "Epoch [2406/10000], Loss: 2.0462\n",
      "Epoch [2407/10000], Loss: 2.0461\n",
      "Epoch [2408/10000], Loss: 2.0459\n",
      "Epoch [2409/10000], Loss: 2.0457\n",
      "Epoch [2410/10000], Loss: 2.0455\n",
      "Epoch [2411/10000], Loss: 2.0453\n",
      "Epoch [2412/10000], Loss: 2.0452\n",
      "Epoch [2413/10000], Loss: 2.0450\n",
      "Epoch [2414/10000], Loss: 2.0448\n",
      "Epoch [2415/10000], Loss: 2.0446\n",
      "Epoch [2416/10000], Loss: 2.0444\n",
      "Epoch [2417/10000], Loss: 2.0443\n",
      "Epoch [2418/10000], Loss: 2.0441\n",
      "Epoch [2419/10000], Loss: 2.0439\n",
      "Epoch [2420/10000], Loss: 2.0437\n",
      "Epoch [2421/10000], Loss: 2.0435\n",
      "Epoch [2422/10000], Loss: 2.0434\n",
      "Epoch [2423/10000], Loss: 2.0432\n",
      "Epoch [2424/10000], Loss: 2.0430\n",
      "Epoch [2425/10000], Loss: 2.0428\n",
      "Epoch [2426/10000], Loss: 2.0426\n",
      "Epoch [2427/10000], Loss: 2.0425\n",
      "Epoch [2428/10000], Loss: 2.0423\n",
      "Epoch [2429/10000], Loss: 2.0421\n",
      "Epoch [2430/10000], Loss: 2.0419\n",
      "Epoch [2431/10000], Loss: 2.0417\n",
      "Epoch [2432/10000], Loss: 2.0416\n",
      "Epoch [2433/10000], Loss: 2.0414\n",
      "Epoch [2434/10000], Loss: 2.0412\n",
      "Epoch [2435/10000], Loss: 2.0410\n",
      "Epoch [2436/10000], Loss: 2.0408\n",
      "Epoch [2437/10000], Loss: 2.0407\n",
      "Epoch [2438/10000], Loss: 2.0405\n",
      "Epoch [2439/10000], Loss: 2.0403\n",
      "Epoch [2440/10000], Loss: 2.0401\n",
      "Epoch [2441/10000], Loss: 2.0399\n",
      "Epoch [2442/10000], Loss: 2.0398\n",
      "Epoch [2443/10000], Loss: 2.0396\n",
      "Epoch [2444/10000], Loss: 2.0394\n",
      "Epoch [2445/10000], Loss: 2.0392\n",
      "Epoch [2446/10000], Loss: 2.0390\n",
      "Epoch [2447/10000], Loss: 2.0389\n",
      "Epoch [2448/10000], Loss: 2.0387\n",
      "Epoch [2449/10000], Loss: 2.0385\n",
      "Epoch [2450/10000], Loss: 2.0383\n",
      "Epoch [2451/10000], Loss: 2.0381\n",
      "Epoch [2452/10000], Loss: 2.0380\n",
      "Epoch [2453/10000], Loss: 2.0378\n",
      "Epoch [2454/10000], Loss: 2.0376\n",
      "Epoch [2455/10000], Loss: 2.0374\n",
      "Epoch [2456/10000], Loss: 2.0373\n",
      "Epoch [2457/10000], Loss: 2.0371\n",
      "Epoch [2458/10000], Loss: 2.0369\n",
      "Epoch [2459/10000], Loss: 2.0367\n",
      "Epoch [2460/10000], Loss: 2.0365\n",
      "Epoch [2461/10000], Loss: 2.0364\n",
      "Epoch [2462/10000], Loss: 2.0362\n",
      "Epoch [2463/10000], Loss: 2.0360\n",
      "Epoch [2464/10000], Loss: 2.0358\n",
      "Epoch [2465/10000], Loss: 2.0356\n",
      "Epoch [2466/10000], Loss: 2.0355\n",
      "Epoch [2467/10000], Loss: 2.0353\n",
      "Epoch [2468/10000], Loss: 2.0351\n",
      "Epoch [2469/10000], Loss: 2.0349\n",
      "Epoch [2470/10000], Loss: 2.0347\n",
      "Epoch [2471/10000], Loss: 2.0346\n",
      "Epoch [2472/10000], Loss: 2.0344\n",
      "Epoch [2473/10000], Loss: 2.0342\n",
      "Epoch [2474/10000], Loss: 2.0340\n",
      "Epoch [2475/10000], Loss: 2.0339\n",
      "Epoch [2476/10000], Loss: 2.0337\n",
      "Epoch [2477/10000], Loss: 2.0335\n",
      "Epoch [2478/10000], Loss: 2.0333\n",
      "Epoch [2479/10000], Loss: 2.0331\n",
      "Epoch [2480/10000], Loss: 2.0330\n",
      "Epoch [2481/10000], Loss: 2.0328\n",
      "Epoch [2482/10000], Loss: 2.0326\n",
      "Epoch [2483/10000], Loss: 2.0324\n",
      "Epoch [2484/10000], Loss: 2.0322\n",
      "Epoch [2485/10000], Loss: 2.0321\n",
      "Epoch [2486/10000], Loss: 2.0319\n",
      "Epoch [2487/10000], Loss: 2.0317\n",
      "Epoch [2488/10000], Loss: 2.0315\n",
      "Epoch [2489/10000], Loss: 2.0314\n",
      "Epoch [2490/10000], Loss: 2.0312\n",
      "Epoch [2491/10000], Loss: 2.0310\n",
      "Epoch [2492/10000], Loss: 2.0308\n",
      "Epoch [2493/10000], Loss: 2.0306\n",
      "Epoch [2494/10000], Loss: 2.0305\n",
      "Epoch [2495/10000], Loss: 2.0303\n",
      "Epoch [2496/10000], Loss: 2.0301\n",
      "Epoch [2497/10000], Loss: 2.0299\n",
      "Epoch [2498/10000], Loss: 2.0298\n",
      "Epoch [2499/10000], Loss: 2.0296\n",
      "Epoch [2500/10000], Loss: 2.0294\n",
      "Epoch [2501/10000], Loss: 2.0292\n",
      "Epoch [2502/10000], Loss: 2.0290\n",
      "Epoch [2503/10000], Loss: 2.0289\n",
      "Epoch [2504/10000], Loss: 2.0287\n",
      "Epoch [2505/10000], Loss: 2.0285\n",
      "Epoch [2506/10000], Loss: 2.0283\n",
      "Epoch [2507/10000], Loss: 2.0282\n",
      "Epoch [2508/10000], Loss: 2.0280\n",
      "Epoch [2509/10000], Loss: 2.0278\n",
      "Epoch [2510/10000], Loss: 2.0276\n",
      "Epoch [2511/10000], Loss: 2.0274\n",
      "Epoch [2512/10000], Loss: 2.0273\n",
      "Epoch [2513/10000], Loss: 2.0271\n",
      "Epoch [2514/10000], Loss: 2.0269\n",
      "Epoch [2515/10000], Loss: 2.0267\n",
      "Epoch [2516/10000], Loss: 2.0266\n",
      "Epoch [2517/10000], Loss: 2.0264\n",
      "Epoch [2518/10000], Loss: 2.0262\n",
      "Epoch [2519/10000], Loss: 2.0260\n",
      "Epoch [2520/10000], Loss: 2.0258\n",
      "Epoch [2521/10000], Loss: 2.0257\n",
      "Epoch [2522/10000], Loss: 2.0255\n",
      "Epoch [2523/10000], Loss: 2.0253\n",
      "Epoch [2524/10000], Loss: 2.0251\n",
      "Epoch [2525/10000], Loss: 2.0250\n",
      "Epoch [2526/10000], Loss: 2.0248\n",
      "Epoch [2527/10000], Loss: 2.0246\n",
      "Epoch [2528/10000], Loss: 2.0244\n",
      "Epoch [2529/10000], Loss: 2.0243\n",
      "Epoch [2530/10000], Loss: 2.0241\n",
      "Epoch [2531/10000], Loss: 2.0239\n",
      "Epoch [2532/10000], Loss: 2.0237\n",
      "Epoch [2533/10000], Loss: 2.0235\n",
      "Epoch [2534/10000], Loss: 2.0234\n",
      "Epoch [2535/10000], Loss: 2.0232\n",
      "Epoch [2536/10000], Loss: 2.0230\n",
      "Epoch [2537/10000], Loss: 2.0228\n",
      "Epoch [2538/10000], Loss: 2.0227\n",
      "Epoch [2539/10000], Loss: 2.0225\n",
      "Epoch [2540/10000], Loss: 2.0223\n",
      "Epoch [2541/10000], Loss: 2.0221\n",
      "Epoch [2542/10000], Loss: 2.0220\n",
      "Epoch [2543/10000], Loss: 2.0218\n",
      "Epoch [2544/10000], Loss: 2.0216\n",
      "Epoch [2545/10000], Loss: 2.0214\n",
      "Epoch [2546/10000], Loss: 2.0212\n",
      "Epoch [2547/10000], Loss: 2.0211\n",
      "Epoch [2548/10000], Loss: 2.0209\n",
      "Epoch [2549/10000], Loss: 2.0207\n",
      "Epoch [2550/10000], Loss: 2.0205\n",
      "Epoch [2551/10000], Loss: 2.0204\n",
      "Epoch [2552/10000], Loss: 2.0202\n",
      "Epoch [2553/10000], Loss: 2.0200\n",
      "Epoch [2554/10000], Loss: 2.0198\n",
      "Epoch [2555/10000], Loss: 2.0197\n",
      "Epoch [2556/10000], Loss: 2.0195\n",
      "Epoch [2557/10000], Loss: 2.0193\n",
      "Epoch [2558/10000], Loss: 2.0191\n",
      "Epoch [2559/10000], Loss: 2.0190\n",
      "Epoch [2560/10000], Loss: 2.0188\n",
      "Epoch [2561/10000], Loss: 2.0186\n",
      "Epoch [2562/10000], Loss: 2.0184\n",
      "Epoch [2563/10000], Loss: 2.0182\n",
      "Epoch [2564/10000], Loss: 2.0181\n",
      "Epoch [2565/10000], Loss: 2.0179\n",
      "Epoch [2566/10000], Loss: 2.0177\n",
      "Epoch [2567/10000], Loss: 2.0175\n",
      "Epoch [2568/10000], Loss: 2.0174\n",
      "Epoch [2569/10000], Loss: 2.0172\n",
      "Epoch [2570/10000], Loss: 2.0170\n",
      "Epoch [2571/10000], Loss: 2.0168\n",
      "Epoch [2572/10000], Loss: 2.0167\n",
      "Epoch [2573/10000], Loss: 2.0165\n",
      "Epoch [2574/10000], Loss: 2.0163\n",
      "Epoch [2575/10000], Loss: 2.0161\n",
      "Epoch [2576/10000], Loss: 2.0160\n",
      "Epoch [2577/10000], Loss: 2.0158\n",
      "Epoch [2578/10000], Loss: 2.0156\n",
      "Epoch [2579/10000], Loss: 2.0154\n",
      "Epoch [2580/10000], Loss: 2.0153\n",
      "Epoch [2581/10000], Loss: 2.0151\n",
      "Epoch [2582/10000], Loss: 2.0149\n",
      "Epoch [2583/10000], Loss: 2.0147\n",
      "Epoch [2584/10000], Loss: 2.0146\n",
      "Epoch [2585/10000], Loss: 2.0144\n",
      "Epoch [2586/10000], Loss: 2.0142\n",
      "Epoch [2587/10000], Loss: 2.0140\n",
      "Epoch [2588/10000], Loss: 2.0139\n",
      "Epoch [2589/10000], Loss: 2.0137\n",
      "Epoch [2590/10000], Loss: 2.0135\n",
      "Epoch [2591/10000], Loss: 2.0133\n",
      "Epoch [2592/10000], Loss: 2.0132\n",
      "Epoch [2593/10000], Loss: 2.0130\n",
      "Epoch [2594/10000], Loss: 2.0128\n",
      "Epoch [2595/10000], Loss: 2.0126\n",
      "Epoch [2596/10000], Loss: 2.0125\n",
      "Epoch [2597/10000], Loss: 2.0123\n",
      "Epoch [2598/10000], Loss: 2.0121\n",
      "Epoch [2599/10000], Loss: 2.0119\n",
      "Epoch [2600/10000], Loss: 2.0118\n",
      "Epoch [2601/10000], Loss: 2.0116\n",
      "Epoch [2602/10000], Loss: 2.0114\n",
      "Epoch [2603/10000], Loss: 2.0112\n",
      "Epoch [2604/10000], Loss: 2.0111\n",
      "Epoch [2605/10000], Loss: 2.0109\n",
      "Epoch [2606/10000], Loss: 2.0107\n",
      "Epoch [2607/10000], Loss: 2.0105\n",
      "Epoch [2608/10000], Loss: 2.0104\n",
      "Epoch [2609/10000], Loss: 2.0102\n",
      "Epoch [2610/10000], Loss: 2.0100\n",
      "Epoch [2611/10000], Loss: 2.0098\n",
      "Epoch [2612/10000], Loss: 2.0097\n",
      "Epoch [2613/10000], Loss: 2.0095\n",
      "Epoch [2614/10000], Loss: 2.0093\n",
      "Epoch [2615/10000], Loss: 2.0091\n",
      "Epoch [2616/10000], Loss: 2.0090\n",
      "Epoch [2617/10000], Loss: 2.0088\n",
      "Epoch [2618/10000], Loss: 2.0086\n",
      "Epoch [2619/10000], Loss: 2.0084\n",
      "Epoch [2620/10000], Loss: 2.0083\n",
      "Epoch [2621/10000], Loss: 2.0081\n",
      "Epoch [2622/10000], Loss: 2.0079\n",
      "Epoch [2623/10000], Loss: 2.0077\n",
      "Epoch [2624/10000], Loss: 2.0076\n",
      "Epoch [2625/10000], Loss: 2.0074\n",
      "Epoch [2626/10000], Loss: 2.0072\n",
      "Epoch [2627/10000], Loss: 2.0070\n",
      "Epoch [2628/10000], Loss: 2.0069\n",
      "Epoch [2629/10000], Loss: 2.0067\n",
      "Epoch [2630/10000], Loss: 2.0065\n",
      "Epoch [2631/10000], Loss: 2.0063\n",
      "Epoch [2632/10000], Loss: 2.0062\n",
      "Epoch [2633/10000], Loss: 2.0060\n",
      "Epoch [2634/10000], Loss: 2.0058\n",
      "Epoch [2635/10000], Loss: 2.0056\n",
      "Epoch [2636/10000], Loss: 2.0055\n",
      "Epoch [2637/10000], Loss: 2.0053\n",
      "Epoch [2638/10000], Loss: 2.0051\n",
      "Epoch [2639/10000], Loss: 2.0050\n",
      "Epoch [2640/10000], Loss: 2.0048\n",
      "Epoch [2641/10000], Loss: 2.0046\n",
      "Epoch [2642/10000], Loss: 2.0044\n",
      "Epoch [2643/10000], Loss: 2.0043\n",
      "Epoch [2644/10000], Loss: 2.0041\n",
      "Epoch [2645/10000], Loss: 2.0039\n",
      "Epoch [2646/10000], Loss: 2.0037\n",
      "Epoch [2647/10000], Loss: 2.0036\n",
      "Epoch [2648/10000], Loss: 2.0034\n",
      "Epoch [2649/10000], Loss: 2.0032\n",
      "Epoch [2650/10000], Loss: 2.0030\n",
      "Epoch [2651/10000], Loss: 2.0029\n",
      "Epoch [2652/10000], Loss: 2.0027\n",
      "Epoch [2653/10000], Loss: 2.0025\n",
      "Epoch [2654/10000], Loss: 2.0023\n",
      "Epoch [2655/10000], Loss: 2.0022\n",
      "Epoch [2656/10000], Loss: 2.0020\n",
      "Epoch [2657/10000], Loss: 2.0018\n",
      "Epoch [2658/10000], Loss: 2.0017\n",
      "Epoch [2659/10000], Loss: 2.0015\n",
      "Epoch [2660/10000], Loss: 2.0013\n",
      "Epoch [2661/10000], Loss: 2.0011\n",
      "Epoch [2662/10000], Loss: 2.0010\n",
      "Epoch [2663/10000], Loss: 2.0008\n",
      "Epoch [2664/10000], Loss: 2.0006\n",
      "Epoch [2665/10000], Loss: 2.0004\n",
      "Epoch [2666/10000], Loss: 2.0003\n",
      "Epoch [2667/10000], Loss: 2.0001\n",
      "Epoch [2668/10000], Loss: 1.9999\n",
      "Epoch [2669/10000], Loss: 1.9997\n",
      "Epoch [2670/10000], Loss: 1.9996\n",
      "Epoch [2671/10000], Loss: 1.9994\n",
      "Epoch [2672/10000], Loss: 1.9992\n",
      "Epoch [2673/10000], Loss: 1.9991\n",
      "Epoch [2674/10000], Loss: 1.9989\n",
      "Epoch [2675/10000], Loss: 1.9987\n",
      "Epoch [2676/10000], Loss: 1.9985\n",
      "Epoch [2677/10000], Loss: 1.9984\n",
      "Epoch [2678/10000], Loss: 1.9982\n",
      "Epoch [2679/10000], Loss: 1.9980\n",
      "Epoch [2680/10000], Loss: 1.9978\n",
      "Epoch [2681/10000], Loss: 1.9977\n",
      "Epoch [2682/10000], Loss: 1.9975\n",
      "Epoch [2683/10000], Loss: 1.9973\n",
      "Epoch [2684/10000], Loss: 1.9972\n",
      "Epoch [2685/10000], Loss: 1.9970\n",
      "Epoch [2686/10000], Loss: 1.9968\n",
      "Epoch [2687/10000], Loss: 1.9966\n",
      "Epoch [2688/10000], Loss: 1.9965\n",
      "Epoch [2689/10000], Loss: 1.9963\n",
      "Epoch [2690/10000], Loss: 1.9961\n",
      "Epoch [2691/10000], Loss: 1.9959\n",
      "Epoch [2692/10000], Loss: 1.9958\n",
      "Epoch [2693/10000], Loss: 1.9956\n",
      "Epoch [2694/10000], Loss: 1.9954\n",
      "Epoch [2695/10000], Loss: 1.9953\n",
      "Epoch [2696/10000], Loss: 1.9951\n",
      "Epoch [2697/10000], Loss: 1.9949\n",
      "Epoch [2698/10000], Loss: 1.9947\n",
      "Epoch [2699/10000], Loss: 1.9946\n",
      "Epoch [2700/10000], Loss: 1.9944\n",
      "Epoch [2701/10000], Loss: 1.9942\n",
      "Epoch [2702/10000], Loss: 1.9941\n",
      "Epoch [2703/10000], Loss: 1.9939\n",
      "Epoch [2704/10000], Loss: 1.9937\n",
      "Epoch [2705/10000], Loss: 1.9935\n",
      "Epoch [2706/10000], Loss: 1.9934\n",
      "Epoch [2707/10000], Loss: 1.9932\n",
      "Epoch [2708/10000], Loss: 1.9930\n",
      "Epoch [2709/10000], Loss: 1.9928\n",
      "Epoch [2710/10000], Loss: 1.9927\n",
      "Epoch [2711/10000], Loss: 1.9925\n",
      "Epoch [2712/10000], Loss: 1.9923\n",
      "Epoch [2713/10000], Loss: 1.9922\n",
      "Epoch [2714/10000], Loss: 1.9920\n",
      "Epoch [2715/10000], Loss: 1.9918\n",
      "Epoch [2716/10000], Loss: 1.9916\n",
      "Epoch [2717/10000], Loss: 1.9915\n",
      "Epoch [2718/10000], Loss: 1.9913\n",
      "Epoch [2719/10000], Loss: 1.9911\n",
      "Epoch [2720/10000], Loss: 1.9910\n",
      "Epoch [2721/10000], Loss: 1.9908\n",
      "Epoch [2722/10000], Loss: 1.9906\n",
      "Epoch [2723/10000], Loss: 1.9904\n",
      "Epoch [2724/10000], Loss: 1.9903\n",
      "Epoch [2725/10000], Loss: 1.9901\n",
      "Epoch [2726/10000], Loss: 1.9899\n",
      "Epoch [2727/10000], Loss: 1.9898\n",
      "Epoch [2728/10000], Loss: 1.9896\n",
      "Epoch [2729/10000], Loss: 1.9894\n",
      "Epoch [2730/10000], Loss: 1.9892\n",
      "Epoch [2731/10000], Loss: 1.9891\n",
      "Epoch [2732/10000], Loss: 1.9889\n",
      "Epoch [2733/10000], Loss: 1.9887\n",
      "Epoch [2734/10000], Loss: 1.9886\n",
      "Epoch [2735/10000], Loss: 1.9884\n",
      "Epoch [2736/10000], Loss: 1.9882\n",
      "Epoch [2737/10000], Loss: 1.9880\n",
      "Epoch [2738/10000], Loss: 1.9879\n",
      "Epoch [2739/10000], Loss: 1.9877\n",
      "Epoch [2740/10000], Loss: 1.9875\n",
      "Epoch [2741/10000], Loss: 1.9874\n",
      "Epoch [2742/10000], Loss: 1.9872\n",
      "Epoch [2743/10000], Loss: 1.9870\n",
      "Epoch [2744/10000], Loss: 1.9868\n",
      "Epoch [2745/10000], Loss: 1.9867\n",
      "Epoch [2746/10000], Loss: 1.9865\n",
      "Epoch [2747/10000], Loss: 1.9863\n",
      "Epoch [2748/10000], Loss: 1.9862\n",
      "Epoch [2749/10000], Loss: 1.9860\n",
      "Epoch [2750/10000], Loss: 1.9858\n",
      "Epoch [2751/10000], Loss: 1.9856\n",
      "Epoch [2752/10000], Loss: 1.9855\n",
      "Epoch [2753/10000], Loss: 1.9853\n",
      "Epoch [2754/10000], Loss: 1.9851\n",
      "Epoch [2755/10000], Loss: 1.9850\n",
      "Epoch [2756/10000], Loss: 1.9848\n",
      "Epoch [2757/10000], Loss: 1.9846\n",
      "Epoch [2758/10000], Loss: 1.9844\n",
      "Epoch [2759/10000], Loss: 1.9843\n",
      "Epoch [2760/10000], Loss: 1.9841\n",
      "Epoch [2761/10000], Loss: 1.9839\n",
      "Epoch [2762/10000], Loss: 1.9838\n",
      "Epoch [2763/10000], Loss: 1.9836\n",
      "Epoch [2764/10000], Loss: 1.9834\n",
      "Epoch [2765/10000], Loss: 1.9833\n",
      "Epoch [2766/10000], Loss: 1.9831\n",
      "Epoch [2767/10000], Loss: 1.9829\n",
      "Epoch [2768/10000], Loss: 1.9827\n",
      "Epoch [2769/10000], Loss: 1.9826\n",
      "Epoch [2770/10000], Loss: 1.9824\n",
      "Epoch [2771/10000], Loss: 1.9822\n",
      "Epoch [2772/10000], Loss: 1.9821\n",
      "Epoch [2773/10000], Loss: 1.9819\n",
      "Epoch [2774/10000], Loss: 1.9817\n",
      "Epoch [2775/10000], Loss: 1.9816\n",
      "Epoch [2776/10000], Loss: 1.9814\n",
      "Epoch [2777/10000], Loss: 1.9812\n",
      "Epoch [2778/10000], Loss: 1.9810\n",
      "Epoch [2779/10000], Loss: 1.9809\n",
      "Epoch [2780/10000], Loss: 1.9807\n",
      "Epoch [2781/10000], Loss: 1.9805\n",
      "Epoch [2782/10000], Loss: 1.9804\n",
      "Epoch [2783/10000], Loss: 1.9802\n",
      "Epoch [2784/10000], Loss: 1.9800\n",
      "Epoch [2785/10000], Loss: 1.9798\n",
      "Epoch [2786/10000], Loss: 1.9797\n",
      "Epoch [2787/10000], Loss: 1.9795\n",
      "Epoch [2788/10000], Loss: 1.9793\n",
      "Epoch [2789/10000], Loss: 1.9792\n",
      "Epoch [2790/10000], Loss: 1.9790\n",
      "Epoch [2791/10000], Loss: 1.9788\n",
      "Epoch [2792/10000], Loss: 1.9787\n",
      "Epoch [2793/10000], Loss: 1.9785\n",
      "Epoch [2794/10000], Loss: 1.9783\n",
      "Epoch [2795/10000], Loss: 1.9781\n",
      "Epoch [2796/10000], Loss: 1.9780\n",
      "Epoch [2797/10000], Loss: 1.9778\n",
      "Epoch [2798/10000], Loss: 1.9776\n",
      "Epoch [2799/10000], Loss: 1.9775\n",
      "Epoch [2800/10000], Loss: 1.9773\n",
      "Epoch [2801/10000], Loss: 1.9771\n",
      "Epoch [2802/10000], Loss: 1.9770\n",
      "Epoch [2803/10000], Loss: 1.9768\n",
      "Epoch [2804/10000], Loss: 1.9766\n",
      "Epoch [2805/10000], Loss: 1.9765\n",
      "Epoch [2806/10000], Loss: 1.9763\n",
      "Epoch [2807/10000], Loss: 1.9761\n",
      "Epoch [2808/10000], Loss: 1.9759\n",
      "Epoch [2809/10000], Loss: 1.9758\n",
      "Epoch [2810/10000], Loss: 1.9756\n",
      "Epoch [2811/10000], Loss: 1.9754\n",
      "Epoch [2812/10000], Loss: 1.9753\n",
      "Epoch [2813/10000], Loss: 1.9751\n",
      "Epoch [2814/10000], Loss: 1.9749\n",
      "Epoch [2815/10000], Loss: 1.9748\n",
      "Epoch [2816/10000], Loss: 1.9746\n",
      "Epoch [2817/10000], Loss: 1.9744\n",
      "Epoch [2818/10000], Loss: 1.9742\n",
      "Epoch [2819/10000], Loss: 1.9741\n",
      "Epoch [2820/10000], Loss: 1.9739\n",
      "Epoch [2821/10000], Loss: 1.9737\n",
      "Epoch [2822/10000], Loss: 1.9736\n",
      "Epoch [2823/10000], Loss: 1.9734\n",
      "Epoch [2824/10000], Loss: 1.9732\n",
      "Epoch [2825/10000], Loss: 1.9731\n",
      "Epoch [2826/10000], Loss: 1.9729\n",
      "Epoch [2827/10000], Loss: 1.9727\n",
      "Epoch [2828/10000], Loss: 1.9726\n",
      "Epoch [2829/10000], Loss: 1.9724\n",
      "Epoch [2830/10000], Loss: 1.9722\n",
      "Epoch [2831/10000], Loss: 1.9721\n",
      "Epoch [2832/10000], Loss: 1.9719\n",
      "Epoch [2833/10000], Loss: 1.9717\n",
      "Epoch [2834/10000], Loss: 1.9715\n",
      "Epoch [2835/10000], Loss: 1.9714\n",
      "Epoch [2836/10000], Loss: 1.9712\n",
      "Epoch [2837/10000], Loss: 1.9710\n",
      "Epoch [2838/10000], Loss: 1.9709\n",
      "Epoch [2839/10000], Loss: 1.9707\n",
      "Epoch [2840/10000], Loss: 1.9705\n",
      "Epoch [2841/10000], Loss: 1.9704\n",
      "Epoch [2842/10000], Loss: 1.9702\n",
      "Epoch [2843/10000], Loss: 1.9700\n",
      "Epoch [2844/10000], Loss: 1.9699\n",
      "Epoch [2845/10000], Loss: 1.9697\n",
      "Epoch [2846/10000], Loss: 1.9695\n",
      "Epoch [2847/10000], Loss: 1.9694\n",
      "Epoch [2848/10000], Loss: 1.9692\n",
      "Epoch [2849/10000], Loss: 1.9690\n",
      "Epoch [2850/10000], Loss: 1.9688\n",
      "Epoch [2851/10000], Loss: 1.9687\n",
      "Epoch [2852/10000], Loss: 1.9685\n",
      "Epoch [2853/10000], Loss: 1.9683\n",
      "Epoch [2854/10000], Loss: 1.9682\n",
      "Epoch [2855/10000], Loss: 1.9680\n",
      "Epoch [2856/10000], Loss: 1.9678\n",
      "Epoch [2857/10000], Loss: 1.9677\n",
      "Epoch [2858/10000], Loss: 1.9675\n",
      "Epoch [2859/10000], Loss: 1.9673\n",
      "Epoch [2860/10000], Loss: 1.9672\n",
      "Epoch [2861/10000], Loss: 1.9670\n",
      "Epoch [2862/10000], Loss: 1.9668\n",
      "Epoch [2863/10000], Loss: 1.9667\n",
      "Epoch [2864/10000], Loss: 1.9665\n",
      "Epoch [2865/10000], Loss: 1.9663\n",
      "Epoch [2866/10000], Loss: 1.9662\n",
      "Epoch [2867/10000], Loss: 1.9660\n",
      "Epoch [2868/10000], Loss: 1.9658\n",
      "Epoch [2869/10000], Loss: 1.9656\n",
      "Epoch [2870/10000], Loss: 1.9655\n",
      "Epoch [2871/10000], Loss: 1.9653\n",
      "Epoch [2872/10000], Loss: 1.9651\n",
      "Epoch [2873/10000], Loss: 1.9650\n",
      "Epoch [2874/10000], Loss: 1.9648\n",
      "Epoch [2875/10000], Loss: 1.9646\n",
      "Epoch [2876/10000], Loss: 1.9645\n",
      "Epoch [2877/10000], Loss: 1.9643\n",
      "Epoch [2878/10000], Loss: 1.9641\n",
      "Epoch [2879/10000], Loss: 1.9640\n",
      "Epoch [2880/10000], Loss: 1.9638\n",
      "Epoch [2881/10000], Loss: 1.9636\n",
      "Epoch [2882/10000], Loss: 1.9635\n",
      "Epoch [2883/10000], Loss: 1.9633\n",
      "Epoch [2884/10000], Loss: 1.9631\n",
      "Epoch [2885/10000], Loss: 1.9630\n",
      "Epoch [2886/10000], Loss: 1.9628\n",
      "Epoch [2887/10000], Loss: 1.9626\n",
      "Epoch [2888/10000], Loss: 1.9625\n",
      "Epoch [2889/10000], Loss: 1.9623\n",
      "Epoch [2890/10000], Loss: 1.9621\n",
      "Epoch [2891/10000], Loss: 1.9620\n",
      "Epoch [2892/10000], Loss: 1.9618\n",
      "Epoch [2893/10000], Loss: 1.9616\n",
      "Epoch [2894/10000], Loss: 1.9615\n",
      "Epoch [2895/10000], Loss: 1.9613\n",
      "Epoch [2896/10000], Loss: 1.9611\n",
      "Epoch [2897/10000], Loss: 1.9610\n",
      "Epoch [2898/10000], Loss: 1.9608\n",
      "Epoch [2899/10000], Loss: 1.9606\n",
      "Epoch [2900/10000], Loss: 1.9605\n",
      "Epoch [2901/10000], Loss: 1.9603\n",
      "Epoch [2902/10000], Loss: 1.9601\n",
      "Epoch [2903/10000], Loss: 1.9600\n",
      "Epoch [2904/10000], Loss: 1.9598\n",
      "Epoch [2905/10000], Loss: 1.9596\n",
      "Epoch [2906/10000], Loss: 1.9594\n",
      "Epoch [2907/10000], Loss: 1.9593\n",
      "Epoch [2908/10000], Loss: 1.9591\n",
      "Epoch [2909/10000], Loss: 1.9589\n",
      "Epoch [2910/10000], Loss: 1.9588\n",
      "Epoch [2911/10000], Loss: 1.9586\n",
      "Epoch [2912/10000], Loss: 1.9584\n",
      "Epoch [2913/10000], Loss: 1.9583\n",
      "Epoch [2914/10000], Loss: 1.9581\n",
      "Epoch [2915/10000], Loss: 1.9579\n",
      "Epoch [2916/10000], Loss: 1.9578\n",
      "Epoch [2917/10000], Loss: 1.9576\n",
      "Epoch [2918/10000], Loss: 1.9574\n",
      "Epoch [2919/10000], Loss: 1.9573\n",
      "Epoch [2920/10000], Loss: 1.9571\n",
      "Epoch [2921/10000], Loss: 1.9569\n",
      "Epoch [2922/10000], Loss: 1.9568\n",
      "Epoch [2923/10000], Loss: 1.9566\n",
      "Epoch [2924/10000], Loss: 1.9564\n",
      "Epoch [2925/10000], Loss: 1.9563\n",
      "Epoch [2926/10000], Loss: 1.9561\n",
      "Epoch [2927/10000], Loss: 1.9559\n",
      "Epoch [2928/10000], Loss: 1.9558\n",
      "Epoch [2929/10000], Loss: 1.9556\n",
      "Epoch [2930/10000], Loss: 1.9554\n",
      "Epoch [2931/10000], Loss: 1.9553\n",
      "Epoch [2932/10000], Loss: 1.9551\n",
      "Epoch [2933/10000], Loss: 1.9549\n",
      "Epoch [2934/10000], Loss: 1.9548\n",
      "Epoch [2935/10000], Loss: 1.9546\n",
      "Epoch [2936/10000], Loss: 1.9544\n",
      "Epoch [2937/10000], Loss: 1.9543\n",
      "Epoch [2938/10000], Loss: 1.9541\n",
      "Epoch [2939/10000], Loss: 1.9539\n",
      "Epoch [2940/10000], Loss: 1.9538\n",
      "Epoch [2941/10000], Loss: 1.9536\n",
      "Epoch [2942/10000], Loss: 1.9534\n",
      "Epoch [2943/10000], Loss: 1.9533\n",
      "Epoch [2944/10000], Loss: 1.9531\n",
      "Epoch [2945/10000], Loss: 1.9529\n",
      "Epoch [2946/10000], Loss: 1.9528\n",
      "Epoch [2947/10000], Loss: 1.9526\n",
      "Epoch [2948/10000], Loss: 1.9525\n",
      "Epoch [2949/10000], Loss: 1.9523\n",
      "Epoch [2950/10000], Loss: 1.9521\n",
      "Epoch [2951/10000], Loss: 1.9520\n",
      "Epoch [2952/10000], Loss: 1.9518\n",
      "Epoch [2953/10000], Loss: 1.9516\n",
      "Epoch [2954/10000], Loss: 1.9515\n",
      "Epoch [2955/10000], Loss: 1.9513\n",
      "Epoch [2956/10000], Loss: 1.9511\n",
      "Epoch [2957/10000], Loss: 1.9510\n",
      "Epoch [2958/10000], Loss: 1.9508\n",
      "Epoch [2959/10000], Loss: 1.9506\n",
      "Epoch [2960/10000], Loss: 1.9505\n",
      "Epoch [2961/10000], Loss: 1.9503\n",
      "Epoch [2962/10000], Loss: 1.9501\n",
      "Epoch [2963/10000], Loss: 1.9500\n",
      "Epoch [2964/10000], Loss: 1.9498\n",
      "Epoch [2965/10000], Loss: 1.9496\n",
      "Epoch [2966/10000], Loss: 1.9495\n",
      "Epoch [2967/10000], Loss: 1.9493\n",
      "Epoch [2968/10000], Loss: 1.9491\n",
      "Epoch [2969/10000], Loss: 1.9490\n",
      "Epoch [2970/10000], Loss: 1.9488\n",
      "Epoch [2971/10000], Loss: 1.9486\n",
      "Epoch [2972/10000], Loss: 1.9485\n",
      "Epoch [2973/10000], Loss: 1.9483\n",
      "Epoch [2974/10000], Loss: 1.9481\n",
      "Epoch [2975/10000], Loss: 1.9480\n",
      "Epoch [2976/10000], Loss: 1.9478\n",
      "Epoch [2977/10000], Loss: 1.9476\n",
      "Epoch [2978/10000], Loss: 1.9475\n",
      "Epoch [2979/10000], Loss: 1.9473\n",
      "Epoch [2980/10000], Loss: 1.9471\n",
      "Epoch [2981/10000], Loss: 1.9470\n",
      "Epoch [2982/10000], Loss: 1.9468\n",
      "Epoch [2983/10000], Loss: 1.9466\n",
      "Epoch [2984/10000], Loss: 1.9465\n",
      "Epoch [2985/10000], Loss: 1.9463\n",
      "Epoch [2986/10000], Loss: 1.9462\n",
      "Epoch [2987/10000], Loss: 1.9460\n",
      "Epoch [2988/10000], Loss: 1.9458\n",
      "Epoch [2989/10000], Loss: 1.9457\n",
      "Epoch [2990/10000], Loss: 1.9455\n",
      "Epoch [2991/10000], Loss: 1.9453\n",
      "Epoch [2992/10000], Loss: 1.9452\n",
      "Epoch [2993/10000], Loss: 1.9450\n",
      "Epoch [2994/10000], Loss: 1.9448\n",
      "Epoch [2995/10000], Loss: 1.9447\n",
      "Epoch [2996/10000], Loss: 1.9445\n",
      "Epoch [2997/10000], Loss: 1.9443\n",
      "Epoch [2998/10000], Loss: 1.9442\n",
      "Epoch [2999/10000], Loss: 1.9440\n",
      "Epoch [3000/10000], Loss: 1.9438\n",
      "Epoch [3001/10000], Loss: 1.9437\n",
      "Epoch [3002/10000], Loss: 1.9435\n",
      "Epoch [3003/10000], Loss: 1.9433\n",
      "Epoch [3004/10000], Loss: 1.9432\n",
      "Epoch [3005/10000], Loss: 1.9430\n",
      "Epoch [3006/10000], Loss: 1.9429\n",
      "Epoch [3007/10000], Loss: 1.9427\n",
      "Epoch [3008/10000], Loss: 1.9425\n",
      "Epoch [3009/10000], Loss: 1.9424\n",
      "Epoch [3010/10000], Loss: 1.9422\n",
      "Epoch [3011/10000], Loss: 1.9420\n",
      "Epoch [3012/10000], Loss: 1.9419\n",
      "Epoch [3013/10000], Loss: 1.9417\n",
      "Epoch [3014/10000], Loss: 1.9415\n",
      "Epoch [3015/10000], Loss: 1.9414\n",
      "Epoch [3016/10000], Loss: 1.9412\n",
      "Epoch [3017/10000], Loss: 1.9410\n",
      "Epoch [3018/10000], Loss: 1.9409\n",
      "Epoch [3019/10000], Loss: 1.9407\n",
      "Epoch [3020/10000], Loss: 1.9405\n",
      "Epoch [3021/10000], Loss: 1.9404\n",
      "Epoch [3022/10000], Loss: 1.9402\n",
      "Epoch [3023/10000], Loss: 1.9401\n",
      "Epoch [3024/10000], Loss: 1.9399\n",
      "Epoch [3025/10000], Loss: 1.9397\n",
      "Epoch [3026/10000], Loss: 1.9396\n",
      "Epoch [3027/10000], Loss: 1.9394\n",
      "Epoch [3028/10000], Loss: 1.9392\n",
      "Epoch [3029/10000], Loss: 1.9391\n",
      "Epoch [3030/10000], Loss: 1.9389\n",
      "Epoch [3031/10000], Loss: 1.9387\n",
      "Epoch [3032/10000], Loss: 1.9386\n",
      "Epoch [3033/10000], Loss: 1.9384\n",
      "Epoch [3034/10000], Loss: 1.9382\n",
      "Epoch [3035/10000], Loss: 1.9381\n",
      "Epoch [3036/10000], Loss: 1.9379\n",
      "Epoch [3037/10000], Loss: 1.9378\n",
      "Epoch [3038/10000], Loss: 1.9376\n",
      "Epoch [3039/10000], Loss: 1.9374\n",
      "Epoch [3040/10000], Loss: 1.9373\n",
      "Epoch [3041/10000], Loss: 1.9371\n",
      "Epoch [3042/10000], Loss: 1.9369\n",
      "Epoch [3043/10000], Loss: 1.9368\n",
      "Epoch [3044/10000], Loss: 1.9366\n",
      "Epoch [3045/10000], Loss: 1.9364\n",
      "Epoch [3046/10000], Loss: 1.9363\n",
      "Epoch [3047/10000], Loss: 1.9361\n",
      "Epoch [3048/10000], Loss: 1.9359\n",
      "Epoch [3049/10000], Loss: 1.9358\n",
      "Epoch [3050/10000], Loss: 1.9356\n",
      "Epoch [3051/10000], Loss: 1.9355\n",
      "Epoch [3052/10000], Loss: 1.9353\n",
      "Epoch [3053/10000], Loss: 1.9351\n",
      "Epoch [3054/10000], Loss: 1.9350\n",
      "Epoch [3055/10000], Loss: 1.9348\n",
      "Epoch [3056/10000], Loss: 1.9346\n",
      "Epoch [3057/10000], Loss: 1.9345\n",
      "Epoch [3058/10000], Loss: 1.9343\n",
      "Epoch [3059/10000], Loss: 1.9341\n",
      "Epoch [3060/10000], Loss: 1.9340\n",
      "Epoch [3061/10000], Loss: 1.9338\n",
      "Epoch [3062/10000], Loss: 1.9337\n",
      "Epoch [3063/10000], Loss: 1.9335\n",
      "Epoch [3064/10000], Loss: 1.9333\n",
      "Epoch [3065/10000], Loss: 1.9332\n",
      "Epoch [3066/10000], Loss: 1.9330\n",
      "Epoch [3067/10000], Loss: 1.9328\n",
      "Epoch [3068/10000], Loss: 1.9327\n",
      "Epoch [3069/10000], Loss: 1.9325\n",
      "Epoch [3070/10000], Loss: 1.9323\n",
      "Epoch [3071/10000], Loss: 1.9322\n",
      "Epoch [3072/10000], Loss: 1.9320\n",
      "Epoch [3073/10000], Loss: 1.9319\n",
      "Epoch [3074/10000], Loss: 1.9317\n",
      "Epoch [3075/10000], Loss: 1.9315\n",
      "Epoch [3076/10000], Loss: 1.9314\n",
      "Epoch [3077/10000], Loss: 1.9312\n",
      "Epoch [3078/10000], Loss: 1.9310\n",
      "Epoch [3079/10000], Loss: 1.9309\n",
      "Epoch [3080/10000], Loss: 1.9307\n",
      "Epoch [3081/10000], Loss: 1.9305\n",
      "Epoch [3082/10000], Loss: 1.9304\n",
      "Epoch [3083/10000], Loss: 1.9302\n",
      "Epoch [3084/10000], Loss: 1.9301\n",
      "Epoch [3085/10000], Loss: 1.9299\n",
      "Epoch [3086/10000], Loss: 1.9297\n",
      "Epoch [3087/10000], Loss: 1.9296\n",
      "Epoch [3088/10000], Loss: 1.9294\n",
      "Epoch [3089/10000], Loss: 1.9292\n",
      "Epoch [3090/10000], Loss: 1.9291\n",
      "Epoch [3091/10000], Loss: 1.9289\n",
      "Epoch [3092/10000], Loss: 1.9288\n",
      "Epoch [3093/10000], Loss: 1.9286\n",
      "Epoch [3094/10000], Loss: 1.9284\n",
      "Epoch [3095/10000], Loss: 1.9283\n",
      "Epoch [3096/10000], Loss: 1.9281\n",
      "Epoch [3097/10000], Loss: 1.9279\n",
      "Epoch [3098/10000], Loss: 1.9278\n",
      "Epoch [3099/10000], Loss: 1.9276\n",
      "Epoch [3100/10000], Loss: 1.9274\n",
      "Epoch [3101/10000], Loss: 1.9273\n",
      "Epoch [3102/10000], Loss: 1.9271\n",
      "Epoch [3103/10000], Loss: 1.9270\n",
      "Epoch [3104/10000], Loss: 1.9268\n",
      "Epoch [3105/10000], Loss: 1.9266\n",
      "Epoch [3106/10000], Loss: 1.9265\n",
      "Epoch [3107/10000], Loss: 1.9263\n",
      "Epoch [3108/10000], Loss: 1.9261\n",
      "Epoch [3109/10000], Loss: 1.9260\n",
      "Epoch [3110/10000], Loss: 1.9258\n",
      "Epoch [3111/10000], Loss: 1.9257\n",
      "Epoch [3112/10000], Loss: 1.9255\n",
      "Epoch [3113/10000], Loss: 1.9253\n",
      "Epoch [3114/10000], Loss: 1.9252\n",
      "Epoch [3115/10000], Loss: 1.9250\n",
      "Epoch [3116/10000], Loss: 1.9248\n",
      "Epoch [3117/10000], Loss: 1.9247\n",
      "Epoch [3118/10000], Loss: 1.9245\n",
      "Epoch [3119/10000], Loss: 1.9244\n",
      "Epoch [3120/10000], Loss: 1.9242\n",
      "Epoch [3121/10000], Loss: 1.9240\n",
      "Epoch [3122/10000], Loss: 1.9239\n",
      "Epoch [3123/10000], Loss: 1.9237\n",
      "Epoch [3124/10000], Loss: 1.9235\n",
      "Epoch [3125/10000], Loss: 1.9234\n",
      "Epoch [3126/10000], Loss: 1.9232\n",
      "Epoch [3127/10000], Loss: 1.9231\n",
      "Epoch [3128/10000], Loss: 1.9229\n",
      "Epoch [3129/10000], Loss: 1.9227\n",
      "Epoch [3130/10000], Loss: 1.9226\n",
      "Epoch [3131/10000], Loss: 1.9224\n",
      "Epoch [3132/10000], Loss: 1.9222\n",
      "Epoch [3133/10000], Loss: 1.9221\n",
      "Epoch [3134/10000], Loss: 1.9219\n",
      "Epoch [3135/10000], Loss: 1.9218\n",
      "Epoch [3136/10000], Loss: 1.9216\n",
      "Epoch [3137/10000], Loss: 1.9214\n",
      "Epoch [3138/10000], Loss: 1.9213\n",
      "Epoch [3139/10000], Loss: 1.9211\n",
      "Epoch [3140/10000], Loss: 1.9210\n",
      "Epoch [3141/10000], Loss: 1.9208\n",
      "Epoch [3142/10000], Loss: 1.9206\n",
      "Epoch [3143/10000], Loss: 1.9205\n",
      "Epoch [3144/10000], Loss: 1.9203\n",
      "Epoch [3145/10000], Loss: 1.9201\n",
      "Epoch [3146/10000], Loss: 1.9200\n",
      "Epoch [3147/10000], Loss: 1.9198\n",
      "Epoch [3148/10000], Loss: 1.9197\n",
      "Epoch [3149/10000], Loss: 1.9195\n",
      "Epoch [3150/10000], Loss: 1.9193\n",
      "Epoch [3151/10000], Loss: 1.9192\n",
      "Epoch [3152/10000], Loss: 1.9190\n",
      "Epoch [3153/10000], Loss: 1.9188\n",
      "Epoch [3154/10000], Loss: 1.9187\n",
      "Epoch [3155/10000], Loss: 1.9185\n",
      "Epoch [3156/10000], Loss: 1.9184\n",
      "Epoch [3157/10000], Loss: 1.9182\n",
      "Epoch [3158/10000], Loss: 1.9180\n",
      "Epoch [3159/10000], Loss: 1.9179\n",
      "Epoch [3160/10000], Loss: 1.9177\n",
      "Epoch [3161/10000], Loss: 1.9176\n",
      "Epoch [3162/10000], Loss: 1.9174\n",
      "Epoch [3163/10000], Loss: 1.9172\n",
      "Epoch [3164/10000], Loss: 1.9171\n",
      "Epoch [3165/10000], Loss: 1.9169\n",
      "Epoch [3166/10000], Loss: 1.9167\n",
      "Epoch [3167/10000], Loss: 1.9166\n",
      "Epoch [3168/10000], Loss: 1.9164\n",
      "Epoch [3169/10000], Loss: 1.9163\n",
      "Epoch [3170/10000], Loss: 1.9161\n",
      "Epoch [3171/10000], Loss: 1.9159\n",
      "Epoch [3172/10000], Loss: 1.9158\n",
      "Epoch [3173/10000], Loss: 1.9156\n",
      "Epoch [3174/10000], Loss: 1.9155\n",
      "Epoch [3175/10000], Loss: 1.9153\n",
      "Epoch [3176/10000], Loss: 1.9151\n",
      "Epoch [3177/10000], Loss: 1.9150\n",
      "Epoch [3178/10000], Loss: 1.9148\n",
      "Epoch [3179/10000], Loss: 1.9146\n",
      "Epoch [3180/10000], Loss: 1.9145\n",
      "Epoch [3181/10000], Loss: 1.9143\n",
      "Epoch [3182/10000], Loss: 1.9142\n",
      "Epoch [3183/10000], Loss: 1.9140\n",
      "Epoch [3184/10000], Loss: 1.9138\n",
      "Epoch [3185/10000], Loss: 1.9137\n",
      "Epoch [3186/10000], Loss: 1.9135\n",
      "Epoch [3187/10000], Loss: 1.9134\n",
      "Epoch [3188/10000], Loss: 1.9132\n",
      "Epoch [3189/10000], Loss: 1.9130\n",
      "Epoch [3190/10000], Loss: 1.9129\n",
      "Epoch [3191/10000], Loss: 1.9127\n",
      "Epoch [3192/10000], Loss: 1.9126\n",
      "Epoch [3193/10000], Loss: 1.9124\n",
      "Epoch [3194/10000], Loss: 1.9122\n",
      "Epoch [3195/10000], Loss: 1.9121\n",
      "Epoch [3196/10000], Loss: 1.9119\n",
      "Epoch [3197/10000], Loss: 1.9117\n",
      "Epoch [3198/10000], Loss: 1.9116\n",
      "Epoch [3199/10000], Loss: 1.9114\n",
      "Epoch [3200/10000], Loss: 1.9113\n",
      "Epoch [3201/10000], Loss: 1.9111\n",
      "Epoch [3202/10000], Loss: 1.9109\n",
      "Epoch [3203/10000], Loss: 1.9108\n",
      "Epoch [3204/10000], Loss: 1.9106\n",
      "Epoch [3205/10000], Loss: 1.9105\n",
      "Epoch [3206/10000], Loss: 1.9103\n",
      "Epoch [3207/10000], Loss: 1.9101\n",
      "Epoch [3208/10000], Loss: 1.9100\n",
      "Epoch [3209/10000], Loss: 1.9098\n",
      "Epoch [3210/10000], Loss: 1.9097\n",
      "Epoch [3211/10000], Loss: 1.9095\n",
      "Epoch [3212/10000], Loss: 1.9093\n",
      "Epoch [3213/10000], Loss: 1.9092\n",
      "Epoch [3214/10000], Loss: 1.9090\n",
      "Epoch [3215/10000], Loss: 1.9089\n",
      "Epoch [3216/10000], Loss: 1.9087\n",
      "Epoch [3217/10000], Loss: 1.9085\n",
      "Epoch [3218/10000], Loss: 1.9084\n",
      "Epoch [3219/10000], Loss: 1.9082\n",
      "Epoch [3220/10000], Loss: 1.9081\n",
      "Epoch [3221/10000], Loss: 1.9079\n",
      "Epoch [3222/10000], Loss: 1.9077\n",
      "Epoch [3223/10000], Loss: 1.9076\n",
      "Epoch [3224/10000], Loss: 1.9074\n",
      "Epoch [3225/10000], Loss: 1.9073\n",
      "Epoch [3226/10000], Loss: 1.9071\n",
      "Epoch [3227/10000], Loss: 1.9069\n",
      "Epoch [3228/10000], Loss: 1.9068\n",
      "Epoch [3229/10000], Loss: 1.9066\n",
      "Epoch [3230/10000], Loss: 1.9065\n",
      "Epoch [3231/10000], Loss: 1.9063\n",
      "Epoch [3232/10000], Loss: 1.9061\n",
      "Epoch [3233/10000], Loss: 1.9060\n",
      "Epoch [3234/10000], Loss: 1.9058\n",
      "Epoch [3235/10000], Loss: 1.9056\n",
      "Epoch [3236/10000], Loss: 1.9055\n",
      "Epoch [3237/10000], Loss: 1.9053\n",
      "Epoch [3238/10000], Loss: 1.9052\n",
      "Epoch [3239/10000], Loss: 1.9050\n",
      "Epoch [3240/10000], Loss: 1.9048\n",
      "Epoch [3241/10000], Loss: 1.9047\n",
      "Epoch [3242/10000], Loss: 1.9045\n",
      "Epoch [3243/10000], Loss: 1.9044\n",
      "Epoch [3244/10000], Loss: 1.9042\n",
      "Epoch [3245/10000], Loss: 1.9040\n",
      "Epoch [3246/10000], Loss: 1.9039\n",
      "Epoch [3247/10000], Loss: 1.9037\n",
      "Epoch [3248/10000], Loss: 1.9036\n",
      "Epoch [3249/10000], Loss: 1.9034\n",
      "Epoch [3250/10000], Loss: 1.9032\n",
      "Epoch [3251/10000], Loss: 1.9031\n",
      "Epoch [3252/10000], Loss: 1.9029\n",
      "Epoch [3253/10000], Loss: 1.9028\n",
      "Epoch [3254/10000], Loss: 1.9026\n",
      "Epoch [3255/10000], Loss: 1.9025\n",
      "Epoch [3256/10000], Loss: 1.9023\n",
      "Epoch [3257/10000], Loss: 1.9021\n",
      "Epoch [3258/10000], Loss: 1.9020\n",
      "Epoch [3259/10000], Loss: 1.9018\n",
      "Epoch [3260/10000], Loss: 1.9017\n",
      "Epoch [3261/10000], Loss: 1.9015\n",
      "Epoch [3262/10000], Loss: 1.9013\n",
      "Epoch [3263/10000], Loss: 1.9012\n",
      "Epoch [3264/10000], Loss: 1.9010\n",
      "Epoch [3265/10000], Loss: 1.9009\n",
      "Epoch [3266/10000], Loss: 1.9007\n",
      "Epoch [3267/10000], Loss: 1.9005\n",
      "Epoch [3268/10000], Loss: 1.9004\n",
      "Epoch [3269/10000], Loss: 1.9002\n",
      "Epoch [3270/10000], Loss: 1.9001\n",
      "Epoch [3271/10000], Loss: 1.8999\n",
      "Epoch [3272/10000], Loss: 1.8997\n",
      "Epoch [3273/10000], Loss: 1.8996\n",
      "Epoch [3274/10000], Loss: 1.8994\n",
      "Epoch [3275/10000], Loss: 1.8993\n",
      "Epoch [3276/10000], Loss: 1.8991\n",
      "Epoch [3277/10000], Loss: 1.8989\n",
      "Epoch [3278/10000], Loss: 1.8988\n",
      "Epoch [3279/10000], Loss: 1.8986\n",
      "Epoch [3280/10000], Loss: 1.8985\n",
      "Epoch [3281/10000], Loss: 1.8983\n",
      "Epoch [3282/10000], Loss: 1.8981\n",
      "Epoch [3283/10000], Loss: 1.8980\n",
      "Epoch [3284/10000], Loss: 1.8978\n",
      "Epoch [3285/10000], Loss: 1.8977\n",
      "Epoch [3286/10000], Loss: 1.8975\n",
      "Epoch [3287/10000], Loss: 1.8973\n",
      "Epoch [3288/10000], Loss: 1.8972\n",
      "Epoch [3289/10000], Loss: 1.8970\n",
      "Epoch [3290/10000], Loss: 1.8969\n",
      "Epoch [3291/10000], Loss: 1.8967\n",
      "Epoch [3292/10000], Loss: 1.8966\n",
      "Epoch [3293/10000], Loss: 1.8964\n",
      "Epoch [3294/10000], Loss: 1.8962\n",
      "Epoch [3295/10000], Loss: 1.8961\n",
      "Epoch [3296/10000], Loss: 1.8959\n",
      "Epoch [3297/10000], Loss: 1.8958\n",
      "Epoch [3298/10000], Loss: 1.8956\n",
      "Epoch [3299/10000], Loss: 1.8954\n",
      "Epoch [3300/10000], Loss: 1.8953\n",
      "Epoch [3301/10000], Loss: 1.8951\n",
      "Epoch [3302/10000], Loss: 1.8950\n",
      "Epoch [3303/10000], Loss: 1.8948\n",
      "Epoch [3304/10000], Loss: 1.8946\n",
      "Epoch [3305/10000], Loss: 1.8945\n",
      "Epoch [3306/10000], Loss: 1.8943\n",
      "Epoch [3307/10000], Loss: 1.8942\n",
      "Epoch [3308/10000], Loss: 1.8940\n",
      "Epoch [3309/10000], Loss: 1.8939\n",
      "Epoch [3310/10000], Loss: 1.8937\n",
      "Epoch [3311/10000], Loss: 1.8935\n",
      "Epoch [3312/10000], Loss: 1.8934\n",
      "Epoch [3313/10000], Loss: 1.8932\n",
      "Epoch [3314/10000], Loss: 1.8931\n",
      "Epoch [3315/10000], Loss: 1.8929\n",
      "Epoch [3316/10000], Loss: 1.8927\n",
      "Epoch [3317/10000], Loss: 1.8926\n",
      "Epoch [3318/10000], Loss: 1.8924\n",
      "Epoch [3319/10000], Loss: 1.8923\n",
      "Epoch [3320/10000], Loss: 1.8921\n",
      "Epoch [3321/10000], Loss: 1.8919\n",
      "Epoch [3322/10000], Loss: 1.8918\n",
      "Epoch [3323/10000], Loss: 1.8916\n",
      "Epoch [3324/10000], Loss: 1.8915\n",
      "Epoch [3325/10000], Loss: 1.8913\n",
      "Epoch [3326/10000], Loss: 1.8912\n",
      "Epoch [3327/10000], Loss: 1.8910\n",
      "Epoch [3328/10000], Loss: 1.8908\n",
      "Epoch [3329/10000], Loss: 1.8907\n",
      "Epoch [3330/10000], Loss: 1.8905\n",
      "Epoch [3331/10000], Loss: 1.8904\n",
      "Epoch [3332/10000], Loss: 1.8902\n",
      "Epoch [3333/10000], Loss: 1.8900\n",
      "Epoch [3334/10000], Loss: 1.8899\n",
      "Epoch [3335/10000], Loss: 1.8897\n",
      "Epoch [3336/10000], Loss: 1.8896\n",
      "Epoch [3337/10000], Loss: 1.8894\n",
      "Epoch [3338/10000], Loss: 1.8893\n",
      "Epoch [3339/10000], Loss: 1.8891\n",
      "Epoch [3340/10000], Loss: 1.8889\n",
      "Epoch [3341/10000], Loss: 1.8888\n",
      "Epoch [3342/10000], Loss: 1.8886\n",
      "Epoch [3343/10000], Loss: 1.8885\n",
      "Epoch [3344/10000], Loss: 1.8883\n",
      "Epoch [3345/10000], Loss: 1.8881\n",
      "Epoch [3346/10000], Loss: 1.8880\n",
      "Epoch [3347/10000], Loss: 1.8878\n",
      "Epoch [3348/10000], Loss: 1.8877\n",
      "Epoch [3349/10000], Loss: 1.8875\n",
      "Epoch [3350/10000], Loss: 1.8874\n",
      "Epoch [3351/10000], Loss: 1.8872\n",
      "Epoch [3352/10000], Loss: 1.8870\n",
      "Epoch [3353/10000], Loss: 1.8869\n",
      "Epoch [3354/10000], Loss: 1.8867\n",
      "Epoch [3355/10000], Loss: 1.8866\n",
      "Epoch [3356/10000], Loss: 1.8864\n",
      "Epoch [3357/10000], Loss: 1.8863\n",
      "Epoch [3358/10000], Loss: 1.8861\n",
      "Epoch [3359/10000], Loss: 1.8859\n",
      "Epoch [3360/10000], Loss: 1.8858\n",
      "Epoch [3361/10000], Loss: 1.8856\n",
      "Epoch [3362/10000], Loss: 1.8855\n",
      "Epoch [3363/10000], Loss: 1.8853\n",
      "Epoch [3364/10000], Loss: 1.8851\n",
      "Epoch [3365/10000], Loss: 1.8850\n",
      "Epoch [3366/10000], Loss: 1.8848\n",
      "Epoch [3367/10000], Loss: 1.8847\n",
      "Epoch [3368/10000], Loss: 1.8845\n",
      "Epoch [3369/10000], Loss: 1.8844\n",
      "Epoch [3370/10000], Loss: 1.8842\n",
      "Epoch [3371/10000], Loss: 1.8840\n",
      "Epoch [3372/10000], Loss: 1.8839\n",
      "Epoch [3373/10000], Loss: 1.8837\n",
      "Epoch [3374/10000], Loss: 1.8836\n",
      "Epoch [3375/10000], Loss: 1.8834\n",
      "Epoch [3376/10000], Loss: 1.8833\n",
      "Epoch [3377/10000], Loss: 1.8831\n",
      "Epoch [3378/10000], Loss: 1.8829\n",
      "Epoch [3379/10000], Loss: 1.8828\n",
      "Epoch [3380/10000], Loss: 1.8826\n",
      "Epoch [3381/10000], Loss: 1.8825\n",
      "Epoch [3382/10000], Loss: 1.8823\n",
      "Epoch [3383/10000], Loss: 1.8822\n",
      "Epoch [3384/10000], Loss: 1.8820\n",
      "Epoch [3385/10000], Loss: 1.8818\n",
      "Epoch [3386/10000], Loss: 1.8817\n",
      "Epoch [3387/10000], Loss: 1.8815\n",
      "Epoch [3388/10000], Loss: 1.8814\n",
      "Epoch [3389/10000], Loss: 1.8812\n",
      "Epoch [3390/10000], Loss: 1.8811\n",
      "Epoch [3391/10000], Loss: 1.8809\n",
      "Epoch [3392/10000], Loss: 1.8807\n",
      "Epoch [3393/10000], Loss: 1.8806\n",
      "Epoch [3394/10000], Loss: 1.8804\n",
      "Epoch [3395/10000], Loss: 1.8803\n",
      "Epoch [3396/10000], Loss: 1.8801\n",
      "Epoch [3397/10000], Loss: 1.8800\n",
      "Epoch [3398/10000], Loss: 1.8798\n",
      "Epoch [3399/10000], Loss: 1.8796\n",
      "Epoch [3400/10000], Loss: 1.8795\n",
      "Epoch [3401/10000], Loss: 1.8793\n",
      "Epoch [3402/10000], Loss: 1.8792\n",
      "Epoch [3403/10000], Loss: 1.8790\n",
      "Epoch [3404/10000], Loss: 1.8789\n",
      "Epoch [3405/10000], Loss: 1.8787\n",
      "Epoch [3406/10000], Loss: 1.8785\n",
      "Epoch [3407/10000], Loss: 1.8784\n",
      "Epoch [3408/10000], Loss: 1.8782\n",
      "Epoch [3409/10000], Loss: 1.8781\n",
      "Epoch [3410/10000], Loss: 1.8779\n",
      "Epoch [3411/10000], Loss: 1.8778\n",
      "Epoch [3412/10000], Loss: 1.8776\n",
      "Epoch [3413/10000], Loss: 1.8774\n",
      "Epoch [3414/10000], Loss: 1.8773\n",
      "Epoch [3415/10000], Loss: 1.8771\n",
      "Epoch [3416/10000], Loss: 1.8770\n",
      "Epoch [3417/10000], Loss: 1.8768\n",
      "Epoch [3418/10000], Loss: 1.8767\n",
      "Epoch [3419/10000], Loss: 1.8765\n",
      "Epoch [3420/10000], Loss: 1.8763\n",
      "Epoch [3421/10000], Loss: 1.8762\n",
      "Epoch [3422/10000], Loss: 1.8760\n",
      "Epoch [3423/10000], Loss: 1.8759\n",
      "Epoch [3424/10000], Loss: 1.8757\n",
      "Epoch [3425/10000], Loss: 1.8756\n",
      "Epoch [3426/10000], Loss: 1.8754\n",
      "Epoch [3427/10000], Loss: 1.8752\n",
      "Epoch [3428/10000], Loss: 1.8751\n",
      "Epoch [3429/10000], Loss: 1.8749\n",
      "Epoch [3430/10000], Loss: 1.8748\n",
      "Epoch [3431/10000], Loss: 1.8746\n",
      "Epoch [3432/10000], Loss: 1.8745\n",
      "Epoch [3433/10000], Loss: 1.8743\n",
      "Epoch [3434/10000], Loss: 1.8741\n",
      "Epoch [3435/10000], Loss: 1.8740\n",
      "Epoch [3436/10000], Loss: 1.8738\n",
      "Epoch [3437/10000], Loss: 1.8737\n",
      "Epoch [3438/10000], Loss: 1.8735\n",
      "Epoch [3439/10000], Loss: 1.8734\n",
      "Epoch [3440/10000], Loss: 1.8732\n",
      "Epoch [3441/10000], Loss: 1.8731\n",
      "Epoch [3442/10000], Loss: 1.8729\n",
      "Epoch [3443/10000], Loss: 1.8727\n",
      "Epoch [3444/10000], Loss: 1.8726\n",
      "Epoch [3445/10000], Loss: 1.8724\n",
      "Epoch [3446/10000], Loss: 1.8723\n",
      "Epoch [3447/10000], Loss: 1.8721\n",
      "Epoch [3448/10000], Loss: 1.8720\n",
      "Epoch [3449/10000], Loss: 1.8718\n",
      "Epoch [3450/10000], Loss: 1.8716\n",
      "Epoch [3451/10000], Loss: 1.8715\n",
      "Epoch [3452/10000], Loss: 1.8713\n",
      "Epoch [3453/10000], Loss: 1.8712\n",
      "Epoch [3454/10000], Loss: 1.8710\n",
      "Epoch [3455/10000], Loss: 1.8709\n",
      "Epoch [3456/10000], Loss: 1.8707\n",
      "Epoch [3457/10000], Loss: 1.8706\n",
      "Epoch [3458/10000], Loss: 1.8704\n",
      "Epoch [3459/10000], Loss: 1.8702\n",
      "Epoch [3460/10000], Loss: 1.8701\n",
      "Epoch [3461/10000], Loss: 1.8699\n",
      "Epoch [3462/10000], Loss: 1.8698\n",
      "Epoch [3463/10000], Loss: 1.8696\n",
      "Epoch [3464/10000], Loss: 1.8695\n",
      "Epoch [3465/10000], Loss: 1.8693\n",
      "Epoch [3466/10000], Loss: 1.8691\n",
      "Epoch [3467/10000], Loss: 1.8690\n",
      "Epoch [3468/10000], Loss: 1.8688\n",
      "Epoch [3469/10000], Loss: 1.8687\n",
      "Epoch [3470/10000], Loss: 1.8685\n",
      "Epoch [3471/10000], Loss: 1.8684\n",
      "Epoch [3472/10000], Loss: 1.8682\n",
      "Epoch [3473/10000], Loss: 1.8681\n",
      "Epoch [3474/10000], Loss: 1.8679\n",
      "Epoch [3475/10000], Loss: 1.8677\n",
      "Epoch [3476/10000], Loss: 1.8676\n",
      "Epoch [3477/10000], Loss: 1.8674\n",
      "Epoch [3478/10000], Loss: 1.8673\n",
      "Epoch [3479/10000], Loss: 1.8671\n",
      "Epoch [3480/10000], Loss: 1.8670\n",
      "Epoch [3481/10000], Loss: 1.8668\n",
      "Epoch [3482/10000], Loss: 1.8667\n",
      "Epoch [3483/10000], Loss: 1.8665\n",
      "Epoch [3484/10000], Loss: 1.8663\n",
      "Epoch [3485/10000], Loss: 1.8662\n",
      "Epoch [3486/10000], Loss: 1.8660\n",
      "Epoch [3487/10000], Loss: 1.8659\n",
      "Epoch [3488/10000], Loss: 1.8657\n",
      "Epoch [3489/10000], Loss: 1.8656\n",
      "Epoch [3490/10000], Loss: 1.8654\n",
      "Epoch [3491/10000], Loss: 1.8653\n",
      "Epoch [3492/10000], Loss: 1.8651\n",
      "Epoch [3493/10000], Loss: 1.8649\n",
      "Epoch [3494/10000], Loss: 1.8648\n",
      "Epoch [3495/10000], Loss: 1.8646\n",
      "Epoch [3496/10000], Loss: 1.8645\n",
      "Epoch [3497/10000], Loss: 1.8643\n",
      "Epoch [3498/10000], Loss: 1.8642\n",
      "Epoch [3499/10000], Loss: 1.8640\n",
      "Epoch [3500/10000], Loss: 1.8639\n",
      "Epoch [3501/10000], Loss: 1.8637\n",
      "Epoch [3502/10000], Loss: 1.8635\n",
      "Epoch [3503/10000], Loss: 1.8634\n",
      "Epoch [3504/10000], Loss: 1.8632\n",
      "Epoch [3505/10000], Loss: 1.8631\n",
      "Epoch [3506/10000], Loss: 1.8629\n",
      "Epoch [3507/10000], Loss: 1.8628\n",
      "Epoch [3508/10000], Loss: 1.8626\n",
      "Epoch [3509/10000], Loss: 1.8625\n",
      "Epoch [3510/10000], Loss: 1.8623\n",
      "Epoch [3511/10000], Loss: 1.8621\n",
      "Epoch [3512/10000], Loss: 1.8620\n",
      "Epoch [3513/10000], Loss: 1.8618\n",
      "Epoch [3514/10000], Loss: 1.8617\n",
      "Epoch [3515/10000], Loss: 1.8615\n",
      "Epoch [3516/10000], Loss: 1.8614\n",
      "Epoch [3517/10000], Loss: 1.8612\n",
      "Epoch [3518/10000], Loss: 1.8611\n",
      "Epoch [3519/10000], Loss: 1.8609\n",
      "Epoch [3520/10000], Loss: 1.8607\n",
      "Epoch [3521/10000], Loss: 1.8606\n",
      "Epoch [3522/10000], Loss: 1.8604\n",
      "Epoch [3523/10000], Loss: 1.8603\n",
      "Epoch [3524/10000], Loss: 1.8601\n",
      "Epoch [3525/10000], Loss: 1.8600\n",
      "Epoch [3526/10000], Loss: 1.8598\n",
      "Epoch [3527/10000], Loss: 1.8597\n",
      "Epoch [3528/10000], Loss: 1.8595\n",
      "Epoch [3529/10000], Loss: 1.8594\n",
      "Epoch [3530/10000], Loss: 1.8592\n",
      "Epoch [3531/10000], Loss: 1.8590\n",
      "Epoch [3532/10000], Loss: 1.8589\n",
      "Epoch [3533/10000], Loss: 1.8587\n",
      "Epoch [3534/10000], Loss: 1.8586\n",
      "Epoch [3535/10000], Loss: 1.8584\n",
      "Epoch [3536/10000], Loss: 1.8583\n",
      "Epoch [3537/10000], Loss: 1.8581\n",
      "Epoch [3538/10000], Loss: 1.8580\n",
      "Epoch [3539/10000], Loss: 1.8578\n",
      "Epoch [3540/10000], Loss: 1.8577\n",
      "Epoch [3541/10000], Loss: 1.8575\n",
      "Epoch [3542/10000], Loss: 1.8573\n",
      "Epoch [3543/10000], Loss: 1.8572\n",
      "Epoch [3544/10000], Loss: 1.8570\n",
      "Epoch [3545/10000], Loss: 1.8569\n",
      "Epoch [3546/10000], Loss: 1.8567\n",
      "Epoch [3547/10000], Loss: 1.8566\n",
      "Epoch [3548/10000], Loss: 1.8564\n",
      "Epoch [3549/10000], Loss: 1.8563\n",
      "Epoch [3550/10000], Loss: 1.8561\n",
      "Epoch [3551/10000], Loss: 1.8559\n",
      "Epoch [3552/10000], Loss: 1.8558\n",
      "Epoch [3553/10000], Loss: 1.8556\n",
      "Epoch [3554/10000], Loss: 1.8555\n",
      "Epoch [3555/10000], Loss: 1.8553\n",
      "Epoch [3556/10000], Loss: 1.8552\n",
      "Epoch [3557/10000], Loss: 1.8550\n",
      "Epoch [3558/10000], Loss: 1.8549\n",
      "Epoch [3559/10000], Loss: 1.8547\n",
      "Epoch [3560/10000], Loss: 1.8546\n",
      "Epoch [3561/10000], Loss: 1.8544\n",
      "Epoch [3562/10000], Loss: 1.8542\n",
      "Epoch [3563/10000], Loss: 1.8541\n",
      "Epoch [3564/10000], Loss: 1.8539\n",
      "Epoch [3565/10000], Loss: 1.8538\n",
      "Epoch [3566/10000], Loss: 1.8536\n",
      "Epoch [3567/10000], Loss: 1.8535\n",
      "Epoch [3568/10000], Loss: 1.8533\n",
      "Epoch [3569/10000], Loss: 1.8532\n",
      "Epoch [3570/10000], Loss: 1.8530\n",
      "Epoch [3571/10000], Loss: 1.8529\n",
      "Epoch [3572/10000], Loss: 1.8527\n",
      "Epoch [3573/10000], Loss: 1.8526\n",
      "Epoch [3574/10000], Loss: 1.8524\n",
      "Epoch [3575/10000], Loss: 1.8522\n",
      "Epoch [3576/10000], Loss: 1.8521\n",
      "Epoch [3577/10000], Loss: 1.8519\n",
      "Epoch [3578/10000], Loss: 1.8518\n",
      "Epoch [3579/10000], Loss: 1.8516\n",
      "Epoch [3580/10000], Loss: 1.8515\n",
      "Epoch [3581/10000], Loss: 1.8513\n",
      "Epoch [3582/10000], Loss: 1.8512\n",
      "Epoch [3583/10000], Loss: 1.8510\n",
      "Epoch [3584/10000], Loss: 1.8509\n",
      "Epoch [3585/10000], Loss: 1.8507\n",
      "Epoch [3586/10000], Loss: 1.8505\n",
      "Epoch [3587/10000], Loss: 1.8504\n",
      "Epoch [3588/10000], Loss: 1.8502\n",
      "Epoch [3589/10000], Loss: 1.8501\n",
      "Epoch [3590/10000], Loss: 1.8499\n",
      "Epoch [3591/10000], Loss: 1.8498\n",
      "Epoch [3592/10000], Loss: 1.8496\n",
      "Epoch [3593/10000], Loss: 1.8495\n",
      "Epoch [3594/10000], Loss: 1.8493\n",
      "Epoch [3595/10000], Loss: 1.8492\n",
      "Epoch [3596/10000], Loss: 1.8490\n",
      "Epoch [3597/10000], Loss: 1.8489\n",
      "Epoch [3598/10000], Loss: 1.8487\n",
      "Epoch [3599/10000], Loss: 1.8485\n",
      "Epoch [3600/10000], Loss: 1.8484\n",
      "Epoch [3601/10000], Loss: 1.8482\n",
      "Epoch [3602/10000], Loss: 1.8481\n",
      "Epoch [3603/10000], Loss: 1.8479\n",
      "Epoch [3604/10000], Loss: 1.8478\n",
      "Epoch [3605/10000], Loss: 1.8476\n",
      "Epoch [3606/10000], Loss: 1.8475\n",
      "Epoch [3607/10000], Loss: 1.8473\n",
      "Epoch [3608/10000], Loss: 1.8472\n",
      "Epoch [3609/10000], Loss: 1.8470\n",
      "Epoch [3610/10000], Loss: 1.8469\n",
      "Epoch [3611/10000], Loss: 1.8467\n",
      "Epoch [3612/10000], Loss: 1.8465\n",
      "Epoch [3613/10000], Loss: 1.8464\n",
      "Epoch [3614/10000], Loss: 1.8462\n",
      "Epoch [3615/10000], Loss: 1.8461\n",
      "Epoch [3616/10000], Loss: 1.8459\n",
      "Epoch [3617/10000], Loss: 1.8458\n",
      "Epoch [3618/10000], Loss: 1.8456\n",
      "Epoch [3619/10000], Loss: 1.8455\n",
      "Epoch [3620/10000], Loss: 1.8453\n",
      "Epoch [3621/10000], Loss: 1.8452\n",
      "Epoch [3622/10000], Loss: 1.8450\n",
      "Epoch [3623/10000], Loss: 1.8449\n",
      "Epoch [3624/10000], Loss: 1.8447\n",
      "Epoch [3625/10000], Loss: 1.8446\n",
      "Epoch [3626/10000], Loss: 1.8444\n",
      "Epoch [3627/10000], Loss: 1.8442\n",
      "Epoch [3628/10000], Loss: 1.8441\n",
      "Epoch [3629/10000], Loss: 1.8439\n",
      "Epoch [3630/10000], Loss: 1.8438\n",
      "Epoch [3631/10000], Loss: 1.8436\n",
      "Epoch [3632/10000], Loss: 1.8435\n",
      "Epoch [3633/10000], Loss: 1.8433\n",
      "Epoch [3634/10000], Loss: 1.8432\n",
      "Epoch [3635/10000], Loss: 1.8430\n",
      "Epoch [3636/10000], Loss: 1.8429\n",
      "Epoch [3637/10000], Loss: 1.8427\n",
      "Epoch [3638/10000], Loss: 1.8426\n",
      "Epoch [3639/10000], Loss: 1.8424\n",
      "Epoch [3640/10000], Loss: 1.8423\n",
      "Epoch [3641/10000], Loss: 1.8421\n",
      "Epoch [3642/10000], Loss: 1.8419\n",
      "Epoch [3643/10000], Loss: 1.8418\n",
      "Epoch [3644/10000], Loss: 1.8416\n",
      "Epoch [3645/10000], Loss: 1.8415\n",
      "Epoch [3646/10000], Loss: 1.8413\n",
      "Epoch [3647/10000], Loss: 1.8412\n",
      "Epoch [3648/10000], Loss: 1.8410\n",
      "Epoch [3649/10000], Loss: 1.8409\n",
      "Epoch [3650/10000], Loss: 1.8407\n",
      "Epoch [3651/10000], Loss: 1.8406\n",
      "Epoch [3652/10000], Loss: 1.8404\n",
      "Epoch [3653/10000], Loss: 1.8403\n",
      "Epoch [3654/10000], Loss: 1.8401\n",
      "Epoch [3655/10000], Loss: 1.8400\n",
      "Epoch [3656/10000], Loss: 1.8398\n",
      "Epoch [3657/10000], Loss: 1.8397\n",
      "Epoch [3658/10000], Loss: 1.8395\n",
      "Epoch [3659/10000], Loss: 1.8393\n",
      "Epoch [3660/10000], Loss: 1.8392\n",
      "Epoch [3661/10000], Loss: 1.8390\n",
      "Epoch [3662/10000], Loss: 1.8389\n",
      "Epoch [3663/10000], Loss: 1.8387\n",
      "Epoch [3664/10000], Loss: 1.8386\n",
      "Epoch [3665/10000], Loss: 1.8384\n",
      "Epoch [3666/10000], Loss: 1.8383\n",
      "Epoch [3667/10000], Loss: 1.8381\n",
      "Epoch [3668/10000], Loss: 1.8380\n",
      "Epoch [3669/10000], Loss: 1.8378\n",
      "Epoch [3670/10000], Loss: 1.8377\n",
      "Epoch [3671/10000], Loss: 1.8375\n",
      "Epoch [3672/10000], Loss: 1.8374\n",
      "Epoch [3673/10000], Loss: 1.8372\n",
      "Epoch [3674/10000], Loss: 1.8371\n",
      "Epoch [3675/10000], Loss: 1.8369\n",
      "Epoch [3676/10000], Loss: 1.8367\n",
      "Epoch [3677/10000], Loss: 1.8366\n",
      "Epoch [3678/10000], Loss: 1.8364\n",
      "Epoch [3679/10000], Loss: 1.8363\n",
      "Epoch [3680/10000], Loss: 1.8361\n",
      "Epoch [3681/10000], Loss: 1.8360\n",
      "Epoch [3682/10000], Loss: 1.8358\n",
      "Epoch [3683/10000], Loss: 1.8357\n",
      "Epoch [3684/10000], Loss: 1.8355\n",
      "Epoch [3685/10000], Loss: 1.8354\n",
      "Epoch [3686/10000], Loss: 1.8352\n",
      "Epoch [3687/10000], Loss: 1.8351\n",
      "Epoch [3688/10000], Loss: 1.8349\n",
      "Epoch [3689/10000], Loss: 1.8348\n",
      "Epoch [3690/10000], Loss: 1.8346\n",
      "Epoch [3691/10000], Loss: 1.8345\n",
      "Epoch [3692/10000], Loss: 1.8343\n",
      "Epoch [3693/10000], Loss: 1.8342\n",
      "Epoch [3694/10000], Loss: 1.8340\n",
      "Epoch [3695/10000], Loss: 1.8339\n",
      "Epoch [3696/10000], Loss: 1.8337\n",
      "Epoch [3697/10000], Loss: 1.8335\n",
      "Epoch [3698/10000], Loss: 1.8334\n",
      "Epoch [3699/10000], Loss: 1.8332\n",
      "Epoch [3700/10000], Loss: 1.8331\n",
      "Epoch [3701/10000], Loss: 1.8329\n",
      "Epoch [3702/10000], Loss: 1.8328\n",
      "Epoch [3703/10000], Loss: 1.8326\n",
      "Epoch [3704/10000], Loss: 1.8325\n",
      "Epoch [3705/10000], Loss: 1.8323\n",
      "Epoch [3706/10000], Loss: 1.8322\n",
      "Epoch [3707/10000], Loss: 1.8320\n",
      "Epoch [3708/10000], Loss: 1.8319\n",
      "Epoch [3709/10000], Loss: 1.8317\n",
      "Epoch [3710/10000], Loss: 1.8316\n",
      "Epoch [3711/10000], Loss: 1.8314\n",
      "Epoch [3712/10000], Loss: 1.8313\n",
      "Epoch [3713/10000], Loss: 1.8311\n",
      "Epoch [3714/10000], Loss: 1.8310\n",
      "Epoch [3715/10000], Loss: 1.8308\n",
      "Epoch [3716/10000], Loss: 1.8307\n",
      "Epoch [3717/10000], Loss: 1.8305\n",
      "Epoch [3718/10000], Loss: 1.8303\n",
      "Epoch [3719/10000], Loss: 1.8302\n",
      "Epoch [3720/10000], Loss: 1.8300\n",
      "Epoch [3721/10000], Loss: 1.8299\n",
      "Epoch [3722/10000], Loss: 1.8297\n",
      "Epoch [3723/10000], Loss: 1.8296\n",
      "Epoch [3724/10000], Loss: 1.8294\n",
      "Epoch [3725/10000], Loss: 1.8293\n",
      "Epoch [3726/10000], Loss: 1.8291\n",
      "Epoch [3727/10000], Loss: 1.8290\n",
      "Epoch [3728/10000], Loss: 1.8288\n",
      "Epoch [3729/10000], Loss: 1.8287\n",
      "Epoch [3730/10000], Loss: 1.8285\n",
      "Epoch [3731/10000], Loss: 1.8284\n",
      "Epoch [3732/10000], Loss: 1.8282\n",
      "Epoch [3733/10000], Loss: 1.8281\n",
      "Epoch [3734/10000], Loss: 1.8279\n",
      "Epoch [3735/10000], Loss: 1.8278\n",
      "Epoch [3736/10000], Loss: 1.8276\n",
      "Epoch [3737/10000], Loss: 1.8275\n",
      "Epoch [3738/10000], Loss: 1.8273\n",
      "Epoch [3739/10000], Loss: 1.8272\n",
      "Epoch [3740/10000], Loss: 1.8270\n",
      "Epoch [3741/10000], Loss: 1.8269\n",
      "Epoch [3742/10000], Loss: 1.8267\n",
      "Epoch [3743/10000], Loss: 1.8266\n",
      "Epoch [3744/10000], Loss: 1.8264\n",
      "Epoch [3745/10000], Loss: 1.8263\n",
      "Epoch [3746/10000], Loss: 1.8261\n",
      "Epoch [3747/10000], Loss: 1.8259\n",
      "Epoch [3748/10000], Loss: 1.8258\n",
      "Epoch [3749/10000], Loss: 1.8256\n",
      "Epoch [3750/10000], Loss: 1.8255\n",
      "Epoch [3751/10000], Loss: 1.8253\n",
      "Epoch [3752/10000], Loss: 1.8252\n",
      "Epoch [3753/10000], Loss: 1.8250\n",
      "Epoch [3754/10000], Loss: 1.8249\n",
      "Epoch [3755/10000], Loss: 1.8247\n",
      "Epoch [3756/10000], Loss: 1.8246\n",
      "Epoch [3757/10000], Loss: 1.8244\n",
      "Epoch [3758/10000], Loss: 1.8243\n",
      "Epoch [3759/10000], Loss: 1.8241\n",
      "Epoch [3760/10000], Loss: 1.8240\n",
      "Epoch [3761/10000], Loss: 1.8238\n",
      "Epoch [3762/10000], Loss: 1.8237\n",
      "Epoch [3763/10000], Loss: 1.8235\n",
      "Epoch [3764/10000], Loss: 1.8234\n",
      "Epoch [3765/10000], Loss: 1.8232\n",
      "Epoch [3766/10000], Loss: 1.8231\n",
      "Epoch [3767/10000], Loss: 1.8229\n",
      "Epoch [3768/10000], Loss: 1.8228\n",
      "Epoch [3769/10000], Loss: 1.8226\n",
      "Epoch [3770/10000], Loss: 1.8225\n",
      "Epoch [3771/10000], Loss: 1.8223\n",
      "Epoch [3772/10000], Loss: 1.8222\n",
      "Epoch [3773/10000], Loss: 1.8220\n",
      "Epoch [3774/10000], Loss: 1.8219\n",
      "Epoch [3775/10000], Loss: 1.8217\n",
      "Epoch [3776/10000], Loss: 1.8216\n",
      "Epoch [3777/10000], Loss: 1.8214\n",
      "Epoch [3778/10000], Loss: 1.8213\n",
      "Epoch [3779/10000], Loss: 1.8211\n",
      "Epoch [3780/10000], Loss: 1.8210\n",
      "Epoch [3781/10000], Loss: 1.8208\n",
      "Epoch [3782/10000], Loss: 1.8206\n",
      "Epoch [3783/10000], Loss: 1.8205\n",
      "Epoch [3784/10000], Loss: 1.8203\n",
      "Epoch [3785/10000], Loss: 1.8202\n",
      "Epoch [3786/10000], Loss: 1.8200\n",
      "Epoch [3787/10000], Loss: 1.8199\n",
      "Epoch [3788/10000], Loss: 1.8197\n",
      "Epoch [3789/10000], Loss: 1.8196\n",
      "Epoch [3790/10000], Loss: 1.8194\n",
      "Epoch [3791/10000], Loss: 1.8193\n",
      "Epoch [3792/10000], Loss: 1.8191\n",
      "Epoch [3793/10000], Loss: 1.8190\n",
      "Epoch [3794/10000], Loss: 1.8188\n",
      "Epoch [3795/10000], Loss: 1.8187\n",
      "Epoch [3796/10000], Loss: 1.8185\n",
      "Epoch [3797/10000], Loss: 1.8184\n",
      "Epoch [3798/10000], Loss: 1.8182\n",
      "Epoch [3799/10000], Loss: 1.8181\n",
      "Epoch [3800/10000], Loss: 1.8179\n",
      "Epoch [3801/10000], Loss: 1.8178\n",
      "Epoch [3802/10000], Loss: 1.8176\n",
      "Epoch [3803/10000], Loss: 1.8175\n",
      "Epoch [3804/10000], Loss: 1.8173\n",
      "Epoch [3805/10000], Loss: 1.8172\n",
      "Epoch [3806/10000], Loss: 1.8170\n",
      "Epoch [3807/10000], Loss: 1.8169\n",
      "Epoch [3808/10000], Loss: 1.8167\n",
      "Epoch [3809/10000], Loss: 1.8166\n",
      "Epoch [3810/10000], Loss: 1.8164\n",
      "Epoch [3811/10000], Loss: 1.8163\n",
      "Epoch [3812/10000], Loss: 1.8161\n",
      "Epoch [3813/10000], Loss: 1.8160\n",
      "Epoch [3814/10000], Loss: 1.8158\n",
      "Epoch [3815/10000], Loss: 1.8157\n",
      "Epoch [3816/10000], Loss: 1.8155\n",
      "Epoch [3817/10000], Loss: 1.8154\n",
      "Epoch [3818/10000], Loss: 1.8152\n",
      "Epoch [3819/10000], Loss: 1.8151\n",
      "Epoch [3820/10000], Loss: 1.8149\n",
      "Epoch [3821/10000], Loss: 1.8148\n",
      "Epoch [3822/10000], Loss: 1.8146\n",
      "Epoch [3823/10000], Loss: 1.8145\n",
      "Epoch [3824/10000], Loss: 1.8143\n",
      "Epoch [3825/10000], Loss: 1.8142\n",
      "Epoch [3826/10000], Loss: 1.8140\n",
      "Epoch [3827/10000], Loss: 1.8139\n",
      "Epoch [3828/10000], Loss: 1.8137\n",
      "Epoch [3829/10000], Loss: 1.8136\n",
      "Epoch [3830/10000], Loss: 1.8134\n",
      "Epoch [3831/10000], Loss: 1.8133\n",
      "Epoch [3832/10000], Loss: 1.8131\n",
      "Epoch [3833/10000], Loss: 1.8130\n",
      "Epoch [3834/10000], Loss: 1.8128\n",
      "Epoch [3835/10000], Loss: 1.8127\n",
      "Epoch [3836/10000], Loss: 1.8125\n",
      "Epoch [3837/10000], Loss: 1.8124\n",
      "Epoch [3838/10000], Loss: 1.8122\n",
      "Epoch [3839/10000], Loss: 1.8121\n",
      "Epoch [3840/10000], Loss: 1.8119\n",
      "Epoch [3841/10000], Loss: 1.8118\n",
      "Epoch [3842/10000], Loss: 1.8116\n",
      "Epoch [3843/10000], Loss: 1.8115\n",
      "Epoch [3844/10000], Loss: 1.8113\n",
      "Epoch [3845/10000], Loss: 1.8112\n",
      "Epoch [3846/10000], Loss: 1.8110\n",
      "Epoch [3847/10000], Loss: 1.8109\n",
      "Epoch [3848/10000], Loss: 1.8107\n",
      "Epoch [3849/10000], Loss: 1.8106\n",
      "Epoch [3850/10000], Loss: 1.8104\n",
      "Epoch [3851/10000], Loss: 1.8103\n",
      "Epoch [3852/10000], Loss: 1.8101\n",
      "Epoch [3853/10000], Loss: 1.8100\n",
      "Epoch [3854/10000], Loss: 1.8098\n",
      "Epoch [3855/10000], Loss: 1.8097\n",
      "Epoch [3856/10000], Loss: 1.8095\n",
      "Epoch [3857/10000], Loss: 1.8094\n",
      "Epoch [3858/10000], Loss: 1.8092\n",
      "Epoch [3859/10000], Loss: 1.8091\n",
      "Epoch [3860/10000], Loss: 1.8089\n",
      "Epoch [3861/10000], Loss: 1.8088\n",
      "Epoch [3862/10000], Loss: 1.8086\n",
      "Epoch [3863/10000], Loss: 1.8085\n",
      "Epoch [3864/10000], Loss: 1.8083\n",
      "Epoch [3865/10000], Loss: 1.8082\n",
      "Epoch [3866/10000], Loss: 1.8080\n",
      "Epoch [3867/10000], Loss: 1.8079\n",
      "Epoch [3868/10000], Loss: 1.8077\n",
      "Epoch [3869/10000], Loss: 1.8076\n",
      "Epoch [3870/10000], Loss: 1.8074\n",
      "Epoch [3871/10000], Loss: 1.8073\n",
      "Epoch [3872/10000], Loss: 1.8071\n",
      "Epoch [3873/10000], Loss: 1.8070\n",
      "Epoch [3874/10000], Loss: 1.8068\n",
      "Epoch [3875/10000], Loss: 1.8067\n",
      "Epoch [3876/10000], Loss: 1.8065\n",
      "Epoch [3877/10000], Loss: 1.8064\n",
      "Epoch [3878/10000], Loss: 1.8062\n",
      "Epoch [3879/10000], Loss: 1.8061\n",
      "Epoch [3880/10000], Loss: 1.8059\n",
      "Epoch [3881/10000], Loss: 1.8058\n",
      "Epoch [3882/10000], Loss: 1.8056\n",
      "Epoch [3883/10000], Loss: 1.8055\n",
      "Epoch [3884/10000], Loss: 1.8053\n",
      "Epoch [3885/10000], Loss: 1.8052\n",
      "Epoch [3886/10000], Loss: 1.8050\n",
      "Epoch [3887/10000], Loss: 1.8049\n",
      "Epoch [3888/10000], Loss: 1.8047\n",
      "Epoch [3889/10000], Loss: 1.8046\n",
      "Epoch [3890/10000], Loss: 1.8044\n",
      "Epoch [3891/10000], Loss: 1.8043\n",
      "Epoch [3892/10000], Loss: 1.8041\n",
      "Epoch [3893/10000], Loss: 1.8040\n",
      "Epoch [3894/10000], Loss: 1.8038\n",
      "Epoch [3895/10000], Loss: 1.8037\n",
      "Epoch [3896/10000], Loss: 1.8035\n",
      "Epoch [3897/10000], Loss: 1.8034\n",
      "Epoch [3898/10000], Loss: 1.8032\n",
      "Epoch [3899/10000], Loss: 1.8031\n",
      "Epoch [3900/10000], Loss: 1.8029\n",
      "Epoch [3901/10000], Loss: 1.8028\n",
      "Epoch [3902/10000], Loss: 1.8026\n",
      "Epoch [3903/10000], Loss: 1.8025\n",
      "Epoch [3904/10000], Loss: 1.8023\n",
      "Epoch [3905/10000], Loss: 1.8022\n",
      "Epoch [3906/10000], Loss: 1.8020\n",
      "Epoch [3907/10000], Loss: 1.8019\n",
      "Epoch [3908/10000], Loss: 1.8017\n",
      "Epoch [3909/10000], Loss: 1.8016\n",
      "Epoch [3910/10000], Loss: 1.8014\n",
      "Epoch [3911/10000], Loss: 1.8013\n",
      "Epoch [3912/10000], Loss: 1.8011\n",
      "Epoch [3913/10000], Loss: 1.8010\n",
      "Epoch [3914/10000], Loss: 1.8008\n",
      "Epoch [3915/10000], Loss: 1.8007\n",
      "Epoch [3916/10000], Loss: 1.8005\n",
      "Epoch [3917/10000], Loss: 1.8004\n",
      "Epoch [3918/10000], Loss: 1.8002\n",
      "Epoch [3919/10000], Loss: 1.8001\n",
      "Epoch [3920/10000], Loss: 1.7999\n",
      "Epoch [3921/10000], Loss: 1.7998\n",
      "Epoch [3922/10000], Loss: 1.7996\n",
      "Epoch [3923/10000], Loss: 1.7995\n",
      "Epoch [3924/10000], Loss: 1.7993\n",
      "Epoch [3925/10000], Loss: 1.7992\n",
      "Epoch [3926/10000], Loss: 1.7990\n",
      "Epoch [3927/10000], Loss: 1.7989\n",
      "Epoch [3928/10000], Loss: 1.7987\n",
      "Epoch [3929/10000], Loss: 1.7986\n",
      "Epoch [3930/10000], Loss: 1.7984\n",
      "Epoch [3931/10000], Loss: 1.7983\n",
      "Epoch [3932/10000], Loss: 1.7981\n",
      "Epoch [3933/10000], Loss: 1.7980\n",
      "Epoch [3934/10000], Loss: 1.7978\n",
      "Epoch [3935/10000], Loss: 1.7977\n",
      "Epoch [3936/10000], Loss: 1.7975\n",
      "Epoch [3937/10000], Loss: 1.7974\n",
      "Epoch [3938/10000], Loss: 1.7972\n",
      "Epoch [3939/10000], Loss: 1.7971\n",
      "Epoch [3940/10000], Loss: 1.7969\n",
      "Epoch [3941/10000], Loss: 1.7968\n",
      "Epoch [3942/10000], Loss: 1.7966\n",
      "Epoch [3943/10000], Loss: 1.7965\n",
      "Epoch [3944/10000], Loss: 1.7963\n",
      "Epoch [3945/10000], Loss: 1.7962\n",
      "Epoch [3946/10000], Loss: 1.7960\n",
      "Epoch [3947/10000], Loss: 1.7959\n",
      "Epoch [3948/10000], Loss: 1.7958\n",
      "Epoch [3949/10000], Loss: 1.7956\n",
      "Epoch [3950/10000], Loss: 1.7955\n",
      "Epoch [3951/10000], Loss: 1.7953\n",
      "Epoch [3952/10000], Loss: 1.7952\n",
      "Epoch [3953/10000], Loss: 1.7950\n",
      "Epoch [3954/10000], Loss: 1.7949\n",
      "Epoch [3955/10000], Loss: 1.7947\n",
      "Epoch [3956/10000], Loss: 1.7946\n",
      "Epoch [3957/10000], Loss: 1.7944\n",
      "Epoch [3958/10000], Loss: 1.7943\n",
      "Epoch [3959/10000], Loss: 1.7941\n",
      "Epoch [3960/10000], Loss: 1.7940\n",
      "Epoch [3961/10000], Loss: 1.7938\n",
      "Epoch [3962/10000], Loss: 1.7937\n",
      "Epoch [3963/10000], Loss: 1.7935\n",
      "Epoch [3964/10000], Loss: 1.7934\n",
      "Epoch [3965/10000], Loss: 1.7932\n",
      "Epoch [3966/10000], Loss: 1.7931\n",
      "Epoch [3967/10000], Loss: 1.7929\n",
      "Epoch [3968/10000], Loss: 1.7928\n",
      "Epoch [3969/10000], Loss: 1.7926\n",
      "Epoch [3970/10000], Loss: 1.7925\n",
      "Epoch [3971/10000], Loss: 1.7923\n",
      "Epoch [3972/10000], Loss: 1.7922\n",
      "Epoch [3973/10000], Loss: 1.7920\n",
      "Epoch [3974/10000], Loss: 1.7919\n",
      "Epoch [3975/10000], Loss: 1.7917\n",
      "Epoch [3976/10000], Loss: 1.7916\n",
      "Epoch [3977/10000], Loss: 1.7914\n",
      "Epoch [3978/10000], Loss: 1.7913\n",
      "Epoch [3979/10000], Loss: 1.7911\n",
      "Epoch [3980/10000], Loss: 1.7910\n",
      "Epoch [3981/10000], Loss: 1.7908\n",
      "Epoch [3982/10000], Loss: 1.7907\n",
      "Epoch [3983/10000], Loss: 1.7905\n",
      "Epoch [3984/10000], Loss: 1.7904\n",
      "Epoch [3985/10000], Loss: 1.7903\n",
      "Epoch [3986/10000], Loss: 1.7901\n",
      "Epoch [3987/10000], Loss: 1.7900\n",
      "Epoch [3988/10000], Loss: 1.7898\n",
      "Epoch [3989/10000], Loss: 1.7897\n",
      "Epoch [3990/10000], Loss: 1.7895\n",
      "Epoch [3991/10000], Loss: 1.7894\n",
      "Epoch [3992/10000], Loss: 1.7892\n",
      "Epoch [3993/10000], Loss: 1.7891\n",
      "Epoch [3994/10000], Loss: 1.7889\n",
      "Epoch [3995/10000], Loss: 1.7888\n",
      "Epoch [3996/10000], Loss: 1.7886\n",
      "Epoch [3997/10000], Loss: 1.7885\n",
      "Epoch [3998/10000], Loss: 1.7883\n",
      "Epoch [3999/10000], Loss: 1.7882\n",
      "Epoch [4000/10000], Loss: 1.7880\n",
      "Epoch [4001/10000], Loss: 1.7879\n",
      "Epoch [4002/10000], Loss: 1.7877\n",
      "Epoch [4003/10000], Loss: 1.7876\n",
      "Epoch [4004/10000], Loss: 1.7874\n",
      "Epoch [4005/10000], Loss: 1.7873\n",
      "Epoch [4006/10000], Loss: 1.7871\n",
      "Epoch [4007/10000], Loss: 1.7870\n",
      "Epoch [4008/10000], Loss: 1.7868\n",
      "Epoch [4009/10000], Loss: 1.7867\n",
      "Epoch [4010/10000], Loss: 1.7865\n",
      "Epoch [4011/10000], Loss: 1.7864\n",
      "Epoch [4012/10000], Loss: 1.7862\n",
      "Epoch [4013/10000], Loss: 1.7861\n",
      "Epoch [4014/10000], Loss: 1.7860\n",
      "Epoch [4015/10000], Loss: 1.7858\n",
      "Epoch [4016/10000], Loss: 1.7857\n",
      "Epoch [4017/10000], Loss: 1.7855\n",
      "Epoch [4018/10000], Loss: 1.7854\n",
      "Epoch [4019/10000], Loss: 1.7852\n",
      "Epoch [4020/10000], Loss: 1.7851\n",
      "Epoch [4021/10000], Loss: 1.7849\n",
      "Epoch [4022/10000], Loss: 1.7848\n",
      "Epoch [4023/10000], Loss: 1.7846\n",
      "Epoch [4024/10000], Loss: 1.7845\n",
      "Epoch [4025/10000], Loss: 1.7843\n",
      "Epoch [4026/10000], Loss: 1.7842\n",
      "Epoch [4027/10000], Loss: 1.7840\n",
      "Epoch [4028/10000], Loss: 1.7839\n",
      "Epoch [4029/10000], Loss: 1.7837\n",
      "Epoch [4030/10000], Loss: 1.7836\n",
      "Epoch [4031/10000], Loss: 1.7834\n",
      "Epoch [4032/10000], Loss: 1.7833\n",
      "Epoch [4033/10000], Loss: 1.7831\n",
      "Epoch [4034/10000], Loss: 1.7830\n",
      "Epoch [4035/10000], Loss: 1.7828\n",
      "Epoch [4036/10000], Loss: 1.7827\n",
      "Epoch [4037/10000], Loss: 1.7825\n",
      "Epoch [4038/10000], Loss: 1.7824\n",
      "Epoch [4039/10000], Loss: 1.7823\n",
      "Epoch [4040/10000], Loss: 1.7821\n",
      "Epoch [4041/10000], Loss: 1.7820\n",
      "Epoch [4042/10000], Loss: 1.7818\n",
      "Epoch [4043/10000], Loss: 1.7817\n",
      "Epoch [4044/10000], Loss: 1.7815\n",
      "Epoch [4045/10000], Loss: 1.7814\n",
      "Epoch [4046/10000], Loss: 1.7812\n",
      "Epoch [4047/10000], Loss: 1.7811\n",
      "Epoch [4048/10000], Loss: 1.7809\n",
      "Epoch [4049/10000], Loss: 1.7808\n",
      "Epoch [4050/10000], Loss: 1.7806\n",
      "Epoch [4051/10000], Loss: 1.7805\n",
      "Epoch [4052/10000], Loss: 1.7803\n",
      "Epoch [4053/10000], Loss: 1.7802\n",
      "Epoch [4054/10000], Loss: 1.7800\n",
      "Epoch [4055/10000], Loss: 1.7799\n",
      "Epoch [4056/10000], Loss: 1.7797\n",
      "Epoch [4057/10000], Loss: 1.7796\n",
      "Epoch [4058/10000], Loss: 1.7794\n",
      "Epoch [4059/10000], Loss: 1.7793\n",
      "Epoch [4060/10000], Loss: 1.7792\n",
      "Epoch [4061/10000], Loss: 1.7790\n",
      "Epoch [4062/10000], Loss: 1.7789\n",
      "Epoch [4063/10000], Loss: 1.7787\n",
      "Epoch [4064/10000], Loss: 1.7786\n",
      "Epoch [4065/10000], Loss: 1.7784\n",
      "Epoch [4066/10000], Loss: 1.7783\n",
      "Epoch [4067/10000], Loss: 1.7781\n",
      "Epoch [4068/10000], Loss: 1.7780\n",
      "Epoch [4069/10000], Loss: 1.7778\n",
      "Epoch [4070/10000], Loss: 1.7777\n",
      "Epoch [4071/10000], Loss: 1.7775\n",
      "Epoch [4072/10000], Loss: 1.7774\n",
      "Epoch [4073/10000], Loss: 1.7772\n",
      "Epoch [4074/10000], Loss: 1.7771\n",
      "Epoch [4075/10000], Loss: 1.7769\n",
      "Epoch [4076/10000], Loss: 1.7768\n",
      "Epoch [4077/10000], Loss: 1.7766\n",
      "Epoch [4078/10000], Loss: 1.7765\n",
      "Epoch [4079/10000], Loss: 1.7764\n",
      "Epoch [4080/10000], Loss: 1.7762\n",
      "Epoch [4081/10000], Loss: 1.7761\n",
      "Epoch [4082/10000], Loss: 1.7759\n",
      "Epoch [4083/10000], Loss: 1.7758\n",
      "Epoch [4084/10000], Loss: 1.7756\n",
      "Epoch [4085/10000], Loss: 1.7755\n",
      "Epoch [4086/10000], Loss: 1.7753\n",
      "Epoch [4087/10000], Loss: 1.7752\n",
      "Epoch [4088/10000], Loss: 1.7750\n",
      "Epoch [4089/10000], Loss: 1.7749\n",
      "Epoch [4090/10000], Loss: 1.7747\n",
      "Epoch [4091/10000], Loss: 1.7746\n",
      "Epoch [4092/10000], Loss: 1.7744\n",
      "Epoch [4093/10000], Loss: 1.7743\n",
      "Epoch [4094/10000], Loss: 1.7741\n",
      "Epoch [4095/10000], Loss: 1.7740\n",
      "Epoch [4096/10000], Loss: 1.7739\n",
      "Epoch [4097/10000], Loss: 1.7737\n",
      "Epoch [4098/10000], Loss: 1.7736\n",
      "Epoch [4099/10000], Loss: 1.7734\n",
      "Epoch [4100/10000], Loss: 1.7733\n",
      "Epoch [4101/10000], Loss: 1.7731\n",
      "Epoch [4102/10000], Loss: 1.7730\n",
      "Epoch [4103/10000], Loss: 1.7728\n",
      "Epoch [4104/10000], Loss: 1.7727\n",
      "Epoch [4105/10000], Loss: 1.7725\n",
      "Epoch [4106/10000], Loss: 1.7724\n",
      "Epoch [4107/10000], Loss: 1.7722\n",
      "Epoch [4108/10000], Loss: 1.7721\n",
      "Epoch [4109/10000], Loss: 1.7719\n",
      "Epoch [4110/10000], Loss: 1.7718\n",
      "Epoch [4111/10000], Loss: 1.7716\n",
      "Epoch [4112/10000], Loss: 1.7715\n",
      "Epoch [4113/10000], Loss: 1.7714\n",
      "Epoch [4114/10000], Loss: 1.7712\n",
      "Epoch [4115/10000], Loss: 1.7711\n",
      "Epoch [4116/10000], Loss: 1.7709\n",
      "Epoch [4117/10000], Loss: 1.7708\n",
      "Epoch [4118/10000], Loss: 1.7706\n",
      "Epoch [4119/10000], Loss: 1.7705\n",
      "Epoch [4120/10000], Loss: 1.7703\n",
      "Epoch [4121/10000], Loss: 1.7702\n",
      "Epoch [4122/10000], Loss: 1.7700\n",
      "Epoch [4123/10000], Loss: 1.7699\n",
      "Epoch [4124/10000], Loss: 1.7697\n",
      "Epoch [4125/10000], Loss: 1.7696\n",
      "Epoch [4126/10000], Loss: 1.7694\n",
      "Epoch [4127/10000], Loss: 1.7693\n",
      "Epoch [4128/10000], Loss: 1.7692\n",
      "Epoch [4129/10000], Loss: 1.7690\n",
      "Epoch [4130/10000], Loss: 1.7689\n",
      "Epoch [4131/10000], Loss: 1.7687\n",
      "Epoch [4132/10000], Loss: 1.7686\n",
      "Epoch [4133/10000], Loss: 1.7684\n",
      "Epoch [4134/10000], Loss: 1.7683\n",
      "Epoch [4135/10000], Loss: 1.7681\n",
      "Epoch [4136/10000], Loss: 1.7680\n",
      "Epoch [4137/10000], Loss: 1.7678\n",
      "Epoch [4138/10000], Loss: 1.7677\n",
      "Epoch [4139/10000], Loss: 1.7675\n",
      "Epoch [4140/10000], Loss: 1.7674\n",
      "Epoch [4141/10000], Loss: 1.7672\n",
      "Epoch [4142/10000], Loss: 1.7671\n",
      "Epoch [4143/10000], Loss: 1.7670\n",
      "Epoch [4144/10000], Loss: 1.7668\n",
      "Epoch [4145/10000], Loss: 1.7667\n",
      "Epoch [4146/10000], Loss: 1.7665\n",
      "Epoch [4147/10000], Loss: 1.7664\n",
      "Epoch [4148/10000], Loss: 1.7662\n",
      "Epoch [4149/10000], Loss: 1.7661\n",
      "Epoch [4150/10000], Loss: 1.7659\n",
      "Epoch [4151/10000], Loss: 1.7658\n",
      "Epoch [4152/10000], Loss: 1.7656\n",
      "Epoch [4153/10000], Loss: 1.7655\n",
      "Epoch [4154/10000], Loss: 1.7653\n",
      "Epoch [4155/10000], Loss: 1.7652\n",
      "Epoch [4156/10000], Loss: 1.7650\n",
      "Epoch [4157/10000], Loss: 1.7649\n",
      "Epoch [4158/10000], Loss: 1.7648\n",
      "Epoch [4159/10000], Loss: 1.7646\n",
      "Epoch [4160/10000], Loss: 1.7645\n",
      "Epoch [4161/10000], Loss: 1.7643\n",
      "Epoch [4162/10000], Loss: 1.7642\n",
      "Epoch [4163/10000], Loss: 1.7640\n",
      "Epoch [4164/10000], Loss: 1.7639\n",
      "Epoch [4165/10000], Loss: 1.7637\n",
      "Epoch [4166/10000], Loss: 1.7636\n",
      "Epoch [4167/10000], Loss: 1.7634\n",
      "Epoch [4168/10000], Loss: 1.7633\n",
      "Epoch [4169/10000], Loss: 1.7631\n",
      "Epoch [4170/10000], Loss: 1.7630\n",
      "Epoch [4171/10000], Loss: 1.7629\n",
      "Epoch [4172/10000], Loss: 1.7627\n",
      "Epoch [4173/10000], Loss: 1.7626\n",
      "Epoch [4174/10000], Loss: 1.7624\n",
      "Epoch [4175/10000], Loss: 1.7623\n",
      "Epoch [4176/10000], Loss: 1.7621\n",
      "Epoch [4177/10000], Loss: 1.7620\n",
      "Epoch [4178/10000], Loss: 1.7618\n",
      "Epoch [4179/10000], Loss: 1.7617\n",
      "Epoch [4180/10000], Loss: 1.7615\n",
      "Epoch [4181/10000], Loss: 1.7614\n",
      "Epoch [4182/10000], Loss: 1.7612\n",
      "Epoch [4183/10000], Loss: 1.7611\n",
      "Epoch [4184/10000], Loss: 1.7610\n",
      "Epoch [4185/10000], Loss: 1.7608\n",
      "Epoch [4186/10000], Loss: 1.7607\n",
      "Epoch [4187/10000], Loss: 1.7605\n",
      "Epoch [4188/10000], Loss: 1.7604\n",
      "Epoch [4189/10000], Loss: 1.7602\n",
      "Epoch [4190/10000], Loss: 1.7601\n",
      "Epoch [4191/10000], Loss: 1.7599\n",
      "Epoch [4192/10000], Loss: 1.7598\n",
      "Epoch [4193/10000], Loss: 1.7596\n",
      "Epoch [4194/10000], Loss: 1.7595\n",
      "Epoch [4195/10000], Loss: 1.7593\n",
      "Epoch [4196/10000], Loss: 1.7592\n",
      "Epoch [4197/10000], Loss: 1.7591\n",
      "Epoch [4198/10000], Loss: 1.7589\n",
      "Epoch [4199/10000], Loss: 1.7588\n",
      "Epoch [4200/10000], Loss: 1.7586\n",
      "Epoch [4201/10000], Loss: 1.7585\n",
      "Epoch [4202/10000], Loss: 1.7583\n",
      "Epoch [4203/10000], Loss: 1.7582\n",
      "Epoch [4204/10000], Loss: 1.7580\n",
      "Epoch [4205/10000], Loss: 1.7579\n",
      "Epoch [4206/10000], Loss: 1.7577\n",
      "Epoch [4207/10000], Loss: 1.7576\n",
      "Epoch [4208/10000], Loss: 1.7575\n",
      "Epoch [4209/10000], Loss: 1.7573\n",
      "Epoch [4210/10000], Loss: 1.7572\n",
      "Epoch [4211/10000], Loss: 1.7570\n",
      "Epoch [4212/10000], Loss: 1.7569\n",
      "Epoch [4213/10000], Loss: 1.7567\n",
      "Epoch [4214/10000], Loss: 1.7566\n",
      "Epoch [4215/10000], Loss: 1.7564\n",
      "Epoch [4216/10000], Loss: 1.7563\n",
      "Epoch [4217/10000], Loss: 1.7561\n",
      "Epoch [4218/10000], Loss: 1.7560\n",
      "Epoch [4219/10000], Loss: 1.7558\n",
      "Epoch [4220/10000], Loss: 1.7557\n",
      "Epoch [4221/10000], Loss: 1.7556\n",
      "Epoch [4222/10000], Loss: 1.7554\n",
      "Epoch [4223/10000], Loss: 1.7553\n",
      "Epoch [4224/10000], Loss: 1.7551\n",
      "Epoch [4225/10000], Loss: 1.7550\n",
      "Epoch [4226/10000], Loss: 1.7548\n",
      "Epoch [4227/10000], Loss: 1.7547\n",
      "Epoch [4228/10000], Loss: 1.7545\n",
      "Epoch [4229/10000], Loss: 1.7544\n",
      "Epoch [4230/10000], Loss: 1.7542\n",
      "Epoch [4231/10000], Loss: 1.7541\n",
      "Epoch [4232/10000], Loss: 1.7540\n",
      "Epoch [4233/10000], Loss: 1.7538\n",
      "Epoch [4234/10000], Loss: 1.7537\n",
      "Epoch [4235/10000], Loss: 1.7535\n",
      "Epoch [4236/10000], Loss: 1.7534\n",
      "Epoch [4237/10000], Loss: 1.7532\n",
      "Epoch [4238/10000], Loss: 1.7531\n",
      "Epoch [4239/10000], Loss: 1.7529\n",
      "Epoch [4240/10000], Loss: 1.7528\n",
      "Epoch [4241/10000], Loss: 1.7526\n",
      "Epoch [4242/10000], Loss: 1.7525\n",
      "Epoch [4243/10000], Loss: 1.7524\n",
      "Epoch [4244/10000], Loss: 1.7522\n",
      "Epoch [4245/10000], Loss: 1.7521\n",
      "Epoch [4246/10000], Loss: 1.7519\n",
      "Epoch [4247/10000], Loss: 1.7518\n",
      "Epoch [4248/10000], Loss: 1.7516\n",
      "Epoch [4249/10000], Loss: 1.7515\n",
      "Epoch [4250/10000], Loss: 1.7513\n",
      "Epoch [4251/10000], Loss: 1.7512\n",
      "Epoch [4252/10000], Loss: 1.7510\n",
      "Epoch [4253/10000], Loss: 1.7509\n",
      "Epoch [4254/10000], Loss: 1.7508\n",
      "Epoch [4255/10000], Loss: 1.7506\n",
      "Epoch [4256/10000], Loss: 1.7505\n",
      "Epoch [4257/10000], Loss: 1.7503\n",
      "Epoch [4258/10000], Loss: 1.7502\n",
      "Epoch [4259/10000], Loss: 1.7500\n",
      "Epoch [4260/10000], Loss: 1.7499\n",
      "Epoch [4261/10000], Loss: 1.7497\n",
      "Epoch [4262/10000], Loss: 1.7496\n",
      "Epoch [4263/10000], Loss: 1.7494\n",
      "Epoch [4264/10000], Loss: 1.7493\n",
      "Epoch [4265/10000], Loss: 1.7492\n",
      "Epoch [4266/10000], Loss: 1.7490\n",
      "Epoch [4267/10000], Loss: 1.7489\n",
      "Epoch [4268/10000], Loss: 1.7487\n",
      "Epoch [4269/10000], Loss: 1.7486\n",
      "Epoch [4270/10000], Loss: 1.7484\n",
      "Epoch [4271/10000], Loss: 1.7483\n",
      "Epoch [4272/10000], Loss: 1.7481\n",
      "Epoch [4273/10000], Loss: 1.7480\n",
      "Epoch [4274/10000], Loss: 1.7479\n",
      "Epoch [4275/10000], Loss: 1.7477\n",
      "Epoch [4276/10000], Loss: 1.7476\n",
      "Epoch [4277/10000], Loss: 1.7474\n",
      "Epoch [4278/10000], Loss: 1.7473\n",
      "Epoch [4279/10000], Loss: 1.7471\n",
      "Epoch [4280/10000], Loss: 1.7470\n",
      "Epoch [4281/10000], Loss: 1.7468\n",
      "Epoch [4282/10000], Loss: 1.7467\n",
      "Epoch [4283/10000], Loss: 1.7465\n",
      "Epoch [4284/10000], Loss: 1.7464\n",
      "Epoch [4285/10000], Loss: 1.7463\n",
      "Epoch [4286/10000], Loss: 1.7461\n",
      "Epoch [4287/10000], Loss: 1.7460\n",
      "Epoch [4288/10000], Loss: 1.7458\n",
      "Epoch [4289/10000], Loss: 1.7457\n",
      "Epoch [4290/10000], Loss: 1.7455\n",
      "Epoch [4291/10000], Loss: 1.7454\n",
      "Epoch [4292/10000], Loss: 1.7452\n",
      "Epoch [4293/10000], Loss: 1.7451\n",
      "Epoch [4294/10000], Loss: 1.7450\n",
      "Epoch [4295/10000], Loss: 1.7448\n",
      "Epoch [4296/10000], Loss: 1.7447\n",
      "Epoch [4297/10000], Loss: 1.7445\n",
      "Epoch [4298/10000], Loss: 1.7444\n",
      "Epoch [4299/10000], Loss: 1.7442\n",
      "Epoch [4300/10000], Loss: 1.7441\n",
      "Epoch [4301/10000], Loss: 1.7439\n",
      "Epoch [4302/10000], Loss: 1.7438\n",
      "Epoch [4303/10000], Loss: 1.7436\n",
      "Epoch [4304/10000], Loss: 1.7435\n",
      "Epoch [4305/10000], Loss: 1.7434\n",
      "Epoch [4306/10000], Loss: 1.7432\n",
      "Epoch [4307/10000], Loss: 1.7431\n",
      "Epoch [4308/10000], Loss: 1.7429\n",
      "Epoch [4309/10000], Loss: 1.7428\n",
      "Epoch [4310/10000], Loss: 1.7426\n",
      "Epoch [4311/10000], Loss: 1.7425\n",
      "Epoch [4312/10000], Loss: 1.7423\n",
      "Epoch [4313/10000], Loss: 1.7422\n",
      "Epoch [4314/10000], Loss: 1.7421\n",
      "Epoch [4315/10000], Loss: 1.7419\n",
      "Epoch [4316/10000], Loss: 1.7418\n",
      "Epoch [4317/10000], Loss: 1.7416\n",
      "Epoch [4318/10000], Loss: 1.7415\n",
      "Epoch [4319/10000], Loss: 1.7413\n",
      "Epoch [4320/10000], Loss: 1.7412\n",
      "Epoch [4321/10000], Loss: 1.7410\n",
      "Epoch [4322/10000], Loss: 1.7409\n",
      "Epoch [4323/10000], Loss: 1.7408\n",
      "Epoch [4324/10000], Loss: 1.7406\n",
      "Epoch [4325/10000], Loss: 1.7405\n",
      "Epoch [4326/10000], Loss: 1.7403\n",
      "Epoch [4327/10000], Loss: 1.7402\n",
      "Epoch [4328/10000], Loss: 1.7400\n",
      "Epoch [4329/10000], Loss: 1.7399\n",
      "Epoch [4330/10000], Loss: 1.7397\n",
      "Epoch [4331/10000], Loss: 1.7396\n",
      "Epoch [4332/10000], Loss: 1.7395\n",
      "Epoch [4333/10000], Loss: 1.7393\n",
      "Epoch [4334/10000], Loss: 1.7392\n",
      "Epoch [4335/10000], Loss: 1.7390\n",
      "Epoch [4336/10000], Loss: 1.7389\n",
      "Epoch [4337/10000], Loss: 1.7387\n",
      "Epoch [4338/10000], Loss: 1.7386\n",
      "Epoch [4339/10000], Loss: 1.7384\n",
      "Epoch [4340/10000], Loss: 1.7383\n",
      "Epoch [4341/10000], Loss: 1.7382\n",
      "Epoch [4342/10000], Loss: 1.7380\n",
      "Epoch [4343/10000], Loss: 1.7379\n",
      "Epoch [4344/10000], Loss: 1.7377\n",
      "Epoch [4345/10000], Loss: 1.7376\n",
      "Epoch [4346/10000], Loss: 1.7374\n",
      "Epoch [4347/10000], Loss: 1.7373\n",
      "Epoch [4348/10000], Loss: 1.7371\n",
      "Epoch [4349/10000], Loss: 1.7370\n",
      "Epoch [4350/10000], Loss: 1.7369\n",
      "Epoch [4351/10000], Loss: 1.7367\n",
      "Epoch [4352/10000], Loss: 1.7366\n",
      "Epoch [4353/10000], Loss: 1.7364\n",
      "Epoch [4354/10000], Loss: 1.7363\n",
      "Epoch [4355/10000], Loss: 1.7361\n",
      "Epoch [4356/10000], Loss: 1.7360\n",
      "Epoch [4357/10000], Loss: 1.7358\n",
      "Epoch [4358/10000], Loss: 1.7357\n",
      "Epoch [4359/10000], Loss: 1.7356\n",
      "Epoch [4360/10000], Loss: 1.7354\n",
      "Epoch [4361/10000], Loss: 1.7353\n",
      "Epoch [4362/10000], Loss: 1.7351\n",
      "Epoch [4363/10000], Loss: 1.7350\n",
      "Epoch [4364/10000], Loss: 1.7348\n",
      "Epoch [4365/10000], Loss: 1.7347\n",
      "Epoch [4366/10000], Loss: 1.7345\n",
      "Epoch [4367/10000], Loss: 1.7344\n",
      "Epoch [4368/10000], Loss: 1.7343\n",
      "Epoch [4369/10000], Loss: 1.7341\n",
      "Epoch [4370/10000], Loss: 1.7340\n",
      "Epoch [4371/10000], Loss: 1.7338\n",
      "Epoch [4372/10000], Loss: 1.7337\n",
      "Epoch [4373/10000], Loss: 1.7335\n",
      "Epoch [4374/10000], Loss: 1.7334\n",
      "Epoch [4375/10000], Loss: 1.7333\n",
      "Epoch [4376/10000], Loss: 1.7331\n",
      "Epoch [4377/10000], Loss: 1.7330\n",
      "Epoch [4378/10000], Loss: 1.7328\n",
      "Epoch [4379/10000], Loss: 1.7327\n",
      "Epoch [4380/10000], Loss: 1.7325\n",
      "Epoch [4381/10000], Loss: 1.7324\n",
      "Epoch [4382/10000], Loss: 1.7322\n",
      "Epoch [4383/10000], Loss: 1.7321\n",
      "Epoch [4384/10000], Loss: 1.7320\n",
      "Epoch [4385/10000], Loss: 1.7318\n",
      "Epoch [4386/10000], Loss: 1.7317\n",
      "Epoch [4387/10000], Loss: 1.7315\n",
      "Epoch [4388/10000], Loss: 1.7314\n",
      "Epoch [4389/10000], Loss: 1.7312\n",
      "Epoch [4390/10000], Loss: 1.7311\n",
      "Epoch [4391/10000], Loss: 1.7309\n",
      "Epoch [4392/10000], Loss: 1.7308\n",
      "Epoch [4393/10000], Loss: 1.7307\n",
      "Epoch [4394/10000], Loss: 1.7305\n",
      "Epoch [4395/10000], Loss: 1.7304\n",
      "Epoch [4396/10000], Loss: 1.7302\n",
      "Epoch [4397/10000], Loss: 1.7301\n",
      "Epoch [4398/10000], Loss: 1.7299\n",
      "Epoch [4399/10000], Loss: 1.7298\n",
      "Epoch [4400/10000], Loss: 1.7297\n",
      "Epoch [4401/10000], Loss: 1.7295\n",
      "Epoch [4402/10000], Loss: 1.7294\n",
      "Epoch [4403/10000], Loss: 1.7292\n",
      "Epoch [4404/10000], Loss: 1.7291\n",
      "Epoch [4405/10000], Loss: 1.7289\n",
      "Epoch [4406/10000], Loss: 1.7288\n",
      "Epoch [4407/10000], Loss: 1.7286\n",
      "Epoch [4408/10000], Loss: 1.7285\n",
      "Epoch [4409/10000], Loss: 1.7284\n",
      "Epoch [4410/10000], Loss: 1.7282\n",
      "Epoch [4411/10000], Loss: 1.7281\n",
      "Epoch [4412/10000], Loss: 1.7279\n",
      "Epoch [4413/10000], Loss: 1.7278\n",
      "Epoch [4414/10000], Loss: 1.7276\n",
      "Epoch [4415/10000], Loss: 1.7275\n",
      "Epoch [4416/10000], Loss: 1.7274\n",
      "Epoch [4417/10000], Loss: 1.7272\n",
      "Epoch [4418/10000], Loss: 1.7271\n",
      "Epoch [4419/10000], Loss: 1.7269\n",
      "Epoch [4420/10000], Loss: 1.7268\n",
      "Epoch [4421/10000], Loss: 1.7266\n",
      "Epoch [4422/10000], Loss: 1.7265\n",
      "Epoch [4423/10000], Loss: 1.7263\n",
      "Epoch [4424/10000], Loss: 1.7262\n",
      "Epoch [4425/10000], Loss: 1.7261\n",
      "Epoch [4426/10000], Loss: 1.7259\n",
      "Epoch [4427/10000], Loss: 1.7258\n",
      "Epoch [4428/10000], Loss: 1.7256\n",
      "Epoch [4429/10000], Loss: 1.7255\n",
      "Epoch [4430/10000], Loss: 1.7253\n",
      "Epoch [4431/10000], Loss: 1.7252\n",
      "Epoch [4432/10000], Loss: 1.7251\n",
      "Epoch [4433/10000], Loss: 1.7249\n",
      "Epoch [4434/10000], Loss: 1.7248\n",
      "Epoch [4435/10000], Loss: 1.7246\n",
      "Epoch [4436/10000], Loss: 1.7245\n",
      "Epoch [4437/10000], Loss: 1.7243\n",
      "Epoch [4438/10000], Loss: 1.7242\n",
      "Epoch [4439/10000], Loss: 1.7241\n",
      "Epoch [4440/10000], Loss: 1.7239\n",
      "Epoch [4441/10000], Loss: 1.7238\n",
      "Epoch [4442/10000], Loss: 1.7236\n",
      "Epoch [4443/10000], Loss: 1.7235\n",
      "Epoch [4444/10000], Loss: 1.7233\n",
      "Epoch [4445/10000], Loss: 1.7232\n",
      "Epoch [4446/10000], Loss: 1.7230\n",
      "Epoch [4447/10000], Loss: 1.7229\n",
      "Epoch [4448/10000], Loss: 1.7228\n",
      "Epoch [4449/10000], Loss: 1.7226\n",
      "Epoch [4450/10000], Loss: 1.7225\n",
      "Epoch [4451/10000], Loss: 1.7223\n",
      "Epoch [4452/10000], Loss: 1.7222\n",
      "Epoch [4453/10000], Loss: 1.7220\n",
      "Epoch [4454/10000], Loss: 1.7219\n",
      "Epoch [4455/10000], Loss: 1.7218\n",
      "Epoch [4456/10000], Loss: 1.7216\n",
      "Epoch [4457/10000], Loss: 1.7215\n",
      "Epoch [4458/10000], Loss: 1.7213\n",
      "Epoch [4459/10000], Loss: 1.7212\n",
      "Epoch [4460/10000], Loss: 1.7210\n",
      "Epoch [4461/10000], Loss: 1.7209\n",
      "Epoch [4462/10000], Loss: 1.7208\n",
      "Epoch [4463/10000], Loss: 1.7206\n",
      "Epoch [4464/10000], Loss: 1.7205\n",
      "Epoch [4465/10000], Loss: 1.7203\n",
      "Epoch [4466/10000], Loss: 1.7202\n",
      "Epoch [4467/10000], Loss: 1.7200\n",
      "Epoch [4468/10000], Loss: 1.7199\n",
      "Epoch [4469/10000], Loss: 1.7198\n",
      "Epoch [4470/10000], Loss: 1.7196\n",
      "Epoch [4471/10000], Loss: 1.7195\n",
      "Epoch [4472/10000], Loss: 1.7193\n",
      "Epoch [4473/10000], Loss: 1.7192\n",
      "Epoch [4474/10000], Loss: 1.7190\n",
      "Epoch [4475/10000], Loss: 1.7189\n",
      "Epoch [4476/10000], Loss: 1.7188\n",
      "Epoch [4477/10000], Loss: 1.7186\n",
      "Epoch [4478/10000], Loss: 1.7185\n",
      "Epoch [4479/10000], Loss: 1.7183\n",
      "Epoch [4480/10000], Loss: 1.7182\n",
      "Epoch [4481/10000], Loss: 1.7180\n",
      "Epoch [4482/10000], Loss: 1.7179\n",
      "Epoch [4483/10000], Loss: 1.7178\n",
      "Epoch [4484/10000], Loss: 1.7176\n",
      "Epoch [4485/10000], Loss: 1.7175\n",
      "Epoch [4486/10000], Loss: 1.7173\n",
      "Epoch [4487/10000], Loss: 1.7172\n",
      "Epoch [4488/10000], Loss: 1.7170\n",
      "Epoch [4489/10000], Loss: 1.7169\n",
      "Epoch [4490/10000], Loss: 1.7167\n",
      "Epoch [4491/10000], Loss: 1.7166\n",
      "Epoch [4492/10000], Loss: 1.7165\n",
      "Epoch [4493/10000], Loss: 1.7163\n",
      "Epoch [4494/10000], Loss: 1.7162\n",
      "Epoch [4495/10000], Loss: 1.7160\n",
      "Epoch [4496/10000], Loss: 1.7159\n",
      "Epoch [4497/10000], Loss: 1.7157\n",
      "Epoch [4498/10000], Loss: 1.7156\n",
      "Epoch [4499/10000], Loss: 1.7155\n",
      "Epoch [4500/10000], Loss: 1.7153\n",
      "Epoch [4501/10000], Loss: 1.7152\n",
      "Epoch [4502/10000], Loss: 1.7150\n",
      "Epoch [4503/10000], Loss: 1.7149\n",
      "Epoch [4504/10000], Loss: 1.7147\n",
      "Epoch [4505/10000], Loss: 1.7146\n",
      "Epoch [4506/10000], Loss: 1.7145\n",
      "Epoch [4507/10000], Loss: 1.7143\n",
      "Epoch [4508/10000], Loss: 1.7142\n",
      "Epoch [4509/10000], Loss: 1.7140\n",
      "Epoch [4510/10000], Loss: 1.7139\n",
      "Epoch [4511/10000], Loss: 1.7138\n",
      "Epoch [4512/10000], Loss: 1.7136\n",
      "Epoch [4513/10000], Loss: 1.7135\n",
      "Epoch [4514/10000], Loss: 1.7133\n",
      "Epoch [4515/10000], Loss: 1.7132\n",
      "Epoch [4516/10000], Loss: 1.7130\n",
      "Epoch [4517/10000], Loss: 1.7129\n",
      "Epoch [4518/10000], Loss: 1.7128\n",
      "Epoch [4519/10000], Loss: 1.7126\n",
      "Epoch [4520/10000], Loss: 1.7125\n",
      "Epoch [4521/10000], Loss: 1.7123\n",
      "Epoch [4522/10000], Loss: 1.7122\n",
      "Epoch [4523/10000], Loss: 1.7120\n",
      "Epoch [4524/10000], Loss: 1.7119\n",
      "Epoch [4525/10000], Loss: 1.7118\n",
      "Epoch [4526/10000], Loss: 1.7116\n",
      "Epoch [4527/10000], Loss: 1.7115\n",
      "Epoch [4528/10000], Loss: 1.7113\n",
      "Epoch [4529/10000], Loss: 1.7112\n",
      "Epoch [4530/10000], Loss: 1.7110\n",
      "Epoch [4531/10000], Loss: 1.7109\n",
      "Epoch [4532/10000], Loss: 1.7108\n",
      "Epoch [4533/10000], Loss: 1.7106\n",
      "Epoch [4534/10000], Loss: 1.7105\n",
      "Epoch [4535/10000], Loss: 1.7103\n",
      "Epoch [4536/10000], Loss: 1.7102\n",
      "Epoch [4537/10000], Loss: 1.7100\n",
      "Epoch [4538/10000], Loss: 1.7099\n",
      "Epoch [4539/10000], Loss: 1.7098\n",
      "Epoch [4540/10000], Loss: 1.7096\n",
      "Epoch [4541/10000], Loss: 1.7095\n",
      "Epoch [4542/10000], Loss: 1.7093\n",
      "Epoch [4543/10000], Loss: 1.7092\n",
      "Epoch [4544/10000], Loss: 1.7090\n",
      "Epoch [4545/10000], Loss: 1.7089\n",
      "Epoch [4546/10000], Loss: 1.7088\n",
      "Epoch [4547/10000], Loss: 1.7086\n",
      "Epoch [4548/10000], Loss: 1.7085\n",
      "Epoch [4549/10000], Loss: 1.7083\n",
      "Epoch [4550/10000], Loss: 1.7082\n",
      "Epoch [4551/10000], Loss: 1.7080\n",
      "Epoch [4552/10000], Loss: 1.7079\n",
      "Epoch [4553/10000], Loss: 1.7078\n",
      "Epoch [4554/10000], Loss: 1.7076\n",
      "Epoch [4555/10000], Loss: 1.7075\n",
      "Epoch [4556/10000], Loss: 1.7073\n",
      "Epoch [4557/10000], Loss: 1.7072\n",
      "Epoch [4558/10000], Loss: 1.7071\n",
      "Epoch [4559/10000], Loss: 1.7069\n",
      "Epoch [4560/10000], Loss: 1.7068\n",
      "Epoch [4561/10000], Loss: 1.7066\n",
      "Epoch [4562/10000], Loss: 1.7065\n",
      "Epoch [4563/10000], Loss: 1.7063\n",
      "Epoch [4564/10000], Loss: 1.7062\n",
      "Epoch [4565/10000], Loss: 1.7061\n",
      "Epoch [4566/10000], Loss: 1.7059\n",
      "Epoch [4567/10000], Loss: 1.7058\n",
      "Epoch [4568/10000], Loss: 1.7056\n",
      "Epoch [4569/10000], Loss: 1.7055\n",
      "Epoch [4570/10000], Loss: 1.7053\n",
      "Epoch [4571/10000], Loss: 1.7052\n",
      "Epoch [4572/10000], Loss: 1.7051\n",
      "Epoch [4573/10000], Loss: 1.7049\n",
      "Epoch [4574/10000], Loss: 1.7048\n",
      "Epoch [4575/10000], Loss: 1.7046\n",
      "Epoch [4576/10000], Loss: 1.7045\n",
      "Epoch [4577/10000], Loss: 1.7044\n",
      "Epoch [4578/10000], Loss: 1.7042\n",
      "Epoch [4579/10000], Loss: 1.7041\n",
      "Epoch [4580/10000], Loss: 1.7039\n",
      "Epoch [4581/10000], Loss: 1.7038\n",
      "Epoch [4582/10000], Loss: 1.7036\n",
      "Epoch [4583/10000], Loss: 1.7035\n",
      "Epoch [4584/10000], Loss: 1.7034\n",
      "Epoch [4585/10000], Loss: 1.7032\n",
      "Epoch [4586/10000], Loss: 1.7031\n",
      "Epoch [4587/10000], Loss: 1.7029\n",
      "Epoch [4588/10000], Loss: 1.7028\n",
      "Epoch [4589/10000], Loss: 1.7026\n",
      "Epoch [4590/10000], Loss: 1.7025\n",
      "Epoch [4591/10000], Loss: 1.7024\n",
      "Epoch [4592/10000], Loss: 1.7022\n",
      "Epoch [4593/10000], Loss: 1.7021\n",
      "Epoch [4594/10000], Loss: 1.7019\n",
      "Epoch [4595/10000], Loss: 1.7018\n",
      "Epoch [4596/10000], Loss: 1.7017\n",
      "Epoch [4597/10000], Loss: 1.7015\n",
      "Epoch [4598/10000], Loss: 1.7014\n",
      "Epoch [4599/10000], Loss: 1.7012\n",
      "Epoch [4600/10000], Loss: 1.7011\n",
      "Epoch [4601/10000], Loss: 1.7009\n",
      "Epoch [4602/10000], Loss: 1.7008\n",
      "Epoch [4603/10000], Loss: 1.7007\n",
      "Epoch [4604/10000], Loss: 1.7005\n",
      "Epoch [4605/10000], Loss: 1.7004\n",
      "Epoch [4606/10000], Loss: 1.7002\n",
      "Epoch [4607/10000], Loss: 1.7001\n",
      "Epoch [4608/10000], Loss: 1.6999\n",
      "Epoch [4609/10000], Loss: 1.6998\n",
      "Epoch [4610/10000], Loss: 1.6997\n",
      "Epoch [4611/10000], Loss: 1.6995\n",
      "Epoch [4612/10000], Loss: 1.6994\n",
      "Epoch [4613/10000], Loss: 1.6992\n",
      "Epoch [4614/10000], Loss: 1.6991\n",
      "Epoch [4615/10000], Loss: 1.6990\n",
      "Epoch [4616/10000], Loss: 1.6988\n",
      "Epoch [4617/10000], Loss: 1.6987\n",
      "Epoch [4618/10000], Loss: 1.6985\n",
      "Epoch [4619/10000], Loss: 1.6984\n",
      "Epoch [4620/10000], Loss: 1.6982\n",
      "Epoch [4621/10000], Loss: 1.6981\n",
      "Epoch [4622/10000], Loss: 1.6980\n",
      "Epoch [4623/10000], Loss: 1.6978\n",
      "Epoch [4624/10000], Loss: 1.6977\n",
      "Epoch [4625/10000], Loss: 1.6975\n",
      "Epoch [4626/10000], Loss: 1.6974\n",
      "Epoch [4627/10000], Loss: 1.6973\n",
      "Epoch [4628/10000], Loss: 1.6971\n",
      "Epoch [4629/10000], Loss: 1.6970\n",
      "Epoch [4630/10000], Loss: 1.6968\n",
      "Epoch [4631/10000], Loss: 1.6967\n",
      "Epoch [4632/10000], Loss: 1.6965\n",
      "Epoch [4633/10000], Loss: 1.6964\n",
      "Epoch [4634/10000], Loss: 1.6963\n",
      "Epoch [4635/10000], Loss: 1.6961\n",
      "Epoch [4636/10000], Loss: 1.6960\n",
      "Epoch [4637/10000], Loss: 1.6958\n",
      "Epoch [4638/10000], Loss: 1.6957\n",
      "Epoch [4639/10000], Loss: 1.6956\n",
      "Epoch [4640/10000], Loss: 1.6954\n",
      "Epoch [4641/10000], Loss: 1.6953\n",
      "Epoch [4642/10000], Loss: 1.6951\n",
      "Epoch [4643/10000], Loss: 1.6950\n",
      "Epoch [4644/10000], Loss: 1.6948\n",
      "Epoch [4645/10000], Loss: 1.6947\n",
      "Epoch [4646/10000], Loss: 1.6946\n",
      "Epoch [4647/10000], Loss: 1.6944\n",
      "Epoch [4648/10000], Loss: 1.6943\n",
      "Epoch [4649/10000], Loss: 1.6941\n",
      "Epoch [4650/10000], Loss: 1.6940\n",
      "Epoch [4651/10000], Loss: 1.6939\n",
      "Epoch [4652/10000], Loss: 1.6937\n",
      "Epoch [4653/10000], Loss: 1.6936\n",
      "Epoch [4654/10000], Loss: 1.6934\n",
      "Epoch [4655/10000], Loss: 1.6933\n",
      "Epoch [4656/10000], Loss: 1.6932\n",
      "Epoch [4657/10000], Loss: 1.6930\n",
      "Epoch [4658/10000], Loss: 1.6929\n",
      "Epoch [4659/10000], Loss: 1.6927\n",
      "Epoch [4660/10000], Loss: 1.6926\n",
      "Epoch [4661/10000], Loss: 1.6924\n",
      "Epoch [4662/10000], Loss: 1.6923\n",
      "Epoch [4663/10000], Loss: 1.6922\n",
      "Epoch [4664/10000], Loss: 1.6920\n",
      "Epoch [4665/10000], Loss: 1.6919\n",
      "Epoch [4666/10000], Loss: 1.6917\n",
      "Epoch [4667/10000], Loss: 1.6916\n",
      "Epoch [4668/10000], Loss: 1.6915\n",
      "Epoch [4669/10000], Loss: 1.6913\n",
      "Epoch [4670/10000], Loss: 1.6912\n",
      "Epoch [4671/10000], Loss: 1.6910\n",
      "Epoch [4672/10000], Loss: 1.6909\n",
      "Epoch [4673/10000], Loss: 1.6907\n",
      "Epoch [4674/10000], Loss: 1.6906\n",
      "Epoch [4675/10000], Loss: 1.6905\n",
      "Epoch [4676/10000], Loss: 1.6903\n",
      "Epoch [4677/10000], Loss: 1.6902\n",
      "Epoch [4678/10000], Loss: 1.6900\n",
      "Epoch [4679/10000], Loss: 1.6899\n",
      "Epoch [4680/10000], Loss: 1.6898\n",
      "Epoch [4681/10000], Loss: 1.6896\n",
      "Epoch [4682/10000], Loss: 1.6895\n",
      "Epoch [4683/10000], Loss: 1.6893\n",
      "Epoch [4684/10000], Loss: 1.6892\n",
      "Epoch [4685/10000], Loss: 1.6891\n",
      "Epoch [4686/10000], Loss: 1.6889\n",
      "Epoch [4687/10000], Loss: 1.6888\n",
      "Epoch [4688/10000], Loss: 1.6886\n",
      "Epoch [4689/10000], Loss: 1.6885\n",
      "Epoch [4690/10000], Loss: 1.6883\n",
      "Epoch [4691/10000], Loss: 1.6882\n",
      "Epoch [4692/10000], Loss: 1.6881\n",
      "Epoch [4693/10000], Loss: 1.6879\n",
      "Epoch [4694/10000], Loss: 1.6878\n",
      "Epoch [4695/10000], Loss: 1.6876\n",
      "Epoch [4696/10000], Loss: 1.6875\n",
      "Epoch [4697/10000], Loss: 1.6874\n",
      "Epoch [4698/10000], Loss: 1.6872\n",
      "Epoch [4699/10000], Loss: 1.6871\n",
      "Epoch [4700/10000], Loss: 1.6869\n",
      "Epoch [4701/10000], Loss: 1.6868\n",
      "Epoch [4702/10000], Loss: 1.6867\n",
      "Epoch [4703/10000], Loss: 1.6865\n",
      "Epoch [4704/10000], Loss: 1.6864\n",
      "Epoch [4705/10000], Loss: 1.6862\n",
      "Epoch [4706/10000], Loss: 1.6861\n",
      "Epoch [4707/10000], Loss: 1.6859\n",
      "Epoch [4708/10000], Loss: 1.6858\n",
      "Epoch [4709/10000], Loss: 1.6857\n",
      "Epoch [4710/10000], Loss: 1.6855\n",
      "Epoch [4711/10000], Loss: 1.6854\n",
      "Epoch [4712/10000], Loss: 1.6852\n",
      "Epoch [4713/10000], Loss: 1.6851\n",
      "Epoch [4714/10000], Loss: 1.6850\n",
      "Epoch [4715/10000], Loss: 1.6848\n",
      "Epoch [4716/10000], Loss: 1.6847\n",
      "Epoch [4717/10000], Loss: 1.6845\n",
      "Epoch [4718/10000], Loss: 1.6844\n",
      "Epoch [4719/10000], Loss: 1.6843\n",
      "Epoch [4720/10000], Loss: 1.6841\n",
      "Epoch [4721/10000], Loss: 1.6840\n",
      "Epoch [4722/10000], Loss: 1.6838\n",
      "Epoch [4723/10000], Loss: 1.6837\n",
      "Epoch [4724/10000], Loss: 1.6836\n",
      "Epoch [4725/10000], Loss: 1.6834\n",
      "Epoch [4726/10000], Loss: 1.6833\n",
      "Epoch [4727/10000], Loss: 1.6831\n",
      "Epoch [4728/10000], Loss: 1.6830\n",
      "Epoch [4729/10000], Loss: 1.6828\n",
      "Epoch [4730/10000], Loss: 1.6827\n",
      "Epoch [4731/10000], Loss: 1.6826\n",
      "Epoch [4732/10000], Loss: 1.6824\n",
      "Epoch [4733/10000], Loss: 1.6823\n",
      "Epoch [4734/10000], Loss: 1.6821\n",
      "Epoch [4735/10000], Loss: 1.6820\n",
      "Epoch [4736/10000], Loss: 1.6819\n",
      "Epoch [4737/10000], Loss: 1.6817\n",
      "Epoch [4738/10000], Loss: 1.6816\n",
      "Epoch [4739/10000], Loss: 1.6814\n",
      "Epoch [4740/10000], Loss: 1.6813\n",
      "Epoch [4741/10000], Loss: 1.6812\n",
      "Epoch [4742/10000], Loss: 1.6810\n",
      "Epoch [4743/10000], Loss: 1.6809\n",
      "Epoch [4744/10000], Loss: 1.6807\n",
      "Epoch [4745/10000], Loss: 1.6806\n",
      "Epoch [4746/10000], Loss: 1.6805\n",
      "Epoch [4747/10000], Loss: 1.6803\n",
      "Epoch [4748/10000], Loss: 1.6802\n",
      "Epoch [4749/10000], Loss: 1.6800\n",
      "Epoch [4750/10000], Loss: 1.6799\n",
      "Epoch [4751/10000], Loss: 1.6798\n",
      "Epoch [4752/10000], Loss: 1.6796\n",
      "Epoch [4753/10000], Loss: 1.6795\n",
      "Epoch [4754/10000], Loss: 1.6793\n",
      "Epoch [4755/10000], Loss: 1.6792\n",
      "Epoch [4756/10000], Loss: 1.6790\n",
      "Epoch [4757/10000], Loss: 1.6789\n",
      "Epoch [4758/10000], Loss: 1.6788\n",
      "Epoch [4759/10000], Loss: 1.6786\n",
      "Epoch [4760/10000], Loss: 1.6785\n",
      "Epoch [4761/10000], Loss: 1.6783\n",
      "Epoch [4762/10000], Loss: 1.6782\n",
      "Epoch [4763/10000], Loss: 1.6781\n",
      "Epoch [4764/10000], Loss: 1.6779\n",
      "Epoch [4765/10000], Loss: 1.6778\n",
      "Epoch [4766/10000], Loss: 1.6776\n",
      "Epoch [4767/10000], Loss: 1.6775\n",
      "Epoch [4768/10000], Loss: 1.6774\n",
      "Epoch [4769/10000], Loss: 1.6772\n",
      "Epoch [4770/10000], Loss: 1.6771\n",
      "Epoch [4771/10000], Loss: 1.6769\n",
      "Epoch [4772/10000], Loss: 1.6768\n",
      "Epoch [4773/10000], Loss: 1.6767\n",
      "Epoch [4774/10000], Loss: 1.6765\n",
      "Epoch [4775/10000], Loss: 1.6764\n",
      "Epoch [4776/10000], Loss: 1.6762\n",
      "Epoch [4777/10000], Loss: 1.6761\n",
      "Epoch [4778/10000], Loss: 1.6760\n",
      "Epoch [4779/10000], Loss: 1.6758\n",
      "Epoch [4780/10000], Loss: 1.6757\n",
      "Epoch [4781/10000], Loss: 1.6755\n",
      "Epoch [4782/10000], Loss: 1.6754\n",
      "Epoch [4783/10000], Loss: 1.6753\n",
      "Epoch [4784/10000], Loss: 1.6751\n",
      "Epoch [4785/10000], Loss: 1.6750\n",
      "Epoch [4786/10000], Loss: 1.6748\n",
      "Epoch [4787/10000], Loss: 1.6747\n",
      "Epoch [4788/10000], Loss: 1.6746\n",
      "Epoch [4789/10000], Loss: 1.6744\n",
      "Epoch [4790/10000], Loss: 1.6743\n",
      "Epoch [4791/10000], Loss: 1.6741\n",
      "Epoch [4792/10000], Loss: 1.6740\n",
      "Epoch [4793/10000], Loss: 1.6739\n",
      "Epoch [4794/10000], Loss: 1.6737\n",
      "Epoch [4795/10000], Loss: 1.6736\n",
      "Epoch [4796/10000], Loss: 1.6734\n",
      "Epoch [4797/10000], Loss: 1.6733\n",
      "Epoch [4798/10000], Loss: 1.6732\n",
      "Epoch [4799/10000], Loss: 1.6730\n",
      "Epoch [4800/10000], Loss: 1.6729\n",
      "Epoch [4801/10000], Loss: 1.6727\n",
      "Epoch [4802/10000], Loss: 1.6726\n",
      "Epoch [4803/10000], Loss: 1.6725\n",
      "Epoch [4804/10000], Loss: 1.6723\n",
      "Epoch [4805/10000], Loss: 1.6722\n",
      "Epoch [4806/10000], Loss: 1.6720\n",
      "Epoch [4807/10000], Loss: 1.6719\n",
      "Epoch [4808/10000], Loss: 1.6718\n",
      "Epoch [4809/10000], Loss: 1.6716\n",
      "Epoch [4810/10000], Loss: 1.6715\n",
      "Epoch [4811/10000], Loss: 1.6713\n",
      "Epoch [4812/10000], Loss: 1.6712\n",
      "Epoch [4813/10000], Loss: 1.6710\n",
      "Epoch [4814/10000], Loss: 1.6709\n",
      "Epoch [4815/10000], Loss: 1.6708\n",
      "Epoch [4816/10000], Loss: 1.6706\n",
      "Epoch [4817/10000], Loss: 1.6705\n",
      "Epoch [4818/10000], Loss: 1.6703\n",
      "Epoch [4819/10000], Loss: 1.6702\n",
      "Epoch [4820/10000], Loss: 1.6701\n",
      "Epoch [4821/10000], Loss: 1.6699\n",
      "Epoch [4822/10000], Loss: 1.6698\n",
      "Epoch [4823/10000], Loss: 1.6696\n",
      "Epoch [4824/10000], Loss: 1.6695\n",
      "Epoch [4825/10000], Loss: 1.6694\n",
      "Epoch [4826/10000], Loss: 1.6692\n",
      "Epoch [4827/10000], Loss: 1.6691\n",
      "Epoch [4828/10000], Loss: 1.6689\n",
      "Epoch [4829/10000], Loss: 1.6688\n",
      "Epoch [4830/10000], Loss: 1.6687\n",
      "Epoch [4831/10000], Loss: 1.6685\n",
      "Epoch [4832/10000], Loss: 1.6684\n",
      "Epoch [4833/10000], Loss: 1.6682\n",
      "Epoch [4834/10000], Loss: 1.6681\n",
      "Epoch [4835/10000], Loss: 1.6680\n",
      "Epoch [4836/10000], Loss: 1.6678\n",
      "Epoch [4837/10000], Loss: 1.6677\n",
      "Epoch [4838/10000], Loss: 1.6675\n",
      "Epoch [4839/10000], Loss: 1.6674\n",
      "Epoch [4840/10000], Loss: 1.6673\n",
      "Epoch [4841/10000], Loss: 1.6671\n",
      "Epoch [4842/10000], Loss: 1.6670\n",
      "Epoch [4843/10000], Loss: 1.6668\n",
      "Epoch [4844/10000], Loss: 1.6667\n",
      "Epoch [4845/10000], Loss: 1.6666\n",
      "Epoch [4846/10000], Loss: 1.6664\n",
      "Epoch [4847/10000], Loss: 1.6663\n",
      "Epoch [4848/10000], Loss: 1.6661\n",
      "Epoch [4849/10000], Loss: 1.6660\n",
      "Epoch [4850/10000], Loss: 1.6659\n",
      "Epoch [4851/10000], Loss: 1.6657\n",
      "Epoch [4852/10000], Loss: 1.6656\n",
      "Epoch [4853/10000], Loss: 1.6655\n",
      "Epoch [4854/10000], Loss: 1.6653\n",
      "Epoch [4855/10000], Loss: 1.6652\n",
      "Epoch [4856/10000], Loss: 1.6650\n",
      "Epoch [4857/10000], Loss: 1.6649\n",
      "Epoch [4858/10000], Loss: 1.6648\n",
      "Epoch [4859/10000], Loss: 1.6646\n",
      "Epoch [4860/10000], Loss: 1.6645\n",
      "Epoch [4861/10000], Loss: 1.6643\n",
      "Epoch [4862/10000], Loss: 1.6642\n",
      "Epoch [4863/10000], Loss: 1.6641\n",
      "Epoch [4864/10000], Loss: 1.6639\n",
      "Epoch [4865/10000], Loss: 1.6638\n",
      "Epoch [4866/10000], Loss: 1.6636\n",
      "Epoch [4867/10000], Loss: 1.6635\n",
      "Epoch [4868/10000], Loss: 1.6634\n",
      "Epoch [4869/10000], Loss: 1.6632\n",
      "Epoch [4870/10000], Loss: 1.6631\n",
      "Epoch [4871/10000], Loss: 1.6629\n",
      "Epoch [4872/10000], Loss: 1.6628\n",
      "Epoch [4873/10000], Loss: 1.6627\n",
      "Epoch [4874/10000], Loss: 1.6625\n",
      "Epoch [4875/10000], Loss: 1.6624\n",
      "Epoch [4876/10000], Loss: 1.6622\n",
      "Epoch [4877/10000], Loss: 1.6621\n",
      "Epoch [4878/10000], Loss: 1.6620\n",
      "Epoch [4879/10000], Loss: 1.6618\n",
      "Epoch [4880/10000], Loss: 1.6617\n",
      "Epoch [4881/10000], Loss: 1.6615\n",
      "Epoch [4882/10000], Loss: 1.6614\n",
      "Epoch [4883/10000], Loss: 1.6613\n",
      "Epoch [4884/10000], Loss: 1.6611\n",
      "Epoch [4885/10000], Loss: 1.6610\n",
      "Epoch [4886/10000], Loss: 1.6608\n",
      "Epoch [4887/10000], Loss: 1.6607\n",
      "Epoch [4888/10000], Loss: 1.6606\n",
      "Epoch [4889/10000], Loss: 1.6604\n",
      "Epoch [4890/10000], Loss: 1.6603\n",
      "Epoch [4891/10000], Loss: 1.6601\n",
      "Epoch [4892/10000], Loss: 1.6600\n",
      "Epoch [4893/10000], Loss: 1.6599\n",
      "Epoch [4894/10000], Loss: 1.6597\n",
      "Epoch [4895/10000], Loss: 1.6596\n",
      "Epoch [4896/10000], Loss: 1.6594\n",
      "Epoch [4897/10000], Loss: 1.6593\n",
      "Epoch [4898/10000], Loss: 1.6592\n",
      "Epoch [4899/10000], Loss: 1.6590\n",
      "Epoch [4900/10000], Loss: 1.6589\n",
      "Epoch [4901/10000], Loss: 1.6587\n",
      "Epoch [4902/10000], Loss: 1.6586\n",
      "Epoch [4903/10000], Loss: 1.6585\n",
      "Epoch [4904/10000], Loss: 1.6583\n",
      "Epoch [4905/10000], Loss: 1.6582\n",
      "Epoch [4906/10000], Loss: 1.6581\n",
      "Epoch [4907/10000], Loss: 1.6579\n",
      "Epoch [4908/10000], Loss: 1.6578\n",
      "Epoch [4909/10000], Loss: 1.6576\n",
      "Epoch [4910/10000], Loss: 1.6575\n",
      "Epoch [4911/10000], Loss: 1.6574\n",
      "Epoch [4912/10000], Loss: 1.6572\n",
      "Epoch [4913/10000], Loss: 1.6571\n",
      "Epoch [4914/10000], Loss: 1.6569\n",
      "Epoch [4915/10000], Loss: 1.6568\n",
      "Epoch [4916/10000], Loss: 1.6567\n",
      "Epoch [4917/10000], Loss: 1.6565\n",
      "Epoch [4918/10000], Loss: 1.6564\n",
      "Epoch [4919/10000], Loss: 1.6562\n",
      "Epoch [4920/10000], Loss: 1.6561\n",
      "Epoch [4921/10000], Loss: 1.6560\n",
      "Epoch [4922/10000], Loss: 1.6558\n",
      "Epoch [4923/10000], Loss: 1.6557\n",
      "Epoch [4924/10000], Loss: 1.6555\n",
      "Epoch [4925/10000], Loss: 1.6554\n",
      "Epoch [4926/10000], Loss: 1.6553\n",
      "Epoch [4927/10000], Loss: 1.6551\n",
      "Epoch [4928/10000], Loss: 1.6550\n",
      "Epoch [4929/10000], Loss: 1.6548\n",
      "Epoch [4930/10000], Loss: 1.6547\n",
      "Epoch [4931/10000], Loss: 1.6546\n",
      "Epoch [4932/10000], Loss: 1.6544\n",
      "Epoch [4933/10000], Loss: 1.6543\n",
      "Epoch [4934/10000], Loss: 1.6541\n",
      "Epoch [4935/10000], Loss: 1.6540\n",
      "Epoch [4936/10000], Loss: 1.6539\n",
      "Epoch [4937/10000], Loss: 1.6537\n",
      "Epoch [4938/10000], Loss: 1.6536\n",
      "Epoch [4939/10000], Loss: 1.6535\n",
      "Epoch [4940/10000], Loss: 1.6533\n",
      "Epoch [4941/10000], Loss: 1.6532\n",
      "Epoch [4942/10000], Loss: 1.6530\n",
      "Epoch [4943/10000], Loss: 1.6529\n",
      "Epoch [4944/10000], Loss: 1.6528\n",
      "Epoch [4945/10000], Loss: 1.6526\n",
      "Epoch [4946/10000], Loss: 1.6525\n",
      "Epoch [4947/10000], Loss: 1.6523\n",
      "Epoch [4948/10000], Loss: 1.6522\n",
      "Epoch [4949/10000], Loss: 1.6521\n",
      "Epoch [4950/10000], Loss: 1.6519\n",
      "Epoch [4951/10000], Loss: 1.6518\n",
      "Epoch [4952/10000], Loss: 1.6516\n",
      "Epoch [4953/10000], Loss: 1.6515\n",
      "Epoch [4954/10000], Loss: 1.6514\n",
      "Epoch [4955/10000], Loss: 1.6512\n",
      "Epoch [4956/10000], Loss: 1.6511\n",
      "Epoch [4957/10000], Loss: 1.6509\n",
      "Epoch [4958/10000], Loss: 1.6508\n",
      "Epoch [4959/10000], Loss: 1.6507\n",
      "Epoch [4960/10000], Loss: 1.6505\n",
      "Epoch [4961/10000], Loss: 1.6504\n",
      "Epoch [4962/10000], Loss: 1.6503\n",
      "Epoch [4963/10000], Loss: 1.6501\n",
      "Epoch [4964/10000], Loss: 1.6500\n",
      "Epoch [4965/10000], Loss: 1.6498\n",
      "Epoch [4966/10000], Loss: 1.6497\n",
      "Epoch [4967/10000], Loss: 1.6496\n",
      "Epoch [4968/10000], Loss: 1.6494\n",
      "Epoch [4969/10000], Loss: 1.6493\n",
      "Epoch [4970/10000], Loss: 1.6491\n",
      "Epoch [4971/10000], Loss: 1.6490\n",
      "Epoch [4972/10000], Loss: 1.6489\n",
      "Epoch [4973/10000], Loss: 1.6487\n",
      "Epoch [4974/10000], Loss: 1.6486\n",
      "Epoch [4975/10000], Loss: 1.6484\n",
      "Epoch [4976/10000], Loss: 1.6483\n",
      "Epoch [4977/10000], Loss: 1.6482\n",
      "Epoch [4978/10000], Loss: 1.6480\n",
      "Epoch [4979/10000], Loss: 1.6479\n",
      "Epoch [4980/10000], Loss: 1.6478\n",
      "Epoch [4981/10000], Loss: 1.6476\n",
      "Epoch [4982/10000], Loss: 1.6475\n",
      "Epoch [4983/10000], Loss: 1.6473\n",
      "Epoch [4984/10000], Loss: 1.6472\n",
      "Epoch [4985/10000], Loss: 1.6471\n",
      "Epoch [4986/10000], Loss: 1.6469\n",
      "Epoch [4987/10000], Loss: 1.6468\n",
      "Epoch [4988/10000], Loss: 1.6466\n",
      "Epoch [4989/10000], Loss: 1.6465\n",
      "Epoch [4990/10000], Loss: 1.6464\n",
      "Epoch [4991/10000], Loss: 1.6462\n",
      "Epoch [4992/10000], Loss: 1.6461\n",
      "Epoch [4993/10000], Loss: 1.6459\n",
      "Epoch [4994/10000], Loss: 1.6458\n",
      "Epoch [4995/10000], Loss: 1.6457\n",
      "Epoch [4996/10000], Loss: 1.6455\n",
      "Epoch [4997/10000], Loss: 1.6454\n",
      "Epoch [4998/10000], Loss: 1.6453\n",
      "Epoch [4999/10000], Loss: 1.6451\n",
      "Epoch [5000/10000], Loss: 1.6450\n",
      "Epoch [5001/10000], Loss: 1.6448\n",
      "Epoch [5002/10000], Loss: 1.6447\n",
      "Epoch [5003/10000], Loss: 1.6446\n",
      "Epoch [5004/10000], Loss: 1.6444\n",
      "Epoch [5005/10000], Loss: 1.6443\n",
      "Epoch [5006/10000], Loss: 1.6441\n",
      "Epoch [5007/10000], Loss: 1.6440\n",
      "Epoch [5008/10000], Loss: 1.6439\n",
      "Epoch [5009/10000], Loss: 1.6437\n",
      "Epoch [5010/10000], Loss: 1.6436\n",
      "Epoch [5011/10000], Loss: 1.6435\n",
      "Epoch [5012/10000], Loss: 1.6433\n",
      "Epoch [5013/10000], Loss: 1.6432\n",
      "Epoch [5014/10000], Loss: 1.6430\n",
      "Epoch [5015/10000], Loss: 1.6429\n",
      "Epoch [5016/10000], Loss: 1.6428\n",
      "Epoch [5017/10000], Loss: 1.6426\n",
      "Epoch [5018/10000], Loss: 1.6425\n",
      "Epoch [5019/10000], Loss: 1.6423\n",
      "Epoch [5020/10000], Loss: 1.6422\n",
      "Epoch [5021/10000], Loss: 1.6421\n",
      "Epoch [5022/10000], Loss: 1.6419\n",
      "Epoch [5023/10000], Loss: 1.6418\n",
      "Epoch [5024/10000], Loss: 1.6417\n",
      "Epoch [5025/10000], Loss: 1.6415\n",
      "Epoch [5026/10000], Loss: 1.6414\n",
      "Epoch [5027/10000], Loss: 1.6412\n",
      "Epoch [5028/10000], Loss: 1.6411\n",
      "Epoch [5029/10000], Loss: 1.6410\n",
      "Epoch [5030/10000], Loss: 1.6408\n",
      "Epoch [5031/10000], Loss: 1.6407\n",
      "Epoch [5032/10000], Loss: 1.6405\n",
      "Epoch [5033/10000], Loss: 1.6404\n",
      "Epoch [5034/10000], Loss: 1.6403\n",
      "Epoch [5035/10000], Loss: 1.6401\n",
      "Epoch [5036/10000], Loss: 1.6400\n",
      "Epoch [5037/10000], Loss: 1.6398\n",
      "Epoch [5038/10000], Loss: 1.6397\n",
      "Epoch [5039/10000], Loss: 1.6396\n",
      "Epoch [5040/10000], Loss: 1.6394\n",
      "Epoch [5041/10000], Loss: 1.6393\n",
      "Epoch [5042/10000], Loss: 1.6392\n",
      "Epoch [5043/10000], Loss: 1.6390\n",
      "Epoch [5044/10000], Loss: 1.6389\n",
      "Epoch [5045/10000], Loss: 1.6387\n",
      "Epoch [5046/10000], Loss: 1.6386\n",
      "Epoch [5047/10000], Loss: 1.6385\n",
      "Epoch [5048/10000], Loss: 1.6383\n",
      "Epoch [5049/10000], Loss: 1.6382\n",
      "Epoch [5050/10000], Loss: 1.6381\n",
      "Epoch [5051/10000], Loss: 1.6379\n",
      "Epoch [5052/10000], Loss: 1.6378\n",
      "Epoch [5053/10000], Loss: 1.6376\n",
      "Epoch [5054/10000], Loss: 1.6375\n",
      "Epoch [5055/10000], Loss: 1.6374\n",
      "Epoch [5056/10000], Loss: 1.6372\n",
      "Epoch [5057/10000], Loss: 1.6371\n",
      "Epoch [5058/10000], Loss: 1.6369\n",
      "Epoch [5059/10000], Loss: 1.6368\n",
      "Epoch [5060/10000], Loss: 1.6367\n",
      "Epoch [5061/10000], Loss: 1.6365\n",
      "Epoch [5062/10000], Loss: 1.6364\n",
      "Epoch [5063/10000], Loss: 1.6363\n",
      "Epoch [5064/10000], Loss: 1.6361\n",
      "Epoch [5065/10000], Loss: 1.6360\n",
      "Epoch [5066/10000], Loss: 1.6358\n",
      "Epoch [5067/10000], Loss: 1.6357\n",
      "Epoch [5068/10000], Loss: 1.6356\n",
      "Epoch [5069/10000], Loss: 1.6354\n",
      "Epoch [5070/10000], Loss: 1.6353\n",
      "Epoch [5071/10000], Loss: 1.6351\n",
      "Epoch [5072/10000], Loss: 1.6350\n",
      "Epoch [5073/10000], Loss: 1.6349\n",
      "Epoch [5074/10000], Loss: 1.6347\n",
      "Epoch [5075/10000], Loss: 1.6346\n",
      "Epoch [5076/10000], Loss: 1.6345\n",
      "Epoch [5077/10000], Loss: 1.6343\n",
      "Epoch [5078/10000], Loss: 1.6342\n",
      "Epoch [5079/10000], Loss: 1.6340\n",
      "Epoch [5080/10000], Loss: 1.6339\n",
      "Epoch [5081/10000], Loss: 1.6338\n",
      "Epoch [5082/10000], Loss: 1.6336\n",
      "Epoch [5083/10000], Loss: 1.6335\n",
      "Epoch [5084/10000], Loss: 1.6333\n",
      "Epoch [5085/10000], Loss: 1.6332\n",
      "Epoch [5086/10000], Loss: 1.6331\n",
      "Epoch [5087/10000], Loss: 1.6329\n",
      "Epoch [5088/10000], Loss: 1.6328\n",
      "Epoch [5089/10000], Loss: 1.6327\n",
      "Epoch [5090/10000], Loss: 1.6325\n",
      "Epoch [5091/10000], Loss: 1.6324\n",
      "Epoch [5092/10000], Loss: 1.6322\n",
      "Epoch [5093/10000], Loss: 1.6321\n",
      "Epoch [5094/10000], Loss: 1.6320\n",
      "Epoch [5095/10000], Loss: 1.6318\n",
      "Epoch [5096/10000], Loss: 1.6317\n",
      "Epoch [5097/10000], Loss: 1.6316\n",
      "Epoch [5098/10000], Loss: 1.6314\n",
      "Epoch [5099/10000], Loss: 1.6313\n",
      "Epoch [5100/10000], Loss: 1.6311\n",
      "Epoch [5101/10000], Loss: 1.6310\n",
      "Epoch [5102/10000], Loss: 1.6309\n",
      "Epoch [5103/10000], Loss: 1.6307\n",
      "Epoch [5104/10000], Loss: 1.6306\n",
      "Epoch [5105/10000], Loss: 1.6305\n",
      "Epoch [5106/10000], Loss: 1.6303\n",
      "Epoch [5107/10000], Loss: 1.6302\n",
      "Epoch [5108/10000], Loss: 1.6300\n",
      "Epoch [5109/10000], Loss: 1.6299\n",
      "Epoch [5110/10000], Loss: 1.6298\n",
      "Epoch [5111/10000], Loss: 1.6296\n",
      "Epoch [5112/10000], Loss: 1.6295\n",
      "Epoch [5113/10000], Loss: 1.6293\n",
      "Epoch [5114/10000], Loss: 1.6292\n",
      "Epoch [5115/10000], Loss: 1.6291\n",
      "Epoch [5116/10000], Loss: 1.6289\n",
      "Epoch [5117/10000], Loss: 1.6288\n",
      "Epoch [5118/10000], Loss: 1.6287\n",
      "Epoch [5119/10000], Loss: 1.6285\n",
      "Epoch [5120/10000], Loss: 1.6284\n",
      "Epoch [5121/10000], Loss: 1.6282\n",
      "Epoch [5122/10000], Loss: 1.6281\n",
      "Epoch [5123/10000], Loss: 1.6280\n",
      "Epoch [5124/10000], Loss: 1.6278\n",
      "Epoch [5125/10000], Loss: 1.6277\n",
      "Epoch [5126/10000], Loss: 1.6276\n",
      "Epoch [5127/10000], Loss: 1.6274\n",
      "Epoch [5128/10000], Loss: 1.6273\n",
      "Epoch [5129/10000], Loss: 1.6271\n",
      "Epoch [5130/10000], Loss: 1.6270\n",
      "Epoch [5131/10000], Loss: 1.6269\n",
      "Epoch [5132/10000], Loss: 1.6267\n",
      "Epoch [5133/10000], Loss: 1.6266\n",
      "Epoch [5134/10000], Loss: 1.6265\n",
      "Epoch [5135/10000], Loss: 1.6263\n",
      "Epoch [5136/10000], Loss: 1.6262\n",
      "Epoch [5137/10000], Loss: 1.6260\n",
      "Epoch [5138/10000], Loss: 1.6259\n",
      "Epoch [5139/10000], Loss: 1.6258\n",
      "Epoch [5140/10000], Loss: 1.6256\n",
      "Epoch [5141/10000], Loss: 1.6255\n",
      "Epoch [5142/10000], Loss: 1.6253\n",
      "Epoch [5143/10000], Loss: 1.6252\n",
      "Epoch [5144/10000], Loss: 1.6251\n",
      "Epoch [5145/10000], Loss: 1.6249\n",
      "Epoch [5146/10000], Loss: 1.6248\n",
      "Epoch [5147/10000], Loss: 1.6247\n",
      "Epoch [5148/10000], Loss: 1.6245\n",
      "Epoch [5149/10000], Loss: 1.6244\n",
      "Epoch [5150/10000], Loss: 1.6242\n",
      "Epoch [5151/10000], Loss: 1.6241\n",
      "Epoch [5152/10000], Loss: 1.6240\n",
      "Epoch [5153/10000], Loss: 1.6238\n",
      "Epoch [5154/10000], Loss: 1.6237\n",
      "Epoch [5155/10000], Loss: 1.6236\n",
      "Epoch [5156/10000], Loss: 1.6234\n",
      "Epoch [5157/10000], Loss: 1.6233\n",
      "Epoch [5158/10000], Loss: 1.6231\n",
      "Epoch [5159/10000], Loss: 1.6230\n",
      "Epoch [5160/10000], Loss: 1.6229\n",
      "Epoch [5161/10000], Loss: 1.6227\n",
      "Epoch [5162/10000], Loss: 1.6226\n",
      "Epoch [5163/10000], Loss: 1.6225\n",
      "Epoch [5164/10000], Loss: 1.6223\n",
      "Epoch [5165/10000], Loss: 1.6222\n",
      "Epoch [5166/10000], Loss: 1.6220\n",
      "Epoch [5167/10000], Loss: 1.6219\n",
      "Epoch [5168/10000], Loss: 1.6218\n",
      "Epoch [5169/10000], Loss: 1.6216\n",
      "Epoch [5170/10000], Loss: 1.6215\n",
      "Epoch [5171/10000], Loss: 1.6214\n",
      "Epoch [5172/10000], Loss: 1.6212\n",
      "Epoch [5173/10000], Loss: 1.6211\n",
      "Epoch [5174/10000], Loss: 1.6209\n",
      "Epoch [5175/10000], Loss: 1.6208\n",
      "Epoch [5176/10000], Loss: 1.6207\n",
      "Epoch [5177/10000], Loss: 1.6205\n",
      "Epoch [5178/10000], Loss: 1.6204\n",
      "Epoch [5179/10000], Loss: 1.6203\n",
      "Epoch [5180/10000], Loss: 1.6201\n",
      "Epoch [5181/10000], Loss: 1.6200\n",
      "Epoch [5182/10000], Loss: 1.6198\n",
      "Epoch [5183/10000], Loss: 1.6197\n",
      "Epoch [5184/10000], Loss: 1.6196\n",
      "Epoch [5185/10000], Loss: 1.6194\n",
      "Epoch [5186/10000], Loss: 1.6193\n",
      "Epoch [5187/10000], Loss: 1.6192\n",
      "Epoch [5188/10000], Loss: 1.6190\n",
      "Epoch [5189/10000], Loss: 1.6189\n",
      "Epoch [5190/10000], Loss: 1.6187\n",
      "Epoch [5191/10000], Loss: 1.6186\n",
      "Epoch [5192/10000], Loss: 1.6185\n",
      "Epoch [5193/10000], Loss: 1.6183\n",
      "Epoch [5194/10000], Loss: 1.6182\n",
      "Epoch [5195/10000], Loss: 1.6181\n",
      "Epoch [5196/10000], Loss: 1.6179\n",
      "Epoch [5197/10000], Loss: 1.6178\n",
      "Epoch [5198/10000], Loss: 1.6176\n",
      "Epoch [5199/10000], Loss: 1.6175\n",
      "Epoch [5200/10000], Loss: 1.6174\n",
      "Epoch [5201/10000], Loss: 1.6172\n",
      "Epoch [5202/10000], Loss: 1.6171\n",
      "Epoch [5203/10000], Loss: 1.6170\n",
      "Epoch [5204/10000], Loss: 1.6168\n",
      "Epoch [5205/10000], Loss: 1.6167\n",
      "Epoch [5206/10000], Loss: 1.6165\n",
      "Epoch [5207/10000], Loss: 1.6164\n",
      "Epoch [5208/10000], Loss: 1.6163\n",
      "Epoch [5209/10000], Loss: 1.6161\n",
      "Epoch [5210/10000], Loss: 1.6160\n",
      "Epoch [5211/10000], Loss: 1.6159\n",
      "Epoch [5212/10000], Loss: 1.6157\n",
      "Epoch [5213/10000], Loss: 1.6156\n",
      "Epoch [5214/10000], Loss: 1.6154\n",
      "Epoch [5215/10000], Loss: 1.6153\n",
      "Epoch [5216/10000], Loss: 1.6152\n",
      "Epoch [5217/10000], Loss: 1.6150\n",
      "Epoch [5218/10000], Loss: 1.6149\n",
      "Epoch [5219/10000], Loss: 1.6148\n",
      "Epoch [5220/10000], Loss: 1.6146\n",
      "Epoch [5221/10000], Loss: 1.6145\n",
      "Epoch [5222/10000], Loss: 1.6143\n",
      "Epoch [5223/10000], Loss: 1.6142\n",
      "Epoch [5224/10000], Loss: 1.6141\n",
      "Epoch [5225/10000], Loss: 1.6139\n",
      "Epoch [5226/10000], Loss: 1.6138\n",
      "Epoch [5227/10000], Loss: 1.6137\n",
      "Epoch [5228/10000], Loss: 1.6135\n",
      "Epoch [5229/10000], Loss: 1.6134\n",
      "Epoch [5230/10000], Loss: 1.6133\n",
      "Epoch [5231/10000], Loss: 1.6131\n",
      "Epoch [5232/10000], Loss: 1.6130\n",
      "Epoch [5233/10000], Loss: 1.6128\n",
      "Epoch [5234/10000], Loss: 1.6127\n",
      "Epoch [5235/10000], Loss: 1.6126\n",
      "Epoch [5236/10000], Loss: 1.6124\n",
      "Epoch [5237/10000], Loss: 1.6123\n",
      "Epoch [5238/10000], Loss: 1.6122\n",
      "Epoch [5239/10000], Loss: 1.6120\n",
      "Epoch [5240/10000], Loss: 1.6119\n",
      "Epoch [5241/10000], Loss: 1.6117\n",
      "Epoch [5242/10000], Loss: 1.6116\n",
      "Epoch [5243/10000], Loss: 1.6115\n",
      "Epoch [5244/10000], Loss: 1.6113\n",
      "Epoch [5245/10000], Loss: 1.6112\n",
      "Epoch [5246/10000], Loss: 1.6111\n",
      "Epoch [5247/10000], Loss: 1.6109\n",
      "Epoch [5248/10000], Loss: 1.6108\n",
      "Epoch [5249/10000], Loss: 1.6106\n",
      "Epoch [5250/10000], Loss: 1.6105\n",
      "Epoch [5251/10000], Loss: 1.6104\n",
      "Epoch [5252/10000], Loss: 1.6102\n",
      "Epoch [5253/10000], Loss: 1.6101\n",
      "Epoch [5254/10000], Loss: 1.6100\n",
      "Epoch [5255/10000], Loss: 1.6098\n",
      "Epoch [5256/10000], Loss: 1.6097\n",
      "Epoch [5257/10000], Loss: 1.6096\n",
      "Epoch [5258/10000], Loss: 1.6094\n",
      "Epoch [5259/10000], Loss: 1.6093\n",
      "Epoch [5260/10000], Loss: 1.6091\n",
      "Epoch [5261/10000], Loss: 1.6090\n",
      "Epoch [5262/10000], Loss: 1.6089\n",
      "Epoch [5263/10000], Loss: 1.6087\n",
      "Epoch [5264/10000], Loss: 1.6086\n",
      "Epoch [5265/10000], Loss: 1.6085\n",
      "Epoch [5266/10000], Loss: 1.6083\n",
      "Epoch [5267/10000], Loss: 1.6082\n",
      "Epoch [5268/10000], Loss: 1.6080\n",
      "Epoch [5269/10000], Loss: 1.6079\n",
      "Epoch [5270/10000], Loss: 1.6078\n",
      "Epoch [5271/10000], Loss: 1.6076\n",
      "Epoch [5272/10000], Loss: 1.6075\n",
      "Epoch [5273/10000], Loss: 1.6074\n",
      "Epoch [5274/10000], Loss: 1.6072\n",
      "Epoch [5275/10000], Loss: 1.6071\n",
      "Epoch [5276/10000], Loss: 1.6069\n",
      "Epoch [5277/10000], Loss: 1.6068\n",
      "Epoch [5278/10000], Loss: 1.6067\n",
      "Epoch [5279/10000], Loss: 1.6065\n",
      "Epoch [5280/10000], Loss: 1.6064\n",
      "Epoch [5281/10000], Loss: 1.6063\n",
      "Epoch [5282/10000], Loss: 1.6061\n",
      "Epoch [5283/10000], Loss: 1.6060\n",
      "Epoch [5284/10000], Loss: 1.6059\n",
      "Epoch [5285/10000], Loss: 1.6057\n",
      "Epoch [5286/10000], Loss: 1.6056\n",
      "Epoch [5287/10000], Loss: 1.6054\n",
      "Epoch [5288/10000], Loss: 1.6053\n",
      "Epoch [5289/10000], Loss: 1.6052\n",
      "Epoch [5290/10000], Loss: 1.6050\n",
      "Epoch [5291/10000], Loss: 1.6049\n",
      "Epoch [5292/10000], Loss: 1.6048\n",
      "Epoch [5293/10000], Loss: 1.6046\n",
      "Epoch [5294/10000], Loss: 1.6045\n",
      "Epoch [5295/10000], Loss: 1.6043\n",
      "Epoch [5296/10000], Loss: 1.6042\n",
      "Epoch [5297/10000], Loss: 1.6041\n",
      "Epoch [5298/10000], Loss: 1.6039\n",
      "Epoch [5299/10000], Loss: 1.6038\n",
      "Epoch [5300/10000], Loss: 1.6037\n",
      "Epoch [5301/10000], Loss: 1.6035\n",
      "Epoch [5302/10000], Loss: 1.6034\n",
      "Epoch [5303/10000], Loss: 1.6033\n",
      "Epoch [5304/10000], Loss: 1.6031\n",
      "Epoch [5305/10000], Loss: 1.6030\n",
      "Epoch [5306/10000], Loss: 1.6028\n",
      "Epoch [5307/10000], Loss: 1.6027\n",
      "Epoch [5308/10000], Loss: 1.6026\n",
      "Epoch [5309/10000], Loss: 1.6024\n",
      "Epoch [5310/10000], Loss: 1.6023\n",
      "Epoch [5311/10000], Loss: 1.6022\n",
      "Epoch [5312/10000], Loss: 1.6020\n",
      "Epoch [5313/10000], Loss: 1.6019\n",
      "Epoch [5314/10000], Loss: 1.6018\n",
      "Epoch [5315/10000], Loss: 1.6016\n",
      "Epoch [5316/10000], Loss: 1.6015\n",
      "Epoch [5317/10000], Loss: 1.6013\n",
      "Epoch [5318/10000], Loss: 1.6012\n",
      "Epoch [5319/10000], Loss: 1.6011\n",
      "Epoch [5320/10000], Loss: 1.6009\n",
      "Epoch [5321/10000], Loss: 1.6008\n",
      "Epoch [5322/10000], Loss: 1.6007\n",
      "Epoch [5323/10000], Loss: 1.6005\n",
      "Epoch [5324/10000], Loss: 1.6004\n",
      "Epoch [5325/10000], Loss: 1.6002\n",
      "Epoch [5326/10000], Loss: 1.6001\n",
      "Epoch [5327/10000], Loss: 1.6000\n",
      "Epoch [5328/10000], Loss: 1.5998\n",
      "Epoch [5329/10000], Loss: 1.5997\n",
      "Epoch [5330/10000], Loss: 1.5996\n",
      "Epoch [5331/10000], Loss: 1.5994\n",
      "Epoch [5332/10000], Loss: 1.5993\n",
      "Epoch [5333/10000], Loss: 1.5992\n",
      "Epoch [5334/10000], Loss: 1.5990\n",
      "Epoch [5335/10000], Loss: 1.5989\n",
      "Epoch [5336/10000], Loss: 1.5987\n",
      "Epoch [5337/10000], Loss: 1.5986\n",
      "Epoch [5338/10000], Loss: 1.5985\n",
      "Epoch [5339/10000], Loss: 1.5983\n",
      "Epoch [5340/10000], Loss: 1.5982\n",
      "Epoch [5341/10000], Loss: 1.5981\n",
      "Epoch [5342/10000], Loss: 1.5979\n",
      "Epoch [5343/10000], Loss: 1.5978\n",
      "Epoch [5344/10000], Loss: 1.5977\n",
      "Epoch [5345/10000], Loss: 1.5975\n",
      "Epoch [5346/10000], Loss: 1.5974\n",
      "Epoch [5347/10000], Loss: 1.5972\n",
      "Epoch [5348/10000], Loss: 1.5971\n",
      "Epoch [5349/10000], Loss: 1.5970\n",
      "Epoch [5350/10000], Loss: 1.5968\n",
      "Epoch [5351/10000], Loss: 1.5967\n",
      "Epoch [5352/10000], Loss: 1.5966\n",
      "Epoch [5353/10000], Loss: 1.5964\n",
      "Epoch [5354/10000], Loss: 1.5963\n",
      "Epoch [5355/10000], Loss: 1.5962\n",
      "Epoch [5356/10000], Loss: 1.5960\n",
      "Epoch [5357/10000], Loss: 1.5959\n",
      "Epoch [5358/10000], Loss: 1.5957\n",
      "Epoch [5359/10000], Loss: 1.5956\n",
      "Epoch [5360/10000], Loss: 1.5955\n",
      "Epoch [5361/10000], Loss: 1.5953\n",
      "Epoch [5362/10000], Loss: 1.5952\n",
      "Epoch [5363/10000], Loss: 1.5951\n",
      "Epoch [5364/10000], Loss: 1.5949\n",
      "Epoch [5365/10000], Loss: 1.5948\n",
      "Epoch [5366/10000], Loss: 1.5947\n",
      "Epoch [5367/10000], Loss: 1.5945\n",
      "Epoch [5368/10000], Loss: 1.5944\n",
      "Epoch [5369/10000], Loss: 1.5942\n",
      "Epoch [5370/10000], Loss: 1.5941\n",
      "Epoch [5371/10000], Loss: 1.5940\n",
      "Epoch [5372/10000], Loss: 1.5938\n",
      "Epoch [5373/10000], Loss: 1.5937\n",
      "Epoch [5374/10000], Loss: 1.5936\n",
      "Epoch [5375/10000], Loss: 1.5934\n",
      "Epoch [5376/10000], Loss: 1.5933\n",
      "Epoch [5377/10000], Loss: 1.5932\n",
      "Epoch [5378/10000], Loss: 1.5930\n",
      "Epoch [5379/10000], Loss: 1.5929\n",
      "Epoch [5380/10000], Loss: 1.5927\n",
      "Epoch [5381/10000], Loss: 1.5926\n",
      "Epoch [5382/10000], Loss: 1.5925\n",
      "Epoch [5383/10000], Loss: 1.5923\n",
      "Epoch [5384/10000], Loss: 1.5922\n",
      "Epoch [5385/10000], Loss: 1.5921\n",
      "Epoch [5386/10000], Loss: 1.5919\n",
      "Epoch [5387/10000], Loss: 1.5918\n",
      "Epoch [5388/10000], Loss: 1.5917\n",
      "Epoch [5389/10000], Loss: 1.5915\n",
      "Epoch [5390/10000], Loss: 1.5914\n",
      "Epoch [5391/10000], Loss: 1.5912\n",
      "Epoch [5392/10000], Loss: 1.5911\n",
      "Epoch [5393/10000], Loss: 1.5910\n",
      "Epoch [5394/10000], Loss: 1.5908\n",
      "Epoch [5395/10000], Loss: 1.5907\n",
      "Epoch [5396/10000], Loss: 1.5906\n",
      "Epoch [5397/10000], Loss: 1.5904\n",
      "Epoch [5398/10000], Loss: 1.5903\n",
      "Epoch [5399/10000], Loss: 1.5902\n",
      "Epoch [5400/10000], Loss: 1.5900\n",
      "Epoch [5401/10000], Loss: 1.5899\n",
      "Epoch [5402/10000], Loss: 1.5897\n",
      "Epoch [5403/10000], Loss: 1.5896\n",
      "Epoch [5404/10000], Loss: 1.5895\n",
      "Epoch [5405/10000], Loss: 1.5893\n",
      "Epoch [5406/10000], Loss: 1.5892\n",
      "Epoch [5407/10000], Loss: 1.5891\n",
      "Epoch [5408/10000], Loss: 1.5889\n",
      "Epoch [5409/10000], Loss: 1.5888\n",
      "Epoch [5410/10000], Loss: 1.5887\n",
      "Epoch [5411/10000], Loss: 1.5885\n",
      "Epoch [5412/10000], Loss: 1.5884\n",
      "Epoch [5413/10000], Loss: 1.5882\n",
      "Epoch [5414/10000], Loss: 1.5881\n",
      "Epoch [5415/10000], Loss: 1.5880\n",
      "Epoch [5416/10000], Loss: 1.5878\n",
      "Epoch [5417/10000], Loss: 1.5877\n",
      "Epoch [5418/10000], Loss: 1.5876\n",
      "Epoch [5419/10000], Loss: 1.5874\n",
      "Epoch [5420/10000], Loss: 1.5873\n",
      "Epoch [5421/10000], Loss: 1.5872\n",
      "Epoch [5422/10000], Loss: 1.5870\n",
      "Epoch [5423/10000], Loss: 1.5869\n",
      "Epoch [5424/10000], Loss: 1.5868\n",
      "Epoch [5425/10000], Loss: 1.5866\n",
      "Epoch [5426/10000], Loss: 1.5865\n",
      "Epoch [5427/10000], Loss: 1.5863\n",
      "Epoch [5428/10000], Loss: 1.5862\n",
      "Epoch [5429/10000], Loss: 1.5861\n",
      "Epoch [5430/10000], Loss: 1.5859\n",
      "Epoch [5431/10000], Loss: 1.5858\n",
      "Epoch [5432/10000], Loss: 1.5857\n",
      "Epoch [5433/10000], Loss: 1.5855\n",
      "Epoch [5434/10000], Loss: 1.5854\n",
      "Epoch [5435/10000], Loss: 1.5853\n",
      "Epoch [5436/10000], Loss: 1.5851\n",
      "Epoch [5437/10000], Loss: 1.5850\n",
      "Epoch [5438/10000], Loss: 1.5848\n",
      "Epoch [5439/10000], Loss: 1.5847\n",
      "Epoch [5440/10000], Loss: 1.5846\n",
      "Epoch [5441/10000], Loss: 1.5844\n",
      "Epoch [5442/10000], Loss: 1.5843\n",
      "Epoch [5443/10000], Loss: 1.5842\n",
      "Epoch [5444/10000], Loss: 1.5840\n",
      "Epoch [5445/10000], Loss: 1.5839\n",
      "Epoch [5446/10000], Loss: 1.5838\n",
      "Epoch [5447/10000], Loss: 1.5836\n",
      "Epoch [5448/10000], Loss: 1.5835\n",
      "Epoch [5449/10000], Loss: 1.5834\n",
      "Epoch [5450/10000], Loss: 1.5832\n",
      "Epoch [5451/10000], Loss: 1.5831\n",
      "Epoch [5452/10000], Loss: 1.5829\n",
      "Epoch [5453/10000], Loss: 1.5828\n",
      "Epoch [5454/10000], Loss: 1.5827\n",
      "Epoch [5455/10000], Loss: 1.5825\n",
      "Epoch [5456/10000], Loss: 1.5824\n",
      "Epoch [5457/10000], Loss: 1.5823\n",
      "Epoch [5458/10000], Loss: 1.5821\n",
      "Epoch [5459/10000], Loss: 1.5820\n",
      "Epoch [5460/10000], Loss: 1.5819\n",
      "Epoch [5461/10000], Loss: 1.5817\n",
      "Epoch [5462/10000], Loss: 1.5816\n",
      "Epoch [5463/10000], Loss: 1.5815\n",
      "Epoch [5464/10000], Loss: 1.5813\n",
      "Epoch [5465/10000], Loss: 1.5812\n",
      "Epoch [5466/10000], Loss: 1.5810\n",
      "Epoch [5467/10000], Loss: 1.5809\n",
      "Epoch [5468/10000], Loss: 1.5808\n",
      "Epoch [5469/10000], Loss: 1.5806\n",
      "Epoch [5470/10000], Loss: 1.5805\n",
      "Epoch [5471/10000], Loss: 1.5804\n",
      "Epoch [5472/10000], Loss: 1.5802\n",
      "Epoch [5473/10000], Loss: 1.5801\n",
      "Epoch [5474/10000], Loss: 1.5800\n",
      "Epoch [5475/10000], Loss: 1.5798\n",
      "Epoch [5476/10000], Loss: 1.5797\n",
      "Epoch [5477/10000], Loss: 1.5796\n",
      "Epoch [5478/10000], Loss: 1.5794\n",
      "Epoch [5479/10000], Loss: 1.5793\n",
      "Epoch [5480/10000], Loss: 1.5791\n",
      "Epoch [5481/10000], Loss: 1.5790\n",
      "Epoch [5482/10000], Loss: 1.5789\n",
      "Epoch [5483/10000], Loss: 1.5787\n",
      "Epoch [5484/10000], Loss: 1.5786\n",
      "Epoch [5485/10000], Loss: 1.5785\n",
      "Epoch [5486/10000], Loss: 1.5783\n",
      "Epoch [5487/10000], Loss: 1.5782\n",
      "Epoch [5488/10000], Loss: 1.5781\n",
      "Epoch [5489/10000], Loss: 1.5779\n",
      "Epoch [5490/10000], Loss: 1.5778\n",
      "Epoch [5491/10000], Loss: 1.5777\n",
      "Epoch [5492/10000], Loss: 1.5775\n",
      "Epoch [5493/10000], Loss: 1.5774\n",
      "Epoch [5494/10000], Loss: 1.5772\n",
      "Epoch [5495/10000], Loss: 1.5771\n",
      "Epoch [5496/10000], Loss: 1.5770\n",
      "Epoch [5497/10000], Loss: 1.5768\n",
      "Epoch [5498/10000], Loss: 1.5767\n",
      "Epoch [5499/10000], Loss: 1.5766\n",
      "Epoch [5500/10000], Loss: 1.5764\n",
      "Epoch [5501/10000], Loss: 1.5763\n",
      "Epoch [5502/10000], Loss: 1.5762\n",
      "Epoch [5503/10000], Loss: 1.5760\n",
      "Epoch [5504/10000], Loss: 1.5759\n",
      "Epoch [5505/10000], Loss: 1.5758\n",
      "Epoch [5506/10000], Loss: 1.5756\n",
      "Epoch [5507/10000], Loss: 1.5755\n",
      "Epoch [5508/10000], Loss: 1.5753\n",
      "Epoch [5509/10000], Loss: 1.5752\n",
      "Epoch [5510/10000], Loss: 1.5751\n",
      "Epoch [5511/10000], Loss: 1.5749\n",
      "Epoch [5512/10000], Loss: 1.5748\n",
      "Epoch [5513/10000], Loss: 1.5747\n",
      "Epoch [5514/10000], Loss: 1.5745\n",
      "Epoch [5515/10000], Loss: 1.5744\n",
      "Epoch [5516/10000], Loss: 1.5743\n",
      "Epoch [5517/10000], Loss: 1.5741\n",
      "Epoch [5518/10000], Loss: 1.5740\n",
      "Epoch [5519/10000], Loss: 1.5739\n",
      "Epoch [5520/10000], Loss: 1.5737\n",
      "Epoch [5521/10000], Loss: 1.5736\n",
      "Epoch [5522/10000], Loss: 1.5734\n",
      "Epoch [5523/10000], Loss: 1.5733\n",
      "Epoch [5524/10000], Loss: 1.5732\n",
      "Epoch [5525/10000], Loss: 1.5730\n",
      "Epoch [5526/10000], Loss: 1.5729\n",
      "Epoch [5527/10000], Loss: 1.5728\n",
      "Epoch [5528/10000], Loss: 1.5726\n",
      "Epoch [5529/10000], Loss: 1.5725\n",
      "Epoch [5530/10000], Loss: 1.5724\n",
      "Epoch [5531/10000], Loss: 1.5722\n",
      "Epoch [5532/10000], Loss: 1.5721\n",
      "Epoch [5533/10000], Loss: 1.5720\n",
      "Epoch [5534/10000], Loss: 1.5718\n",
      "Epoch [5535/10000], Loss: 1.5717\n",
      "Epoch [5536/10000], Loss: 1.5716\n",
      "Epoch [5537/10000], Loss: 1.5714\n",
      "Epoch [5538/10000], Loss: 1.5713\n",
      "Epoch [5539/10000], Loss: 1.5711\n",
      "Epoch [5540/10000], Loss: 1.5710\n",
      "Epoch [5541/10000], Loss: 1.5709\n",
      "Epoch [5542/10000], Loss: 1.5707\n",
      "Epoch [5543/10000], Loss: 1.5706\n",
      "Epoch [5544/10000], Loss: 1.5705\n",
      "Epoch [5545/10000], Loss: 1.5703\n",
      "Epoch [5546/10000], Loss: 1.5702\n",
      "Epoch [5547/10000], Loss: 1.5701\n",
      "Epoch [5548/10000], Loss: 1.5699\n",
      "Epoch [5549/10000], Loss: 1.5698\n",
      "Epoch [5550/10000], Loss: 1.5697\n",
      "Epoch [5551/10000], Loss: 1.5695\n",
      "Epoch [5552/10000], Loss: 1.5694\n",
      "Epoch [5553/10000], Loss: 1.5693\n",
      "Epoch [5554/10000], Loss: 1.5691\n",
      "Epoch [5555/10000], Loss: 1.5690\n",
      "Epoch [5556/10000], Loss: 1.5688\n",
      "Epoch [5557/10000], Loss: 1.5687\n",
      "Epoch [5558/10000], Loss: 1.5686\n",
      "Epoch [5559/10000], Loss: 1.5684\n",
      "Epoch [5560/10000], Loss: 1.5683\n",
      "Epoch [5561/10000], Loss: 1.5682\n",
      "Epoch [5562/10000], Loss: 1.5680\n",
      "Epoch [5563/10000], Loss: 1.5679\n",
      "Epoch [5564/10000], Loss: 1.5678\n",
      "Epoch [5565/10000], Loss: 1.5676\n",
      "Epoch [5566/10000], Loss: 1.5675\n",
      "Epoch [5567/10000], Loss: 1.5674\n",
      "Epoch [5568/10000], Loss: 1.5672\n",
      "Epoch [5569/10000], Loss: 1.5671\n",
      "Epoch [5570/10000], Loss: 1.5670\n",
      "Epoch [5571/10000], Loss: 1.5668\n",
      "Epoch [5572/10000], Loss: 1.5667\n",
      "Epoch [5573/10000], Loss: 1.5665\n",
      "Epoch [5574/10000], Loss: 1.5664\n",
      "Epoch [5575/10000], Loss: 1.5663\n",
      "Epoch [5576/10000], Loss: 1.5661\n",
      "Epoch [5577/10000], Loss: 1.5660\n",
      "Epoch [5578/10000], Loss: 1.5659\n",
      "Epoch [5579/10000], Loss: 1.5657\n",
      "Epoch [5580/10000], Loss: 1.5656\n",
      "Epoch [5581/10000], Loss: 1.5655\n",
      "Epoch [5582/10000], Loss: 1.5653\n",
      "Epoch [5583/10000], Loss: 1.5652\n",
      "Epoch [5584/10000], Loss: 1.5651\n",
      "Epoch [5585/10000], Loss: 1.5649\n",
      "Epoch [5586/10000], Loss: 1.5648\n",
      "Epoch [5587/10000], Loss: 1.5647\n",
      "Epoch [5588/10000], Loss: 1.5645\n",
      "Epoch [5589/10000], Loss: 1.5644\n",
      "Epoch [5590/10000], Loss: 1.5642\n",
      "Epoch [5591/10000], Loss: 1.5641\n",
      "Epoch [5592/10000], Loss: 1.5640\n",
      "Epoch [5593/10000], Loss: 1.5638\n",
      "Epoch [5594/10000], Loss: 1.5637\n",
      "Epoch [5595/10000], Loss: 1.5636\n",
      "Epoch [5596/10000], Loss: 1.5634\n",
      "Epoch [5597/10000], Loss: 1.5633\n",
      "Epoch [5598/10000], Loss: 1.5632\n",
      "Epoch [5599/10000], Loss: 1.5630\n",
      "Epoch [5600/10000], Loss: 1.5629\n",
      "Epoch [5601/10000], Loss: 1.5628\n",
      "Epoch [5602/10000], Loss: 1.5626\n",
      "Epoch [5603/10000], Loss: 1.5625\n",
      "Epoch [5604/10000], Loss: 1.5624\n",
      "Epoch [5605/10000], Loss: 1.5622\n",
      "Epoch [5606/10000], Loss: 1.5621\n",
      "Epoch [5607/10000], Loss: 1.5620\n",
      "Epoch [5608/10000], Loss: 1.5618\n",
      "Epoch [5609/10000], Loss: 1.5617\n",
      "Epoch [5610/10000], Loss: 1.5615\n",
      "Epoch [5611/10000], Loss: 1.5614\n",
      "Epoch [5612/10000], Loss: 1.5613\n",
      "Epoch [5613/10000], Loss: 1.5611\n",
      "Epoch [5614/10000], Loss: 1.5610\n",
      "Epoch [5615/10000], Loss: 1.5609\n",
      "Epoch [5616/10000], Loss: 1.5607\n",
      "Epoch [5617/10000], Loss: 1.5606\n",
      "Epoch [5618/10000], Loss: 1.5605\n",
      "Epoch [5619/10000], Loss: 1.5603\n",
      "Epoch [5620/10000], Loss: 1.5602\n",
      "Epoch [5621/10000], Loss: 1.5601\n",
      "Epoch [5622/10000], Loss: 1.5599\n",
      "Epoch [5623/10000], Loss: 1.5598\n",
      "Epoch [5624/10000], Loss: 1.5597\n",
      "Epoch [5625/10000], Loss: 1.5595\n",
      "Epoch [5626/10000], Loss: 1.5594\n",
      "Epoch [5627/10000], Loss: 1.5593\n",
      "Epoch [5628/10000], Loss: 1.5591\n",
      "Epoch [5629/10000], Loss: 1.5590\n",
      "Epoch [5630/10000], Loss: 1.5588\n",
      "Epoch [5631/10000], Loss: 1.5587\n",
      "Epoch [5632/10000], Loss: 1.5586\n",
      "Epoch [5633/10000], Loss: 1.5584\n",
      "Epoch [5634/10000], Loss: 1.5583\n",
      "Epoch [5635/10000], Loss: 1.5582\n",
      "Epoch [5636/10000], Loss: 1.5580\n",
      "Epoch [5637/10000], Loss: 1.5579\n",
      "Epoch [5638/10000], Loss: 1.5578\n",
      "Epoch [5639/10000], Loss: 1.5576\n",
      "Epoch [5640/10000], Loss: 1.5575\n",
      "Epoch [5641/10000], Loss: 1.5574\n",
      "Epoch [5642/10000], Loss: 1.5572\n",
      "Epoch [5643/10000], Loss: 1.5571\n",
      "Epoch [5644/10000], Loss: 1.5570\n",
      "Epoch [5645/10000], Loss: 1.5568\n",
      "Epoch [5646/10000], Loss: 1.5567\n",
      "Epoch [5647/10000], Loss: 1.5566\n",
      "Epoch [5648/10000], Loss: 1.5564\n",
      "Epoch [5649/10000], Loss: 1.5563\n",
      "Epoch [5650/10000], Loss: 1.5562\n",
      "Epoch [5651/10000], Loss: 1.5560\n",
      "Epoch [5652/10000], Loss: 1.5559\n",
      "Epoch [5653/10000], Loss: 1.5557\n",
      "Epoch [5654/10000], Loss: 1.5556\n",
      "Epoch [5655/10000], Loss: 1.5555\n",
      "Epoch [5656/10000], Loss: 1.5553\n",
      "Epoch [5657/10000], Loss: 1.5552\n",
      "Epoch [5658/10000], Loss: 1.5551\n",
      "Epoch [5659/10000], Loss: 1.5549\n",
      "Epoch [5660/10000], Loss: 1.5548\n",
      "Epoch [5661/10000], Loss: 1.5547\n",
      "Epoch [5662/10000], Loss: 1.5545\n",
      "Epoch [5663/10000], Loss: 1.5544\n",
      "Epoch [5664/10000], Loss: 1.5543\n",
      "Epoch [5665/10000], Loss: 1.5541\n",
      "Epoch [5666/10000], Loss: 1.5540\n",
      "Epoch [5667/10000], Loss: 1.5539\n",
      "Epoch [5668/10000], Loss: 1.5537\n",
      "Epoch [5669/10000], Loss: 1.5536\n",
      "Epoch [5670/10000], Loss: 1.5535\n",
      "Epoch [5671/10000], Loss: 1.5533\n",
      "Epoch [5672/10000], Loss: 1.5532\n",
      "Epoch [5673/10000], Loss: 1.5531\n",
      "Epoch [5674/10000], Loss: 1.5529\n",
      "Epoch [5675/10000], Loss: 1.5528\n",
      "Epoch [5676/10000], Loss: 1.5526\n",
      "Epoch [5677/10000], Loss: 1.5525\n",
      "Epoch [5678/10000], Loss: 1.5524\n",
      "Epoch [5679/10000], Loss: 1.5522\n",
      "Epoch [5680/10000], Loss: 1.5521\n",
      "Epoch [5681/10000], Loss: 1.5520\n",
      "Epoch [5682/10000], Loss: 1.5518\n",
      "Epoch [5683/10000], Loss: 1.5517\n",
      "Epoch [5684/10000], Loss: 1.5516\n",
      "Epoch [5685/10000], Loss: 1.5514\n",
      "Epoch [5686/10000], Loss: 1.5513\n",
      "Epoch [5687/10000], Loss: 1.5512\n",
      "Epoch [5688/10000], Loss: 1.5510\n",
      "Epoch [5689/10000], Loss: 1.5509\n",
      "Epoch [5690/10000], Loss: 1.5508\n",
      "Epoch [5691/10000], Loss: 1.5506\n",
      "Epoch [5692/10000], Loss: 1.5505\n",
      "Epoch [5693/10000], Loss: 1.5504\n",
      "Epoch [5694/10000], Loss: 1.5502\n",
      "Epoch [5695/10000], Loss: 1.5501\n",
      "Epoch [5696/10000], Loss: 1.5500\n",
      "Epoch [5697/10000], Loss: 1.5498\n",
      "Epoch [5698/10000], Loss: 1.5497\n",
      "Epoch [5699/10000], Loss: 1.5496\n",
      "Epoch [5700/10000], Loss: 1.5494\n",
      "Epoch [5701/10000], Loss: 1.5493\n",
      "Epoch [5702/10000], Loss: 1.5491\n",
      "Epoch [5703/10000], Loss: 1.5490\n",
      "Epoch [5704/10000], Loss: 1.5489\n",
      "Epoch [5705/10000], Loss: 1.5487\n",
      "Epoch [5706/10000], Loss: 1.5486\n",
      "Epoch [5707/10000], Loss: 1.5485\n",
      "Epoch [5708/10000], Loss: 1.5483\n",
      "Epoch [5709/10000], Loss: 1.5482\n",
      "Epoch [5710/10000], Loss: 1.5481\n",
      "Epoch [5711/10000], Loss: 1.5479\n",
      "Epoch [5712/10000], Loss: 1.5478\n",
      "Epoch [5713/10000], Loss: 1.5477\n",
      "Epoch [5714/10000], Loss: 1.5475\n",
      "Epoch [5715/10000], Loss: 1.5474\n",
      "Epoch [5716/10000], Loss: 1.5473\n",
      "Epoch [5717/10000], Loss: 1.5471\n",
      "Epoch [5718/10000], Loss: 1.5470\n",
      "Epoch [5719/10000], Loss: 1.5469\n",
      "Epoch [5720/10000], Loss: 1.5467\n",
      "Epoch [5721/10000], Loss: 1.5466\n",
      "Epoch [5722/10000], Loss: 1.5465\n",
      "Epoch [5723/10000], Loss: 1.5463\n",
      "Epoch [5724/10000], Loss: 1.5462\n",
      "Epoch [5725/10000], Loss: 1.5461\n",
      "Epoch [5726/10000], Loss: 1.5459\n",
      "Epoch [5727/10000], Loss: 1.5458\n",
      "Epoch [5728/10000], Loss: 1.5457\n",
      "Epoch [5729/10000], Loss: 1.5455\n",
      "Epoch [5730/10000], Loss: 1.5454\n",
      "Epoch [5731/10000], Loss: 1.5452\n",
      "Epoch [5732/10000], Loss: 1.5451\n",
      "Epoch [5733/10000], Loss: 1.5450\n",
      "Epoch [5734/10000], Loss: 1.5448\n",
      "Epoch [5735/10000], Loss: 1.5447\n",
      "Epoch [5736/10000], Loss: 1.5446\n",
      "Epoch [5737/10000], Loss: 1.5444\n",
      "Epoch [5738/10000], Loss: 1.5443\n",
      "Epoch [5739/10000], Loss: 1.5442\n",
      "Epoch [5740/10000], Loss: 1.5440\n",
      "Epoch [5741/10000], Loss: 1.5439\n",
      "Epoch [5742/10000], Loss: 1.5438\n",
      "Epoch [5743/10000], Loss: 1.5436\n",
      "Epoch [5744/10000], Loss: 1.5435\n",
      "Epoch [5745/10000], Loss: 1.5434\n",
      "Epoch [5746/10000], Loss: 1.5432\n",
      "Epoch [5747/10000], Loss: 1.5431\n",
      "Epoch [5748/10000], Loss: 1.5430\n",
      "Epoch [5749/10000], Loss: 1.5428\n",
      "Epoch [5750/10000], Loss: 1.5427\n",
      "Epoch [5751/10000], Loss: 1.5426\n",
      "Epoch [5752/10000], Loss: 1.5424\n",
      "Epoch [5753/10000], Loss: 1.5423\n",
      "Epoch [5754/10000], Loss: 1.5422\n",
      "Epoch [5755/10000], Loss: 1.5420\n",
      "Epoch [5756/10000], Loss: 1.5419\n",
      "Epoch [5757/10000], Loss: 1.5418\n",
      "Epoch [5758/10000], Loss: 1.5416\n",
      "Epoch [5759/10000], Loss: 1.5415\n",
      "Epoch [5760/10000], Loss: 1.5414\n",
      "Epoch [5761/10000], Loss: 1.5412\n",
      "Epoch [5762/10000], Loss: 1.5411\n",
      "Epoch [5763/10000], Loss: 1.5409\n",
      "Epoch [5764/10000], Loss: 1.5408\n",
      "Epoch [5765/10000], Loss: 1.5407\n",
      "Epoch [5766/10000], Loss: 1.5405\n",
      "Epoch [5767/10000], Loss: 1.5404\n",
      "Epoch [5768/10000], Loss: 1.5403\n",
      "Epoch [5769/10000], Loss: 1.5401\n",
      "Epoch [5770/10000], Loss: 1.5400\n",
      "Epoch [5771/10000], Loss: 1.5399\n",
      "Epoch [5772/10000], Loss: 1.5397\n",
      "Epoch [5773/10000], Loss: 1.5396\n",
      "Epoch [5774/10000], Loss: 1.5395\n",
      "Epoch [5775/10000], Loss: 1.5393\n",
      "Epoch [5776/10000], Loss: 1.5392\n",
      "Epoch [5777/10000], Loss: 1.5391\n",
      "Epoch [5778/10000], Loss: 1.5389\n",
      "Epoch [5779/10000], Loss: 1.5388\n",
      "Epoch [5780/10000], Loss: 1.5387\n",
      "Epoch [5781/10000], Loss: 1.5385\n",
      "Epoch [5782/10000], Loss: 1.5384\n",
      "Epoch [5783/10000], Loss: 1.5383\n",
      "Epoch [5784/10000], Loss: 1.5381\n",
      "Epoch [5785/10000], Loss: 1.5380\n",
      "Epoch [5786/10000], Loss: 1.5379\n",
      "Epoch [5787/10000], Loss: 1.5377\n",
      "Epoch [5788/10000], Loss: 1.5376\n",
      "Epoch [5789/10000], Loss: 1.5375\n",
      "Epoch [5790/10000], Loss: 1.5373\n",
      "Epoch [5791/10000], Loss: 1.5372\n",
      "Epoch [5792/10000], Loss: 1.5371\n",
      "Epoch [5793/10000], Loss: 1.5369\n",
      "Epoch [5794/10000], Loss: 1.5368\n",
      "Epoch [5795/10000], Loss: 1.5367\n",
      "Epoch [5796/10000], Loss: 1.5365\n",
      "Epoch [5797/10000], Loss: 1.5364\n",
      "Epoch [5798/10000], Loss: 1.5363\n",
      "Epoch [5799/10000], Loss: 1.5361\n",
      "Epoch [5800/10000], Loss: 1.5360\n",
      "Epoch [5801/10000], Loss: 1.5359\n",
      "Epoch [5802/10000], Loss: 1.5357\n",
      "Epoch [5803/10000], Loss: 1.5356\n",
      "Epoch [5804/10000], Loss: 1.5354\n",
      "Epoch [5805/10000], Loss: 1.5353\n",
      "Epoch [5806/10000], Loss: 1.5352\n",
      "Epoch [5807/10000], Loss: 1.5350\n",
      "Epoch [5808/10000], Loss: 1.5349\n",
      "Epoch [5809/10000], Loss: 1.5348\n",
      "Epoch [5810/10000], Loss: 1.5346\n",
      "Epoch [5811/10000], Loss: 1.5345\n",
      "Epoch [5812/10000], Loss: 1.5344\n",
      "Epoch [5813/10000], Loss: 1.5342\n",
      "Epoch [5814/10000], Loss: 1.5341\n",
      "Epoch [5815/10000], Loss: 1.5340\n",
      "Epoch [5816/10000], Loss: 1.5338\n",
      "Epoch [5817/10000], Loss: 1.5337\n",
      "Epoch [5818/10000], Loss: 1.5336\n",
      "Epoch [5819/10000], Loss: 1.5334\n",
      "Epoch [5820/10000], Loss: 1.5333\n",
      "Epoch [5821/10000], Loss: 1.5332\n",
      "Epoch [5822/10000], Loss: 1.5330\n",
      "Epoch [5823/10000], Loss: 1.5329\n",
      "Epoch [5824/10000], Loss: 1.5328\n",
      "Epoch [5825/10000], Loss: 1.5326\n",
      "Epoch [5826/10000], Loss: 1.5325\n",
      "Epoch [5827/10000], Loss: 1.5324\n",
      "Epoch [5828/10000], Loss: 1.5322\n",
      "Epoch [5829/10000], Loss: 1.5321\n",
      "Epoch [5830/10000], Loss: 1.5320\n",
      "Epoch [5831/10000], Loss: 1.5318\n",
      "Epoch [5832/10000], Loss: 1.5317\n",
      "Epoch [5833/10000], Loss: 1.5316\n",
      "Epoch [5834/10000], Loss: 1.5314\n",
      "Epoch [5835/10000], Loss: 1.5313\n",
      "Epoch [5836/10000], Loss: 1.5312\n",
      "Epoch [5837/10000], Loss: 1.5310\n",
      "Epoch [5838/10000], Loss: 1.5309\n",
      "Epoch [5839/10000], Loss: 1.5308\n",
      "Epoch [5840/10000], Loss: 1.5306\n",
      "Epoch [5841/10000], Loss: 1.5305\n",
      "Epoch [5842/10000], Loss: 1.5304\n",
      "Epoch [5843/10000], Loss: 1.5302\n",
      "Epoch [5844/10000], Loss: 1.5301\n",
      "Epoch [5845/10000], Loss: 1.5300\n",
      "Epoch [5846/10000], Loss: 1.5298\n",
      "Epoch [5847/10000], Loss: 1.5297\n",
      "Epoch [5848/10000], Loss: 1.5296\n",
      "Epoch [5849/10000], Loss: 1.5294\n",
      "Epoch [5850/10000], Loss: 1.5293\n",
      "Epoch [5851/10000], Loss: 1.5292\n",
      "Epoch [5852/10000], Loss: 1.5290\n",
      "Epoch [5853/10000], Loss: 1.5289\n",
      "Epoch [5854/10000], Loss: 1.5288\n",
      "Epoch [5855/10000], Loss: 1.5286\n",
      "Epoch [5856/10000], Loss: 1.5285\n",
      "Epoch [5857/10000], Loss: 1.5284\n",
      "Epoch [5858/10000], Loss: 1.5282\n",
      "Epoch [5859/10000], Loss: 1.5281\n",
      "Epoch [5860/10000], Loss: 1.5279\n",
      "Epoch [5861/10000], Loss: 1.5278\n",
      "Epoch [5862/10000], Loss: 1.5277\n",
      "Epoch [5863/10000], Loss: 1.5275\n",
      "Epoch [5864/10000], Loss: 1.5274\n",
      "Epoch [5865/10000], Loss: 1.5273\n",
      "Epoch [5866/10000], Loss: 1.5271\n",
      "Epoch [5867/10000], Loss: 1.5270\n",
      "Epoch [5868/10000], Loss: 1.5269\n",
      "Epoch [5869/10000], Loss: 1.5267\n",
      "Epoch [5870/10000], Loss: 1.5266\n",
      "Epoch [5871/10000], Loss: 1.5265\n",
      "Epoch [5872/10000], Loss: 1.5263\n",
      "Epoch [5873/10000], Loss: 1.5262\n",
      "Epoch [5874/10000], Loss: 1.5261\n",
      "Epoch [5875/10000], Loss: 1.5259\n",
      "Epoch [5876/10000], Loss: 1.5258\n",
      "Epoch [5877/10000], Loss: 1.5257\n",
      "Epoch [5878/10000], Loss: 1.5255\n",
      "Epoch [5879/10000], Loss: 1.5254\n",
      "Epoch [5880/10000], Loss: 1.5253\n",
      "Epoch [5881/10000], Loss: 1.5251\n",
      "Epoch [5882/10000], Loss: 1.5250\n",
      "Epoch [5883/10000], Loss: 1.5249\n",
      "Epoch [5884/10000], Loss: 1.5247\n",
      "Epoch [5885/10000], Loss: 1.5246\n",
      "Epoch [5886/10000], Loss: 1.5245\n",
      "Epoch [5887/10000], Loss: 1.5243\n",
      "Epoch [5888/10000], Loss: 1.5242\n",
      "Epoch [5889/10000], Loss: 1.5241\n",
      "Epoch [5890/10000], Loss: 1.5239\n",
      "Epoch [5891/10000], Loss: 1.5238\n",
      "Epoch [5892/10000], Loss: 1.5237\n",
      "Epoch [5893/10000], Loss: 1.5235\n",
      "Epoch [5894/10000], Loss: 1.5234\n",
      "Epoch [5895/10000], Loss: 1.5233\n",
      "Epoch [5896/10000], Loss: 1.5231\n",
      "Epoch [5897/10000], Loss: 1.5230\n",
      "Epoch [5898/10000], Loss: 1.5229\n",
      "Epoch [5899/10000], Loss: 1.5227\n",
      "Epoch [5900/10000], Loss: 1.5226\n",
      "Epoch [5901/10000], Loss: 1.5225\n",
      "Epoch [5902/10000], Loss: 1.5223\n",
      "Epoch [5903/10000], Loss: 1.5222\n",
      "Epoch [5904/10000], Loss: 1.5221\n",
      "Epoch [5905/10000], Loss: 1.5219\n",
      "Epoch [5906/10000], Loss: 1.5218\n",
      "Epoch [5907/10000], Loss: 1.5217\n",
      "Epoch [5908/10000], Loss: 1.5215\n",
      "Epoch [5909/10000], Loss: 1.5214\n",
      "Epoch [5910/10000], Loss: 1.5213\n",
      "Epoch [5911/10000], Loss: 1.5211\n",
      "Epoch [5912/10000], Loss: 1.5210\n",
      "Epoch [5913/10000], Loss: 1.5209\n",
      "Epoch [5914/10000], Loss: 1.5207\n",
      "Epoch [5915/10000], Loss: 1.5206\n",
      "Epoch [5916/10000], Loss: 1.5205\n",
      "Epoch [5917/10000], Loss: 1.5203\n",
      "Epoch [5918/10000], Loss: 1.5202\n",
      "Epoch [5919/10000], Loss: 1.5201\n",
      "Epoch [5920/10000], Loss: 1.5199\n",
      "Epoch [5921/10000], Loss: 1.5198\n",
      "Epoch [5922/10000], Loss: 1.5197\n",
      "Epoch [5923/10000], Loss: 1.5195\n",
      "Epoch [5924/10000], Loss: 1.5194\n",
      "Epoch [5925/10000], Loss: 1.5193\n",
      "Epoch [5926/10000], Loss: 1.5191\n",
      "Epoch [5927/10000], Loss: 1.5190\n",
      "Epoch [5928/10000], Loss: 1.5189\n",
      "Epoch [5929/10000], Loss: 1.5187\n",
      "Epoch [5930/10000], Loss: 1.5186\n",
      "Epoch [5931/10000], Loss: 1.5185\n",
      "Epoch [5932/10000], Loss: 1.5183\n",
      "Epoch [5933/10000], Loss: 1.5182\n",
      "Epoch [5934/10000], Loss: 1.5181\n",
      "Epoch [5935/10000], Loss: 1.5179\n",
      "Epoch [5936/10000], Loss: 1.5178\n",
      "Epoch [5937/10000], Loss: 1.5177\n",
      "Epoch [5938/10000], Loss: 1.5175\n",
      "Epoch [5939/10000], Loss: 1.5174\n",
      "Epoch [5940/10000], Loss: 1.5173\n",
      "Epoch [5941/10000], Loss: 1.5171\n",
      "Epoch [5942/10000], Loss: 1.5170\n",
      "Epoch [5943/10000], Loss: 1.5169\n",
      "Epoch [5944/10000], Loss: 1.5167\n",
      "Epoch [5945/10000], Loss: 1.5166\n",
      "Epoch [5946/10000], Loss: 1.5165\n",
      "Epoch [5947/10000], Loss: 1.5163\n",
      "Epoch [5948/10000], Loss: 1.5162\n",
      "Epoch [5949/10000], Loss: 1.5161\n",
      "Epoch [5950/10000], Loss: 1.5159\n",
      "Epoch [5951/10000], Loss: 1.5158\n",
      "Epoch [5952/10000], Loss: 1.5157\n",
      "Epoch [5953/10000], Loss: 1.5155\n",
      "Epoch [5954/10000], Loss: 1.5154\n",
      "Epoch [5955/10000], Loss: 1.5153\n",
      "Epoch [5956/10000], Loss: 1.5151\n",
      "Epoch [5957/10000], Loss: 1.5150\n",
      "Epoch [5958/10000], Loss: 1.5149\n",
      "Epoch [5959/10000], Loss: 1.5147\n",
      "Epoch [5960/10000], Loss: 1.5146\n",
      "Epoch [5961/10000], Loss: 1.5145\n",
      "Epoch [5962/10000], Loss: 1.5143\n",
      "Epoch [5963/10000], Loss: 1.5142\n",
      "Epoch [5964/10000], Loss: 1.5141\n",
      "Epoch [5965/10000], Loss: 1.5139\n",
      "Epoch [5966/10000], Loss: 1.5138\n",
      "Epoch [5967/10000], Loss: 1.5137\n",
      "Epoch [5968/10000], Loss: 1.5135\n",
      "Epoch [5969/10000], Loss: 1.5134\n",
      "Epoch [5970/10000], Loss: 1.5133\n",
      "Epoch [5971/10000], Loss: 1.5131\n",
      "Epoch [5972/10000], Loss: 1.5130\n",
      "Epoch [5973/10000], Loss: 1.5129\n",
      "Epoch [5974/10000], Loss: 1.5127\n",
      "Epoch [5975/10000], Loss: 1.5126\n",
      "Epoch [5976/10000], Loss: 1.5125\n",
      "Epoch [5977/10000], Loss: 1.5123\n",
      "Epoch [5978/10000], Loss: 1.5122\n",
      "Epoch [5979/10000], Loss: 1.5121\n",
      "Epoch [5980/10000], Loss: 1.5119\n",
      "Epoch [5981/10000], Loss: 1.5118\n",
      "Epoch [5982/10000], Loss: 1.5117\n",
      "Epoch [5983/10000], Loss: 1.5115\n",
      "Epoch [5984/10000], Loss: 1.5114\n",
      "Epoch [5985/10000], Loss: 1.5113\n",
      "Epoch [5986/10000], Loss: 1.5111\n",
      "Epoch [5987/10000], Loss: 1.5110\n",
      "Epoch [5988/10000], Loss: 1.5109\n",
      "Epoch [5989/10000], Loss: 1.5107\n",
      "Epoch [5990/10000], Loss: 1.5106\n",
      "Epoch [5991/10000], Loss: 1.5105\n",
      "Epoch [5992/10000], Loss: 1.5103\n",
      "Epoch [5993/10000], Loss: 1.5102\n",
      "Epoch [5994/10000], Loss: 1.5101\n",
      "Epoch [5995/10000], Loss: 1.5099\n",
      "Epoch [5996/10000], Loss: 1.5098\n",
      "Epoch [5997/10000], Loss: 1.5097\n",
      "Epoch [5998/10000], Loss: 1.5095\n",
      "Epoch [5999/10000], Loss: 1.5094\n",
      "Epoch [6000/10000], Loss: 1.5093\n",
      "Epoch [6001/10000], Loss: 1.5091\n",
      "Epoch [6002/10000], Loss: 1.5090\n",
      "Epoch [6003/10000], Loss: 1.5089\n",
      "Epoch [6004/10000], Loss: 1.5087\n",
      "Epoch [6005/10000], Loss: 1.5086\n",
      "Epoch [6006/10000], Loss: 1.5085\n",
      "Epoch [6007/10000], Loss: 1.5083\n",
      "Epoch [6008/10000], Loss: 1.5082\n",
      "Epoch [6009/10000], Loss: 1.5081\n",
      "Epoch [6010/10000], Loss: 1.5079\n",
      "Epoch [6011/10000], Loss: 1.5078\n",
      "Epoch [6012/10000], Loss: 1.5077\n",
      "Epoch [6013/10000], Loss: 1.5075\n",
      "Epoch [6014/10000], Loss: 1.5074\n",
      "Epoch [6015/10000], Loss: 1.5073\n",
      "Epoch [6016/10000], Loss: 1.5071\n",
      "Epoch [6017/10000], Loss: 1.5070\n",
      "Epoch [6018/10000], Loss: 1.5069\n",
      "Epoch [6019/10000], Loss: 1.5067\n",
      "Epoch [6020/10000], Loss: 1.5066\n",
      "Epoch [6021/10000], Loss: 1.5065\n",
      "Epoch [6022/10000], Loss: 1.5063\n",
      "Epoch [6023/10000], Loss: 1.5062\n",
      "Epoch [6024/10000], Loss: 1.5061\n",
      "Epoch [6025/10000], Loss: 1.5059\n",
      "Epoch [6026/10000], Loss: 1.5058\n",
      "Epoch [6027/10000], Loss: 1.5057\n",
      "Epoch [6028/10000], Loss: 1.5055\n",
      "Epoch [6029/10000], Loss: 1.5054\n",
      "Epoch [6030/10000], Loss: 1.5053\n",
      "Epoch [6031/10000], Loss: 1.5051\n",
      "Epoch [6032/10000], Loss: 1.5050\n",
      "Epoch [6033/10000], Loss: 1.5049\n",
      "Epoch [6034/10000], Loss: 1.5047\n",
      "Epoch [6035/10000], Loss: 1.5046\n",
      "Epoch [6036/10000], Loss: 1.5045\n",
      "Epoch [6037/10000], Loss: 1.5043\n",
      "Epoch [6038/10000], Loss: 1.5042\n",
      "Epoch [6039/10000], Loss: 1.5041\n",
      "Epoch [6040/10000], Loss: 1.5039\n",
      "Epoch [6041/10000], Loss: 1.5038\n",
      "Epoch [6042/10000], Loss: 1.5037\n",
      "Epoch [6043/10000], Loss: 1.5035\n",
      "Epoch [6044/10000], Loss: 1.5034\n",
      "Epoch [6045/10000], Loss: 1.5033\n",
      "Epoch [6046/10000], Loss: 1.5031\n",
      "Epoch [6047/10000], Loss: 1.5030\n",
      "Epoch [6048/10000], Loss: 1.5029\n",
      "Epoch [6049/10000], Loss: 1.5027\n",
      "Epoch [6050/10000], Loss: 1.5026\n",
      "Epoch [6051/10000], Loss: 1.5025\n",
      "Epoch [6052/10000], Loss: 1.5023\n",
      "Epoch [6053/10000], Loss: 1.5022\n",
      "Epoch [6054/10000], Loss: 1.5021\n",
      "Epoch [6055/10000], Loss: 1.5019\n",
      "Epoch [6056/10000], Loss: 1.5018\n",
      "Epoch [6057/10000], Loss: 1.5017\n",
      "Epoch [6058/10000], Loss: 1.5015\n",
      "Epoch [6059/10000], Loss: 1.5014\n",
      "Epoch [6060/10000], Loss: 1.5013\n",
      "Epoch [6061/10000], Loss: 1.5011\n",
      "Epoch [6062/10000], Loss: 1.5010\n",
      "Epoch [6063/10000], Loss: 1.5009\n",
      "Epoch [6064/10000], Loss: 1.5007\n",
      "Epoch [6065/10000], Loss: 1.5006\n",
      "Epoch [6066/10000], Loss: 1.5005\n",
      "Epoch [6067/10000], Loss: 1.5003\n",
      "Epoch [6068/10000], Loss: 1.5002\n",
      "Epoch [6069/10000], Loss: 1.5001\n",
      "Epoch [6070/10000], Loss: 1.4999\n",
      "Epoch [6071/10000], Loss: 1.4998\n",
      "Epoch [6072/10000], Loss: 1.4997\n",
      "Epoch [6073/10000], Loss: 1.4995\n",
      "Epoch [6074/10000], Loss: 1.4994\n",
      "Epoch [6075/10000], Loss: 1.4993\n",
      "Epoch [6076/10000], Loss: 1.4991\n",
      "Epoch [6077/10000], Loss: 1.4990\n",
      "Epoch [6078/10000], Loss: 1.4989\n",
      "Epoch [6079/10000], Loss: 1.4987\n",
      "Epoch [6080/10000], Loss: 1.4986\n",
      "Epoch [6081/10000], Loss: 1.4985\n",
      "Epoch [6082/10000], Loss: 1.4984\n",
      "Epoch [6083/10000], Loss: 1.4982\n",
      "Epoch [6084/10000], Loss: 1.4981\n",
      "Epoch [6085/10000], Loss: 1.4980\n",
      "Epoch [6086/10000], Loss: 1.4978\n",
      "Epoch [6087/10000], Loss: 1.4977\n",
      "Epoch [6088/10000], Loss: 1.4976\n",
      "Epoch [6089/10000], Loss: 1.4974\n",
      "Epoch [6090/10000], Loss: 1.4973\n",
      "Epoch [6091/10000], Loss: 1.4972\n",
      "Epoch [6092/10000], Loss: 1.4970\n",
      "Epoch [6093/10000], Loss: 1.4969\n",
      "Epoch [6094/10000], Loss: 1.4968\n",
      "Epoch [6095/10000], Loss: 1.4966\n",
      "Epoch [6096/10000], Loss: 1.4965\n",
      "Epoch [6097/10000], Loss: 1.4964\n",
      "Epoch [6098/10000], Loss: 1.4962\n",
      "Epoch [6099/10000], Loss: 1.4961\n",
      "Epoch [6100/10000], Loss: 1.4960\n",
      "Epoch [6101/10000], Loss: 1.4958\n",
      "Epoch [6102/10000], Loss: 1.4957\n",
      "Epoch [6103/10000], Loss: 1.4956\n",
      "Epoch [6104/10000], Loss: 1.4954\n",
      "Epoch [6105/10000], Loss: 1.4953\n",
      "Epoch [6106/10000], Loss: 1.4952\n",
      "Epoch [6107/10000], Loss: 1.4950\n",
      "Epoch [6108/10000], Loss: 1.4949\n",
      "Epoch [6109/10000], Loss: 1.4948\n",
      "Epoch [6110/10000], Loss: 1.4946\n",
      "Epoch [6111/10000], Loss: 1.4945\n",
      "Epoch [6112/10000], Loss: 1.4944\n",
      "Epoch [6113/10000], Loss: 1.4942\n",
      "Epoch [6114/10000], Loss: 1.4941\n",
      "Epoch [6115/10000], Loss: 1.4940\n",
      "Epoch [6116/10000], Loss: 1.4938\n",
      "Epoch [6117/10000], Loss: 1.4937\n",
      "Epoch [6118/10000], Loss: 1.4936\n",
      "Epoch [6119/10000], Loss: 1.4934\n",
      "Epoch [6120/10000], Loss: 1.4933\n",
      "Epoch [6121/10000], Loss: 1.4932\n",
      "Epoch [6122/10000], Loss: 1.4930\n",
      "Epoch [6123/10000], Loss: 1.4929\n",
      "Epoch [6124/10000], Loss: 1.4928\n",
      "Epoch [6125/10000], Loss: 1.4926\n",
      "Epoch [6126/10000], Loss: 1.4925\n",
      "Epoch [6127/10000], Loss: 1.4924\n",
      "Epoch [6128/10000], Loss: 1.4922\n",
      "Epoch [6129/10000], Loss: 1.4921\n",
      "Epoch [6130/10000], Loss: 1.4920\n",
      "Epoch [6131/10000], Loss: 1.4918\n",
      "Epoch [6132/10000], Loss: 1.4917\n",
      "Epoch [6133/10000], Loss: 1.4916\n",
      "Epoch [6134/10000], Loss: 1.4914\n",
      "Epoch [6135/10000], Loss: 1.4913\n",
      "Epoch [6136/10000], Loss: 1.4912\n",
      "Epoch [6137/10000], Loss: 1.4910\n",
      "Epoch [6138/10000], Loss: 1.4909\n",
      "Epoch [6139/10000], Loss: 1.4908\n",
      "Epoch [6140/10000], Loss: 1.4907\n",
      "Epoch [6141/10000], Loss: 1.4905\n",
      "Epoch [6142/10000], Loss: 1.4904\n",
      "Epoch [6143/10000], Loss: 1.4903\n",
      "Epoch [6144/10000], Loss: 1.4901\n",
      "Epoch [6145/10000], Loss: 1.4900\n",
      "Epoch [6146/10000], Loss: 1.4899\n",
      "Epoch [6147/10000], Loss: 1.4897\n",
      "Epoch [6148/10000], Loss: 1.4896\n",
      "Epoch [6149/10000], Loss: 1.4895\n",
      "Epoch [6150/10000], Loss: 1.4893\n",
      "Epoch [6151/10000], Loss: 1.4892\n",
      "Epoch [6152/10000], Loss: 1.4891\n",
      "Epoch [6153/10000], Loss: 1.4889\n",
      "Epoch [6154/10000], Loss: 1.4888\n",
      "Epoch [6155/10000], Loss: 1.4887\n",
      "Epoch [6156/10000], Loss: 1.4885\n",
      "Epoch [6157/10000], Loss: 1.4884\n",
      "Epoch [6158/10000], Loss: 1.4883\n",
      "Epoch [6159/10000], Loss: 1.4881\n",
      "Epoch [6160/10000], Loss: 1.4880\n",
      "Epoch [6161/10000], Loss: 1.4879\n",
      "Epoch [6162/10000], Loss: 1.4877\n",
      "Epoch [6163/10000], Loss: 1.4876\n",
      "Epoch [6164/10000], Loss: 1.4875\n",
      "Epoch [6165/10000], Loss: 1.4873\n",
      "Epoch [6166/10000], Loss: 1.4872\n",
      "Epoch [6167/10000], Loss: 1.4871\n",
      "Epoch [6168/10000], Loss: 1.4869\n",
      "Epoch [6169/10000], Loss: 1.4868\n",
      "Epoch [6170/10000], Loss: 1.4867\n",
      "Epoch [6171/10000], Loss: 1.4865\n",
      "Epoch [6172/10000], Loss: 1.4864\n",
      "Epoch [6173/10000], Loss: 1.4863\n",
      "Epoch [6174/10000], Loss: 1.4861\n",
      "Epoch [6175/10000], Loss: 1.4860\n",
      "Epoch [6176/10000], Loss: 1.4859\n",
      "Epoch [6177/10000], Loss: 1.4857\n",
      "Epoch [6178/10000], Loss: 1.4856\n",
      "Epoch [6179/10000], Loss: 1.4855\n",
      "Epoch [6180/10000], Loss: 1.4853\n",
      "Epoch [6181/10000], Loss: 1.4852\n",
      "Epoch [6182/10000], Loss: 1.4851\n",
      "Epoch [6183/10000], Loss: 1.4850\n",
      "Epoch [6184/10000], Loss: 1.4848\n",
      "Epoch [6185/10000], Loss: 1.4847\n",
      "Epoch [6186/10000], Loss: 1.4846\n",
      "Epoch [6187/10000], Loss: 1.4844\n",
      "Epoch [6188/10000], Loss: 1.4843\n",
      "Epoch [6189/10000], Loss: 1.4842\n",
      "Epoch [6190/10000], Loss: 1.4840\n",
      "Epoch [6191/10000], Loss: 1.4839\n",
      "Epoch [6192/10000], Loss: 1.4838\n",
      "Epoch [6193/10000], Loss: 1.4836\n",
      "Epoch [6194/10000], Loss: 1.4835\n",
      "Epoch [6195/10000], Loss: 1.4834\n",
      "Epoch [6196/10000], Loss: 1.4832\n",
      "Epoch [6197/10000], Loss: 1.4831\n",
      "Epoch [6198/10000], Loss: 1.4830\n",
      "Epoch [6199/10000], Loss: 1.4828\n",
      "Epoch [6200/10000], Loss: 1.4827\n",
      "Epoch [6201/10000], Loss: 1.4826\n",
      "Epoch [6202/10000], Loss: 1.4824\n",
      "Epoch [6203/10000], Loss: 1.4823\n",
      "Epoch [6204/10000], Loss: 1.4822\n",
      "Epoch [6205/10000], Loss: 1.4820\n",
      "Epoch [6206/10000], Loss: 1.4819\n",
      "Epoch [6207/10000], Loss: 1.4818\n",
      "Epoch [6208/10000], Loss: 1.4816\n",
      "Epoch [6209/10000], Loss: 1.4815\n",
      "Epoch [6210/10000], Loss: 1.4814\n",
      "Epoch [6211/10000], Loss: 1.4812\n",
      "Epoch [6212/10000], Loss: 1.4811\n",
      "Epoch [6213/10000], Loss: 1.4810\n",
      "Epoch [6214/10000], Loss: 1.4808\n",
      "Epoch [6215/10000], Loss: 1.4807\n",
      "Epoch [6216/10000], Loss: 1.4806\n",
      "Epoch [6217/10000], Loss: 1.4804\n",
      "Epoch [6218/10000], Loss: 1.4803\n",
      "Epoch [6219/10000], Loss: 1.4802\n",
      "Epoch [6220/10000], Loss: 1.4801\n",
      "Epoch [6221/10000], Loss: 1.4799\n",
      "Epoch [6222/10000], Loss: 1.4798\n",
      "Epoch [6223/10000], Loss: 1.4797\n",
      "Epoch [6224/10000], Loss: 1.4795\n",
      "Epoch [6225/10000], Loss: 1.4794\n",
      "Epoch [6226/10000], Loss: 1.4793\n",
      "Epoch [6227/10000], Loss: 1.4791\n",
      "Epoch [6228/10000], Loss: 1.4790\n",
      "Epoch [6229/10000], Loss: 1.4789\n",
      "Epoch [6230/10000], Loss: 1.4787\n",
      "Epoch [6231/10000], Loss: 1.4786\n",
      "Epoch [6232/10000], Loss: 1.4785\n",
      "Epoch [6233/10000], Loss: 1.4783\n",
      "Epoch [6234/10000], Loss: 1.4782\n",
      "Epoch [6235/10000], Loss: 1.4781\n",
      "Epoch [6236/10000], Loss: 1.4779\n",
      "Epoch [6237/10000], Loss: 1.4778\n",
      "Epoch [6238/10000], Loss: 1.4777\n",
      "Epoch [6239/10000], Loss: 1.4775\n",
      "Epoch [6240/10000], Loss: 1.4774\n",
      "Epoch [6241/10000], Loss: 1.4773\n",
      "Epoch [6242/10000], Loss: 1.4771\n",
      "Epoch [6243/10000], Loss: 1.4770\n",
      "Epoch [6244/10000], Loss: 1.4769\n",
      "Epoch [6245/10000], Loss: 1.4767\n",
      "Epoch [6246/10000], Loss: 1.4766\n",
      "Epoch [6247/10000], Loss: 1.4765\n",
      "Epoch [6248/10000], Loss: 1.4763\n",
      "Epoch [6249/10000], Loss: 1.4762\n",
      "Epoch [6250/10000], Loss: 1.4761\n",
      "Epoch [6251/10000], Loss: 1.4760\n",
      "Epoch [6252/10000], Loss: 1.4758\n",
      "Epoch [6253/10000], Loss: 1.4757\n",
      "Epoch [6254/10000], Loss: 1.4756\n",
      "Epoch [6255/10000], Loss: 1.4754\n",
      "Epoch [6256/10000], Loss: 1.4753\n",
      "Epoch [6257/10000], Loss: 1.4752\n",
      "Epoch [6258/10000], Loss: 1.4750\n",
      "Epoch [6259/10000], Loss: 1.4749\n",
      "Epoch [6260/10000], Loss: 1.4748\n",
      "Epoch [6261/10000], Loss: 1.4746\n",
      "Epoch [6262/10000], Loss: 1.4745\n",
      "Epoch [6263/10000], Loss: 1.4744\n",
      "Epoch [6264/10000], Loss: 1.4742\n",
      "Epoch [6265/10000], Loss: 1.4741\n",
      "Epoch [6266/10000], Loss: 1.4740\n",
      "Epoch [6267/10000], Loss: 1.4738\n",
      "Epoch [6268/10000], Loss: 1.4737\n",
      "Epoch [6269/10000], Loss: 1.4736\n",
      "Epoch [6270/10000], Loss: 1.4734\n",
      "Epoch [6271/10000], Loss: 1.4733\n",
      "Epoch [6272/10000], Loss: 1.4732\n",
      "Epoch [6273/10000], Loss: 1.4730\n",
      "Epoch [6274/10000], Loss: 1.4729\n",
      "Epoch [6275/10000], Loss: 1.4728\n",
      "Epoch [6276/10000], Loss: 1.4726\n",
      "Epoch [6277/10000], Loss: 1.4725\n",
      "Epoch [6278/10000], Loss: 1.4724\n",
      "Epoch [6279/10000], Loss: 1.4723\n",
      "Epoch [6280/10000], Loss: 1.4721\n",
      "Epoch [6281/10000], Loss: 1.4720\n",
      "Epoch [6282/10000], Loss: 1.4719\n",
      "Epoch [6283/10000], Loss: 1.4717\n",
      "Epoch [6284/10000], Loss: 1.4716\n",
      "Epoch [6285/10000], Loss: 1.4715\n",
      "Epoch [6286/10000], Loss: 1.4713\n",
      "Epoch [6287/10000], Loss: 1.4712\n",
      "Epoch [6288/10000], Loss: 1.4711\n",
      "Epoch [6289/10000], Loss: 1.4709\n",
      "Epoch [6290/10000], Loss: 1.4708\n",
      "Epoch [6291/10000], Loss: 1.4707\n",
      "Epoch [6292/10000], Loss: 1.4705\n",
      "Epoch [6293/10000], Loss: 1.4704\n",
      "Epoch [6294/10000], Loss: 1.4703\n",
      "Epoch [6295/10000], Loss: 1.4701\n",
      "Epoch [6296/10000], Loss: 1.4700\n",
      "Epoch [6297/10000], Loss: 1.4699\n",
      "Epoch [6298/10000], Loss: 1.4697\n",
      "Epoch [6299/10000], Loss: 1.4696\n",
      "Epoch [6300/10000], Loss: 1.4695\n",
      "Epoch [6301/10000], Loss: 1.4693\n",
      "Epoch [6302/10000], Loss: 1.4692\n",
      "Epoch [6303/10000], Loss: 1.4691\n",
      "Epoch [6304/10000], Loss: 1.4690\n",
      "Epoch [6305/10000], Loss: 1.4688\n",
      "Epoch [6306/10000], Loss: 1.4687\n",
      "Epoch [6307/10000], Loss: 1.4686\n",
      "Epoch [6308/10000], Loss: 1.4684\n",
      "Epoch [6309/10000], Loss: 1.4683\n",
      "Epoch [6310/10000], Loss: 1.4682\n",
      "Epoch [6311/10000], Loss: 1.4680\n",
      "Epoch [6312/10000], Loss: 1.4679\n",
      "Epoch [6313/10000], Loss: 1.4678\n",
      "Epoch [6314/10000], Loss: 1.4676\n",
      "Epoch [6315/10000], Loss: 1.4675\n",
      "Epoch [6316/10000], Loss: 1.4674\n",
      "Epoch [6317/10000], Loss: 1.4672\n",
      "Epoch [6318/10000], Loss: 1.4671\n",
      "Epoch [6319/10000], Loss: 1.4670\n",
      "Epoch [6320/10000], Loss: 1.4668\n",
      "Epoch [6321/10000], Loss: 1.4667\n",
      "Epoch [6322/10000], Loss: 1.4666\n",
      "Epoch [6323/10000], Loss: 1.4664\n",
      "Epoch [6324/10000], Loss: 1.4663\n",
      "Epoch [6325/10000], Loss: 1.4662\n",
      "Epoch [6326/10000], Loss: 1.4660\n",
      "Epoch [6327/10000], Loss: 1.4659\n",
      "Epoch [6328/10000], Loss: 1.4658\n",
      "Epoch [6329/10000], Loss: 1.4657\n",
      "Epoch [6330/10000], Loss: 1.4655\n",
      "Epoch [6331/10000], Loss: 1.4654\n",
      "Epoch [6332/10000], Loss: 1.4653\n",
      "Epoch [6333/10000], Loss: 1.4651\n",
      "Epoch [6334/10000], Loss: 1.4650\n",
      "Epoch [6335/10000], Loss: 1.4649\n",
      "Epoch [6336/10000], Loss: 1.4647\n",
      "Epoch [6337/10000], Loss: 1.4646\n",
      "Epoch [6338/10000], Loss: 1.4645\n",
      "Epoch [6339/10000], Loss: 1.4643\n",
      "Epoch [6340/10000], Loss: 1.4642\n",
      "Epoch [6341/10000], Loss: 1.4641\n",
      "Epoch [6342/10000], Loss: 1.4639\n",
      "Epoch [6343/10000], Loss: 1.4638\n",
      "Epoch [6344/10000], Loss: 1.4637\n",
      "Epoch [6345/10000], Loss: 1.4635\n",
      "Epoch [6346/10000], Loss: 1.4634\n",
      "Epoch [6347/10000], Loss: 1.4633\n",
      "Epoch [6348/10000], Loss: 1.4631\n",
      "Epoch [6349/10000], Loss: 1.4630\n",
      "Epoch [6350/10000], Loss: 1.4629\n",
      "Epoch [6351/10000], Loss: 1.4627\n",
      "Epoch [6352/10000], Loss: 1.4626\n",
      "Epoch [6353/10000], Loss: 1.4625\n",
      "Epoch [6354/10000], Loss: 1.4624\n",
      "Epoch [6355/10000], Loss: 1.4622\n",
      "Epoch [6356/10000], Loss: 1.4621\n",
      "Epoch [6357/10000], Loss: 1.4620\n",
      "Epoch [6358/10000], Loss: 1.4618\n",
      "Epoch [6359/10000], Loss: 1.4617\n",
      "Epoch [6360/10000], Loss: 1.4616\n",
      "Epoch [6361/10000], Loss: 1.4614\n",
      "Epoch [6362/10000], Loss: 1.4613\n",
      "Epoch [6363/10000], Loss: 1.4612\n",
      "Epoch [6364/10000], Loss: 1.4610\n",
      "Epoch [6365/10000], Loss: 1.4609\n",
      "Epoch [6366/10000], Loss: 1.4608\n",
      "Epoch [6367/10000], Loss: 1.4606\n",
      "Epoch [6368/10000], Loss: 1.4605\n",
      "Epoch [6369/10000], Loss: 1.4604\n",
      "Epoch [6370/10000], Loss: 1.4602\n",
      "Epoch [6371/10000], Loss: 1.4601\n",
      "Epoch [6372/10000], Loss: 1.4600\n",
      "Epoch [6373/10000], Loss: 1.4598\n",
      "Epoch [6374/10000], Loss: 1.4597\n",
      "Epoch [6375/10000], Loss: 1.4596\n",
      "Epoch [6376/10000], Loss: 1.4595\n",
      "Epoch [6377/10000], Loss: 1.4593\n",
      "Epoch [6378/10000], Loss: 1.4592\n",
      "Epoch [6379/10000], Loss: 1.4591\n",
      "Epoch [6380/10000], Loss: 1.4589\n",
      "Epoch [6381/10000], Loss: 1.4588\n",
      "Epoch [6382/10000], Loss: 1.4587\n",
      "Epoch [6383/10000], Loss: 1.4585\n",
      "Epoch [6384/10000], Loss: 1.4584\n",
      "Epoch [6385/10000], Loss: 1.4583\n",
      "Epoch [6386/10000], Loss: 1.4581\n",
      "Epoch [6387/10000], Loss: 1.4580\n",
      "Epoch [6388/10000], Loss: 1.4579\n",
      "Epoch [6389/10000], Loss: 1.4577\n",
      "Epoch [6390/10000], Loss: 1.4576\n",
      "Epoch [6391/10000], Loss: 1.4575\n",
      "Epoch [6392/10000], Loss: 1.4573\n",
      "Epoch [6393/10000], Loss: 1.4572\n",
      "Epoch [6394/10000], Loss: 1.4571\n",
      "Epoch [6395/10000], Loss: 1.4570\n",
      "Epoch [6396/10000], Loss: 1.4568\n",
      "Epoch [6397/10000], Loss: 1.4567\n",
      "Epoch [6398/10000], Loss: 1.4566\n",
      "Epoch [6399/10000], Loss: 1.4564\n",
      "Epoch [6400/10000], Loss: 1.4563\n",
      "Epoch [6401/10000], Loss: 1.4562\n",
      "Epoch [6402/10000], Loss: 1.4560\n",
      "Epoch [6403/10000], Loss: 1.4559\n",
      "Epoch [6404/10000], Loss: 1.4558\n",
      "Epoch [6405/10000], Loss: 1.4556\n",
      "Epoch [6406/10000], Loss: 1.4555\n",
      "Epoch [6407/10000], Loss: 1.4554\n",
      "Epoch [6408/10000], Loss: 1.4552\n",
      "Epoch [6409/10000], Loss: 1.4551\n",
      "Epoch [6410/10000], Loss: 1.4550\n",
      "Epoch [6411/10000], Loss: 1.4548\n",
      "Epoch [6412/10000], Loss: 1.4547\n",
      "Epoch [6413/10000], Loss: 1.4546\n",
      "Epoch [6414/10000], Loss: 1.4544\n",
      "Epoch [6415/10000], Loss: 1.4543\n",
      "Epoch [6416/10000], Loss: 1.4542\n",
      "Epoch [6417/10000], Loss: 1.4541\n",
      "Epoch [6418/10000], Loss: 1.4539\n",
      "Epoch [6419/10000], Loss: 1.4538\n",
      "Epoch [6420/10000], Loss: 1.4537\n",
      "Epoch [6421/10000], Loss: 1.4535\n",
      "Epoch [6422/10000], Loss: 1.4534\n",
      "Epoch [6423/10000], Loss: 1.4533\n",
      "Epoch [6424/10000], Loss: 1.4531\n",
      "Epoch [6425/10000], Loss: 1.4530\n",
      "Epoch [6426/10000], Loss: 1.4529\n",
      "Epoch [6427/10000], Loss: 1.4527\n",
      "Epoch [6428/10000], Loss: 1.4526\n",
      "Epoch [6429/10000], Loss: 1.4525\n",
      "Epoch [6430/10000], Loss: 1.4523\n",
      "Epoch [6431/10000], Loss: 1.4522\n",
      "Epoch [6432/10000], Loss: 1.4521\n",
      "Epoch [6433/10000], Loss: 1.4519\n",
      "Epoch [6434/10000], Loss: 1.4518\n",
      "Epoch [6435/10000], Loss: 1.4517\n",
      "Epoch [6436/10000], Loss: 1.4516\n",
      "Epoch [6437/10000], Loss: 1.4514\n",
      "Epoch [6438/10000], Loss: 1.4513\n",
      "Epoch [6439/10000], Loss: 1.4512\n",
      "Epoch [6440/10000], Loss: 1.4510\n",
      "Epoch [6441/10000], Loss: 1.4509\n",
      "Epoch [6442/10000], Loss: 1.4508\n",
      "Epoch [6443/10000], Loss: 1.4506\n",
      "Epoch [6444/10000], Loss: 1.4505\n",
      "Epoch [6445/10000], Loss: 1.4504\n",
      "Epoch [6446/10000], Loss: 1.4502\n",
      "Epoch [6447/10000], Loss: 1.4501\n",
      "Epoch [6448/10000], Loss: 1.4500\n",
      "Epoch [6449/10000], Loss: 1.4498\n",
      "Epoch [6450/10000], Loss: 1.4497\n",
      "Epoch [6451/10000], Loss: 1.4496\n",
      "Epoch [6452/10000], Loss: 1.4494\n",
      "Epoch [6453/10000], Loss: 1.4493\n",
      "Epoch [6454/10000], Loss: 1.4492\n",
      "Epoch [6455/10000], Loss: 1.4491\n",
      "Epoch [6456/10000], Loss: 1.4489\n",
      "Epoch [6457/10000], Loss: 1.4488\n",
      "Epoch [6458/10000], Loss: 1.4487\n",
      "Epoch [6459/10000], Loss: 1.4485\n",
      "Epoch [6460/10000], Loss: 1.4484\n",
      "Epoch [6461/10000], Loss: 1.4483\n",
      "Epoch [6462/10000], Loss: 1.4481\n",
      "Epoch [6463/10000], Loss: 1.4480\n",
      "Epoch [6464/10000], Loss: 1.4479\n",
      "Epoch [6465/10000], Loss: 1.4477\n",
      "Epoch [6466/10000], Loss: 1.4476\n",
      "Epoch [6467/10000], Loss: 1.4475\n",
      "Epoch [6468/10000], Loss: 1.4473\n",
      "Epoch [6469/10000], Loss: 1.4472\n",
      "Epoch [6470/10000], Loss: 1.4471\n",
      "Epoch [6471/10000], Loss: 1.4470\n",
      "Epoch [6472/10000], Loss: 1.4468\n",
      "Epoch [6473/10000], Loss: 1.4467\n",
      "Epoch [6474/10000], Loss: 1.4466\n",
      "Epoch [6475/10000], Loss: 1.4464\n",
      "Epoch [6476/10000], Loss: 1.4463\n",
      "Epoch [6477/10000], Loss: 1.4462\n",
      "Epoch [6478/10000], Loss: 1.4460\n",
      "Epoch [6479/10000], Loss: 1.4459\n",
      "Epoch [6480/10000], Loss: 1.4458\n",
      "Epoch [6481/10000], Loss: 1.4456\n",
      "Epoch [6482/10000], Loss: 1.4455\n",
      "Epoch [6483/10000], Loss: 1.4454\n",
      "Epoch [6484/10000], Loss: 1.4452\n",
      "Epoch [6485/10000], Loss: 1.4451\n",
      "Epoch [6486/10000], Loss: 1.4450\n",
      "Epoch [6487/10000], Loss: 1.4448\n",
      "Epoch [6488/10000], Loss: 1.4447\n",
      "Epoch [6489/10000], Loss: 1.4446\n",
      "Epoch [6490/10000], Loss: 1.4445\n",
      "Epoch [6491/10000], Loss: 1.4443\n",
      "Epoch [6492/10000], Loss: 1.4442\n",
      "Epoch [6493/10000], Loss: 1.4441\n",
      "Epoch [6494/10000], Loss: 1.4439\n",
      "Epoch [6495/10000], Loss: 1.4438\n",
      "Epoch [6496/10000], Loss: 1.4437\n",
      "Epoch [6497/10000], Loss: 1.4435\n",
      "Epoch [6498/10000], Loss: 1.4434\n",
      "Epoch [6499/10000], Loss: 1.4433\n",
      "Epoch [6500/10000], Loss: 1.4431\n",
      "Epoch [6501/10000], Loss: 1.4430\n",
      "Epoch [6502/10000], Loss: 1.4429\n",
      "Epoch [6503/10000], Loss: 1.4427\n",
      "Epoch [6504/10000], Loss: 1.4426\n",
      "Epoch [6505/10000], Loss: 1.4425\n",
      "Epoch [6506/10000], Loss: 1.4424\n",
      "Epoch [6507/10000], Loss: 1.4422\n",
      "Epoch [6508/10000], Loss: 1.4421\n",
      "Epoch [6509/10000], Loss: 1.4420\n",
      "Epoch [6510/10000], Loss: 1.4418\n",
      "Epoch [6511/10000], Loss: 1.4417\n",
      "Epoch [6512/10000], Loss: 1.4416\n",
      "Epoch [6513/10000], Loss: 1.4414\n",
      "Epoch [6514/10000], Loss: 1.4413\n",
      "Epoch [6515/10000], Loss: 1.4412\n",
      "Epoch [6516/10000], Loss: 1.4410\n",
      "Epoch [6517/10000], Loss: 1.4409\n",
      "Epoch [6518/10000], Loss: 1.4408\n",
      "Epoch [6519/10000], Loss: 1.4406\n",
      "Epoch [6520/10000], Loss: 1.4405\n",
      "Epoch [6521/10000], Loss: 1.4404\n",
      "Epoch [6522/10000], Loss: 1.4403\n",
      "Epoch [6523/10000], Loss: 1.4401\n",
      "Epoch [6524/10000], Loss: 1.4400\n",
      "Epoch [6525/10000], Loss: 1.4399\n",
      "Epoch [6526/10000], Loss: 1.4397\n",
      "Epoch [6527/10000], Loss: 1.4396\n",
      "Epoch [6528/10000], Loss: 1.4395\n",
      "Epoch [6529/10000], Loss: 1.4393\n",
      "Epoch [6530/10000], Loss: 1.4392\n",
      "Epoch [6531/10000], Loss: 1.4391\n",
      "Epoch [6532/10000], Loss: 1.4389\n",
      "Epoch [6533/10000], Loss: 1.4388\n",
      "Epoch [6534/10000], Loss: 1.4387\n",
      "Epoch [6535/10000], Loss: 1.4385\n",
      "Epoch [6536/10000], Loss: 1.4384\n",
      "Epoch [6537/10000], Loss: 1.4383\n",
      "Epoch [6538/10000], Loss: 1.4382\n",
      "Epoch [6539/10000], Loss: 1.4380\n",
      "Epoch [6540/10000], Loss: 1.4379\n",
      "Epoch [6541/10000], Loss: 1.4378\n",
      "Epoch [6542/10000], Loss: 1.4376\n",
      "Epoch [6543/10000], Loss: 1.4375\n",
      "Epoch [6544/10000], Loss: 1.4374\n",
      "Epoch [6545/10000], Loss: 1.4372\n",
      "Epoch [6546/10000], Loss: 1.4371\n",
      "Epoch [6547/10000], Loss: 1.4370\n",
      "Epoch [6548/10000], Loss: 1.4368\n",
      "Epoch [6549/10000], Loss: 1.4367\n",
      "Epoch [6550/10000], Loss: 1.4366\n",
      "Epoch [6551/10000], Loss: 1.4364\n",
      "Epoch [6552/10000], Loss: 1.4363\n",
      "Epoch [6553/10000], Loss: 1.4362\n",
      "Epoch [6554/10000], Loss: 1.4361\n",
      "Epoch [6555/10000], Loss: 1.4359\n",
      "Epoch [6556/10000], Loss: 1.4358\n",
      "Epoch [6557/10000], Loss: 1.4357\n",
      "Epoch [6558/10000], Loss: 1.4355\n",
      "Epoch [6559/10000], Loss: 1.4354\n",
      "Epoch [6560/10000], Loss: 1.4353\n",
      "Epoch [6561/10000], Loss: 1.4351\n",
      "Epoch [6562/10000], Loss: 1.4350\n",
      "Epoch [6563/10000], Loss: 1.4349\n",
      "Epoch [6564/10000], Loss: 1.4347\n",
      "Epoch [6565/10000], Loss: 1.4346\n",
      "Epoch [6566/10000], Loss: 1.4345\n",
      "Epoch [6567/10000], Loss: 1.4343\n",
      "Epoch [6568/10000], Loss: 1.4342\n",
      "Epoch [6569/10000], Loss: 1.4341\n",
      "Epoch [6570/10000], Loss: 1.4340\n",
      "Epoch [6571/10000], Loss: 1.4338\n",
      "Epoch [6572/10000], Loss: 1.4337\n",
      "Epoch [6573/10000], Loss: 1.4336\n",
      "Epoch [6574/10000], Loss: 1.4334\n",
      "Epoch [6575/10000], Loss: 1.4333\n",
      "Epoch [6576/10000], Loss: 1.4332\n",
      "Epoch [6577/10000], Loss: 1.4330\n",
      "Epoch [6578/10000], Loss: 1.4329\n",
      "Epoch [6579/10000], Loss: 1.4328\n",
      "Epoch [6580/10000], Loss: 1.4326\n",
      "Epoch [6581/10000], Loss: 1.4325\n",
      "Epoch [6582/10000], Loss: 1.4324\n",
      "Epoch [6583/10000], Loss: 1.4322\n",
      "Epoch [6584/10000], Loss: 1.4321\n",
      "Epoch [6585/10000], Loss: 1.4320\n",
      "Epoch [6586/10000], Loss: 1.4319\n",
      "Epoch [6587/10000], Loss: 1.4317\n",
      "Epoch [6588/10000], Loss: 1.4316\n",
      "Epoch [6589/10000], Loss: 1.4315\n",
      "Epoch [6590/10000], Loss: 1.4313\n",
      "Epoch [6591/10000], Loss: 1.4312\n",
      "Epoch [6592/10000], Loss: 1.4311\n",
      "Epoch [6593/10000], Loss: 1.4309\n",
      "Epoch [6594/10000], Loss: 1.4308\n",
      "Epoch [6595/10000], Loss: 1.4307\n",
      "Epoch [6596/10000], Loss: 1.4305\n",
      "Epoch [6597/10000], Loss: 1.4304\n",
      "Epoch [6598/10000], Loss: 1.4303\n",
      "Epoch [6599/10000], Loss: 1.4302\n",
      "Epoch [6600/10000], Loss: 1.4300\n",
      "Epoch [6601/10000], Loss: 1.4299\n",
      "Epoch [6602/10000], Loss: 1.4298\n",
      "Epoch [6603/10000], Loss: 1.4296\n",
      "Epoch [6604/10000], Loss: 1.4295\n",
      "Epoch [6605/10000], Loss: 1.4294\n",
      "Epoch [6606/10000], Loss: 1.4292\n",
      "Epoch [6607/10000], Loss: 1.4291\n",
      "Epoch [6608/10000], Loss: 1.4290\n",
      "Epoch [6609/10000], Loss: 1.4288\n",
      "Epoch [6610/10000], Loss: 1.4287\n",
      "Epoch [6611/10000], Loss: 1.4286\n",
      "Epoch [6612/10000], Loss: 1.4284\n",
      "Epoch [6613/10000], Loss: 1.4283\n",
      "Epoch [6614/10000], Loss: 1.4282\n",
      "Epoch [6615/10000], Loss: 1.4281\n",
      "Epoch [6616/10000], Loss: 1.4279\n",
      "Epoch [6617/10000], Loss: 1.4278\n",
      "Epoch [6618/10000], Loss: 1.4277\n",
      "Epoch [6619/10000], Loss: 1.4275\n",
      "Epoch [6620/10000], Loss: 1.4274\n",
      "Epoch [6621/10000], Loss: 1.4273\n",
      "Epoch [6622/10000], Loss: 1.4271\n",
      "Epoch [6623/10000], Loss: 1.4270\n",
      "Epoch [6624/10000], Loss: 1.4269\n",
      "Epoch [6625/10000], Loss: 1.4267\n",
      "Epoch [6626/10000], Loss: 1.4266\n",
      "Epoch [6627/10000], Loss: 1.4265\n",
      "Epoch [6628/10000], Loss: 1.4264\n",
      "Epoch [6629/10000], Loss: 1.4262\n",
      "Epoch [6630/10000], Loss: 1.4261\n",
      "Epoch [6631/10000], Loss: 1.4260\n",
      "Epoch [6632/10000], Loss: 1.4258\n",
      "Epoch [6633/10000], Loss: 1.4257\n",
      "Epoch [6634/10000], Loss: 1.4256\n",
      "Epoch [6635/10000], Loss: 1.4254\n",
      "Epoch [6636/10000], Loss: 1.4253\n",
      "Epoch [6637/10000], Loss: 1.4252\n",
      "Epoch [6638/10000], Loss: 1.4250\n",
      "Epoch [6639/10000], Loss: 1.4249\n",
      "Epoch [6640/10000], Loss: 1.4248\n",
      "Epoch [6641/10000], Loss: 1.4247\n",
      "Epoch [6642/10000], Loss: 1.4245\n",
      "Epoch [6643/10000], Loss: 1.4244\n",
      "Epoch [6644/10000], Loss: 1.4243\n",
      "Epoch [6645/10000], Loss: 1.4241\n",
      "Epoch [6646/10000], Loss: 1.4240\n",
      "Epoch [6647/10000], Loss: 1.4239\n",
      "Epoch [6648/10000], Loss: 1.4237\n",
      "Epoch [6649/10000], Loss: 1.4236\n",
      "Epoch [6650/10000], Loss: 1.4235\n",
      "Epoch [6651/10000], Loss: 1.4233\n",
      "Epoch [6652/10000], Loss: 1.4232\n",
      "Epoch [6653/10000], Loss: 1.4231\n",
      "Epoch [6654/10000], Loss: 1.4229\n",
      "Epoch [6655/10000], Loss: 1.4228\n",
      "Epoch [6656/10000], Loss: 1.4227\n",
      "Epoch [6657/10000], Loss: 1.4226\n",
      "Epoch [6658/10000], Loss: 1.4224\n",
      "Epoch [6659/10000], Loss: 1.4223\n",
      "Epoch [6660/10000], Loss: 1.4222\n",
      "Epoch [6661/10000], Loss: 1.4220\n",
      "Epoch [6662/10000], Loss: 1.4219\n",
      "Epoch [6663/10000], Loss: 1.4218\n",
      "Epoch [6664/10000], Loss: 1.4216\n",
      "Epoch [6665/10000], Loss: 1.4215\n",
      "Epoch [6666/10000], Loss: 1.4214\n",
      "Epoch [6667/10000], Loss: 1.4212\n",
      "Epoch [6668/10000], Loss: 1.4211\n",
      "Epoch [6669/10000], Loss: 1.4210\n",
      "Epoch [6670/10000], Loss: 1.4209\n",
      "Epoch [6671/10000], Loss: 1.4207\n",
      "Epoch [6672/10000], Loss: 1.4206\n",
      "Epoch [6673/10000], Loss: 1.4205\n",
      "Epoch [6674/10000], Loss: 1.4203\n",
      "Epoch [6675/10000], Loss: 1.4202\n",
      "Epoch [6676/10000], Loss: 1.4201\n",
      "Epoch [6677/10000], Loss: 1.4199\n",
      "Epoch [6678/10000], Loss: 1.4198\n",
      "Epoch [6679/10000], Loss: 1.4197\n",
      "Epoch [6680/10000], Loss: 1.4195\n",
      "Epoch [6681/10000], Loss: 1.4194\n",
      "Epoch [6682/10000], Loss: 1.4193\n",
      "Epoch [6683/10000], Loss: 1.4192\n",
      "Epoch [6684/10000], Loss: 1.4190\n",
      "Epoch [6685/10000], Loss: 1.4189\n",
      "Epoch [6686/10000], Loss: 1.4188\n",
      "Epoch [6687/10000], Loss: 1.4186\n",
      "Epoch [6688/10000], Loss: 1.4185\n",
      "Epoch [6689/10000], Loss: 1.4184\n",
      "Epoch [6690/10000], Loss: 1.4182\n",
      "Epoch [6691/10000], Loss: 1.4181\n",
      "Epoch [6692/10000], Loss: 1.4180\n",
      "Epoch [6693/10000], Loss: 1.4178\n",
      "Epoch [6694/10000], Loss: 1.4177\n",
      "Epoch [6695/10000], Loss: 1.4176\n",
      "Epoch [6696/10000], Loss: 1.4175\n",
      "Epoch [6697/10000], Loss: 1.4173\n",
      "Epoch [6698/10000], Loss: 1.4172\n",
      "Epoch [6699/10000], Loss: 1.4171\n",
      "Epoch [6700/10000], Loss: 1.4169\n",
      "Epoch [6701/10000], Loss: 1.4168\n",
      "Epoch [6702/10000], Loss: 1.4167\n",
      "Epoch [6703/10000], Loss: 1.4165\n",
      "Epoch [6704/10000], Loss: 1.4164\n",
      "Epoch [6705/10000], Loss: 1.4163\n",
      "Epoch [6706/10000], Loss: 1.4161\n",
      "Epoch [6707/10000], Loss: 1.4160\n",
      "Epoch [6708/10000], Loss: 1.4159\n",
      "Epoch [6709/10000], Loss: 1.4158\n",
      "Epoch [6710/10000], Loss: 1.4156\n",
      "Epoch [6711/10000], Loss: 1.4155\n",
      "Epoch [6712/10000], Loss: 1.4154\n",
      "Epoch [6713/10000], Loss: 1.4152\n",
      "Epoch [6714/10000], Loss: 1.4151\n",
      "Epoch [6715/10000], Loss: 1.4150\n",
      "Epoch [6716/10000], Loss: 1.4148\n",
      "Epoch [6717/10000], Loss: 1.4147\n",
      "Epoch [6718/10000], Loss: 1.4146\n",
      "Epoch [6719/10000], Loss: 1.4144\n",
      "Epoch [6720/10000], Loss: 1.4143\n",
      "Epoch [6721/10000], Loss: 1.4142\n",
      "Epoch [6722/10000], Loss: 1.4141\n",
      "Epoch [6723/10000], Loss: 1.4139\n",
      "Epoch [6724/10000], Loss: 1.4138\n",
      "Epoch [6725/10000], Loss: 1.4137\n",
      "Epoch [6726/10000], Loss: 1.4135\n",
      "Epoch [6727/10000], Loss: 1.4134\n",
      "Epoch [6728/10000], Loss: 1.4133\n",
      "Epoch [6729/10000], Loss: 1.4131\n",
      "Epoch [6730/10000], Loss: 1.4130\n",
      "Epoch [6731/10000], Loss: 1.4129\n",
      "Epoch [6732/10000], Loss: 1.4128\n",
      "Epoch [6733/10000], Loss: 1.4126\n",
      "Epoch [6734/10000], Loss: 1.4125\n",
      "Epoch [6735/10000], Loss: 1.4124\n",
      "Epoch [6736/10000], Loss: 1.4122\n",
      "Epoch [6737/10000], Loss: 1.4121\n",
      "Epoch [6738/10000], Loss: 1.4120\n",
      "Epoch [6739/10000], Loss: 1.4118\n",
      "Epoch [6740/10000], Loss: 1.4117\n",
      "Epoch [6741/10000], Loss: 1.4116\n",
      "Epoch [6742/10000], Loss: 1.4114\n",
      "Epoch [6743/10000], Loss: 1.4113\n",
      "Epoch [6744/10000], Loss: 1.4112\n",
      "Epoch [6745/10000], Loss: 1.4111\n",
      "Epoch [6746/10000], Loss: 1.4109\n",
      "Epoch [6747/10000], Loss: 1.4108\n",
      "Epoch [6748/10000], Loss: 1.4107\n",
      "Epoch [6749/10000], Loss: 1.4105\n",
      "Epoch [6750/10000], Loss: 1.4104\n",
      "Epoch [6751/10000], Loss: 1.4103\n",
      "Epoch [6752/10000], Loss: 1.4101\n",
      "Epoch [6753/10000], Loss: 1.4100\n",
      "Epoch [6754/10000], Loss: 1.4099\n",
      "Epoch [6755/10000], Loss: 1.4097\n",
      "Epoch [6756/10000], Loss: 1.4096\n",
      "Epoch [6757/10000], Loss: 1.4095\n",
      "Epoch [6758/10000], Loss: 1.4094\n",
      "Epoch [6759/10000], Loss: 1.4092\n",
      "Epoch [6760/10000], Loss: 1.4091\n",
      "Epoch [6761/10000], Loss: 1.4090\n",
      "Epoch [6762/10000], Loss: 1.4088\n",
      "Epoch [6763/10000], Loss: 1.4087\n",
      "Epoch [6764/10000], Loss: 1.4086\n",
      "Epoch [6765/10000], Loss: 1.4084\n",
      "Epoch [6766/10000], Loss: 1.4083\n",
      "Epoch [6767/10000], Loss: 1.4082\n",
      "Epoch [6768/10000], Loss: 1.4080\n",
      "Epoch [6769/10000], Loss: 1.4079\n",
      "Epoch [6770/10000], Loss: 1.4078\n",
      "Epoch [6771/10000], Loss: 1.4077\n",
      "Epoch [6772/10000], Loss: 1.4075\n",
      "Epoch [6773/10000], Loss: 1.4074\n",
      "Epoch [6774/10000], Loss: 1.4073\n",
      "Epoch [6775/10000], Loss: 1.4071\n",
      "Epoch [6776/10000], Loss: 1.4070\n",
      "Epoch [6777/10000], Loss: 1.4069\n",
      "Epoch [6778/10000], Loss: 1.4067\n",
      "Epoch [6779/10000], Loss: 1.4066\n",
      "Epoch [6780/10000], Loss: 1.4065\n",
      "Epoch [6781/10000], Loss: 1.4064\n",
      "Epoch [6782/10000], Loss: 1.4062\n",
      "Epoch [6783/10000], Loss: 1.4061\n",
      "Epoch [6784/10000], Loss: 1.4060\n",
      "Epoch [6785/10000], Loss: 1.4058\n",
      "Epoch [6786/10000], Loss: 1.4057\n",
      "Epoch [6787/10000], Loss: 1.4056\n",
      "Epoch [6788/10000], Loss: 1.4054\n",
      "Epoch [6789/10000], Loss: 1.4053\n",
      "Epoch [6790/10000], Loss: 1.4052\n",
      "Epoch [6791/10000], Loss: 1.4050\n",
      "Epoch [6792/10000], Loss: 1.4049\n",
      "Epoch [6793/10000], Loss: 1.4048\n",
      "Epoch [6794/10000], Loss: 1.4047\n",
      "Epoch [6795/10000], Loss: 1.4045\n",
      "Epoch [6796/10000], Loss: 1.4044\n",
      "Epoch [6797/10000], Loss: 1.4043\n",
      "Epoch [6798/10000], Loss: 1.4041\n",
      "Epoch [6799/10000], Loss: 1.4040\n",
      "Epoch [6800/10000], Loss: 1.4039\n",
      "Epoch [6801/10000], Loss: 1.4037\n",
      "Epoch [6802/10000], Loss: 1.4036\n",
      "Epoch [6803/10000], Loss: 1.4035\n",
      "Epoch [6804/10000], Loss: 1.4034\n",
      "Epoch [6805/10000], Loss: 1.4032\n",
      "Epoch [6806/10000], Loss: 1.4031\n",
      "Epoch [6807/10000], Loss: 1.4030\n",
      "Epoch [6808/10000], Loss: 1.4028\n",
      "Epoch [6809/10000], Loss: 1.4027\n",
      "Epoch [6810/10000], Loss: 1.4026\n",
      "Epoch [6811/10000], Loss: 1.4024\n",
      "Epoch [6812/10000], Loss: 1.4023\n",
      "Epoch [6813/10000], Loss: 1.4022\n",
      "Epoch [6814/10000], Loss: 1.4020\n",
      "Epoch [6815/10000], Loss: 1.4019\n",
      "Epoch [6816/10000], Loss: 1.4018\n",
      "Epoch [6817/10000], Loss: 1.4017\n",
      "Epoch [6818/10000], Loss: 1.4015\n",
      "Epoch [6819/10000], Loss: 1.4014\n",
      "Epoch [6820/10000], Loss: 1.4013\n",
      "Epoch [6821/10000], Loss: 1.4011\n",
      "Epoch [6822/10000], Loss: 1.4010\n",
      "Epoch [6823/10000], Loss: 1.4009\n",
      "Epoch [6824/10000], Loss: 1.4007\n",
      "Epoch [6825/10000], Loss: 1.4006\n",
      "Epoch [6826/10000], Loss: 1.4005\n",
      "Epoch [6827/10000], Loss: 1.4004\n",
      "Epoch [6828/10000], Loss: 1.4002\n",
      "Epoch [6829/10000], Loss: 1.4001\n",
      "Epoch [6830/10000], Loss: 1.4000\n",
      "Epoch [6831/10000], Loss: 1.3998\n",
      "Epoch [6832/10000], Loss: 1.3997\n",
      "Epoch [6833/10000], Loss: 1.3996\n",
      "Epoch [6834/10000], Loss: 1.3994\n",
      "Epoch [6835/10000], Loss: 1.3993\n",
      "Epoch [6836/10000], Loss: 1.3992\n",
      "Epoch [6837/10000], Loss: 1.3990\n",
      "Epoch [6838/10000], Loss: 1.3989\n",
      "Epoch [6839/10000], Loss: 1.3988\n",
      "Epoch [6840/10000], Loss: 1.3987\n",
      "Epoch [6841/10000], Loss: 1.3985\n",
      "Epoch [6842/10000], Loss: 1.3984\n",
      "Epoch [6843/10000], Loss: 1.3983\n",
      "Epoch [6844/10000], Loss: 1.3981\n",
      "Epoch [6845/10000], Loss: 1.3980\n",
      "Epoch [6846/10000], Loss: 1.3979\n",
      "Epoch [6847/10000], Loss: 1.3977\n",
      "Epoch [6848/10000], Loss: 1.3976\n",
      "Epoch [6849/10000], Loss: 1.3975\n",
      "Epoch [6850/10000], Loss: 1.3974\n",
      "Epoch [6851/10000], Loss: 1.3972\n",
      "Epoch [6852/10000], Loss: 1.3971\n",
      "Epoch [6853/10000], Loss: 1.3970\n",
      "Epoch [6854/10000], Loss: 1.3968\n",
      "Epoch [6855/10000], Loss: 1.3967\n",
      "Epoch [6856/10000], Loss: 1.3966\n",
      "Epoch [6857/10000], Loss: 1.3964\n",
      "Epoch [6858/10000], Loss: 1.3963\n",
      "Epoch [6859/10000], Loss: 1.3962\n",
      "Epoch [6860/10000], Loss: 1.3961\n",
      "Epoch [6861/10000], Loss: 1.3959\n",
      "Epoch [6862/10000], Loss: 1.3958\n",
      "Epoch [6863/10000], Loss: 1.3957\n",
      "Epoch [6864/10000], Loss: 1.3955\n",
      "Epoch [6865/10000], Loss: 1.3954\n",
      "Epoch [6866/10000], Loss: 1.3953\n",
      "Epoch [6867/10000], Loss: 1.3951\n",
      "Epoch [6868/10000], Loss: 1.3950\n",
      "Epoch [6869/10000], Loss: 1.3949\n",
      "Epoch [6870/10000], Loss: 1.3947\n",
      "Epoch [6871/10000], Loss: 1.3946\n",
      "Epoch [6872/10000], Loss: 1.3945\n",
      "Epoch [6873/10000], Loss: 1.3944\n",
      "Epoch [6874/10000], Loss: 1.3942\n",
      "Epoch [6875/10000], Loss: 1.3941\n",
      "Epoch [6876/10000], Loss: 1.3940\n",
      "Epoch [6877/10000], Loss: 1.3938\n",
      "Epoch [6878/10000], Loss: 1.3937\n",
      "Epoch [6879/10000], Loss: 1.3936\n",
      "Epoch [6880/10000], Loss: 1.3934\n",
      "Epoch [6881/10000], Loss: 1.3933\n",
      "Epoch [6882/10000], Loss: 1.3932\n",
      "Epoch [6883/10000], Loss: 1.3931\n",
      "Epoch [6884/10000], Loss: 1.3929\n",
      "Epoch [6885/10000], Loss: 1.3928\n",
      "Epoch [6886/10000], Loss: 1.3927\n",
      "Epoch [6887/10000], Loss: 1.3925\n",
      "Epoch [6888/10000], Loss: 1.3924\n",
      "Epoch [6889/10000], Loss: 1.3923\n",
      "Epoch [6890/10000], Loss: 1.3921\n",
      "Epoch [6891/10000], Loss: 1.3920\n",
      "Epoch [6892/10000], Loss: 1.3919\n",
      "Epoch [6893/10000], Loss: 1.3918\n",
      "Epoch [6894/10000], Loss: 1.3916\n",
      "Epoch [6895/10000], Loss: 1.3915\n",
      "Epoch [6896/10000], Loss: 1.3914\n",
      "Epoch [6897/10000], Loss: 1.3912\n",
      "Epoch [6898/10000], Loss: 1.3911\n",
      "Epoch [6899/10000], Loss: 1.3910\n",
      "Epoch [6900/10000], Loss: 1.3908\n",
      "Epoch [6901/10000], Loss: 1.3907\n",
      "Epoch [6902/10000], Loss: 1.3906\n",
      "Epoch [6903/10000], Loss: 1.3905\n",
      "Epoch [6904/10000], Loss: 1.3903\n",
      "Epoch [6905/10000], Loss: 1.3902\n",
      "Epoch [6906/10000], Loss: 1.3901\n",
      "Epoch [6907/10000], Loss: 1.3899\n",
      "Epoch [6908/10000], Loss: 1.3898\n",
      "Epoch [6909/10000], Loss: 1.3897\n",
      "Epoch [6910/10000], Loss: 1.3895\n",
      "Epoch [6911/10000], Loss: 1.3894\n",
      "Epoch [6912/10000], Loss: 1.3893\n",
      "Epoch [6913/10000], Loss: 1.3891\n",
      "Epoch [6914/10000], Loss: 1.3890\n",
      "Epoch [6915/10000], Loss: 1.3889\n",
      "Epoch [6916/10000], Loss: 1.3888\n",
      "Epoch [6917/10000], Loss: 1.3886\n",
      "Epoch [6918/10000], Loss: 1.3885\n",
      "Epoch [6919/10000], Loss: 1.3884\n",
      "Epoch [6920/10000], Loss: 1.3882\n",
      "Epoch [6921/10000], Loss: 1.3881\n",
      "Epoch [6922/10000], Loss: 1.3880\n",
      "Epoch [6923/10000], Loss: 1.3878\n",
      "Epoch [6924/10000], Loss: 1.3877\n",
      "Epoch [6925/10000], Loss: 1.3876\n",
      "Epoch [6926/10000], Loss: 1.3875\n",
      "Epoch [6927/10000], Loss: 1.3873\n",
      "Epoch [6928/10000], Loss: 1.3872\n",
      "Epoch [6929/10000], Loss: 1.3871\n",
      "Epoch [6930/10000], Loss: 1.3869\n",
      "Epoch [6931/10000], Loss: 1.3868\n",
      "Epoch [6932/10000], Loss: 1.3867\n",
      "Epoch [6933/10000], Loss: 1.3865\n",
      "Epoch [6934/10000], Loss: 1.3864\n",
      "Epoch [6935/10000], Loss: 1.3863\n",
      "Epoch [6936/10000], Loss: 1.3862\n",
      "Epoch [6937/10000], Loss: 1.3860\n",
      "Epoch [6938/10000], Loss: 1.3859\n",
      "Epoch [6939/10000], Loss: 1.3858\n",
      "Epoch [6940/10000], Loss: 1.3856\n",
      "Epoch [6941/10000], Loss: 1.3855\n",
      "Epoch [6942/10000], Loss: 1.3854\n",
      "Epoch [6943/10000], Loss: 1.3852\n",
      "Epoch [6944/10000], Loss: 1.3851\n",
      "Epoch [6945/10000], Loss: 1.3850\n",
      "Epoch [6946/10000], Loss: 1.3849\n",
      "Epoch [6947/10000], Loss: 1.3847\n",
      "Epoch [6948/10000], Loss: 1.3846\n",
      "Epoch [6949/10000], Loss: 1.3845\n",
      "Epoch [6950/10000], Loss: 1.3843\n",
      "Epoch [6951/10000], Loss: 1.3842\n",
      "Epoch [6952/10000], Loss: 1.3841\n",
      "Epoch [6953/10000], Loss: 1.3839\n",
      "Epoch [6954/10000], Loss: 1.3838\n",
      "Epoch [6955/10000], Loss: 1.3837\n",
      "Epoch [6956/10000], Loss: 1.3836\n",
      "Epoch [6957/10000], Loss: 1.3834\n",
      "Epoch [6958/10000], Loss: 1.3833\n",
      "Epoch [6959/10000], Loss: 1.3832\n",
      "Epoch [6960/10000], Loss: 1.3830\n",
      "Epoch [6961/10000], Loss: 1.3829\n",
      "Epoch [6962/10000], Loss: 1.3828\n",
      "Epoch [6963/10000], Loss: 1.3826\n",
      "Epoch [6964/10000], Loss: 1.3825\n",
      "Epoch [6965/10000], Loss: 1.3824\n",
      "Epoch [6966/10000], Loss: 1.3823\n",
      "Epoch [6967/10000], Loss: 1.3821\n",
      "Epoch [6968/10000], Loss: 1.3820\n",
      "Epoch [6969/10000], Loss: 1.3819\n",
      "Epoch [6970/10000], Loss: 1.3817\n",
      "Epoch [6971/10000], Loss: 1.3816\n",
      "Epoch [6972/10000], Loss: 1.3815\n",
      "Epoch [6973/10000], Loss: 1.3813\n",
      "Epoch [6974/10000], Loss: 1.3812\n",
      "Epoch [6975/10000], Loss: 1.3811\n",
      "Epoch [6976/10000], Loss: 1.3810\n",
      "Epoch [6977/10000], Loss: 1.3808\n",
      "Epoch [6978/10000], Loss: 1.3807\n",
      "Epoch [6979/10000], Loss: 1.3806\n",
      "Epoch [6980/10000], Loss: 1.3804\n",
      "Epoch [6981/10000], Loss: 1.3803\n",
      "Epoch [6982/10000], Loss: 1.3802\n",
      "Epoch [6983/10000], Loss: 1.3800\n",
      "Epoch [6984/10000], Loss: 1.3799\n",
      "Epoch [6985/10000], Loss: 1.3798\n",
      "Epoch [6986/10000], Loss: 1.3797\n",
      "Epoch [6987/10000], Loss: 1.3795\n",
      "Epoch [6988/10000], Loss: 1.3794\n",
      "Epoch [6989/10000], Loss: 1.3793\n",
      "Epoch [6990/10000], Loss: 1.3791\n",
      "Epoch [6991/10000], Loss: 1.3790\n",
      "Epoch [6992/10000], Loss: 1.3789\n",
      "Epoch [6993/10000], Loss: 1.3787\n",
      "Epoch [6994/10000], Loss: 1.3786\n",
      "Epoch [6995/10000], Loss: 1.3785\n",
      "Epoch [6996/10000], Loss: 1.3784\n",
      "Epoch [6997/10000], Loss: 1.3782\n",
      "Epoch [6998/10000], Loss: 1.3781\n",
      "Epoch [6999/10000], Loss: 1.3780\n",
      "Epoch [7000/10000], Loss: 1.3778\n",
      "Epoch [7001/10000], Loss: 1.3777\n",
      "Epoch [7002/10000], Loss: 1.3776\n",
      "Epoch [7003/10000], Loss: 1.3774\n",
      "Epoch [7004/10000], Loss: 1.3773\n",
      "Epoch [7005/10000], Loss: 1.3772\n",
      "Epoch [7006/10000], Loss: 1.3771\n",
      "Epoch [7007/10000], Loss: 1.3769\n",
      "Epoch [7008/10000], Loss: 1.3768\n",
      "Epoch [7009/10000], Loss: 1.3767\n",
      "Epoch [7010/10000], Loss: 1.3765\n",
      "Epoch [7011/10000], Loss: 1.3764\n",
      "Epoch [7012/10000], Loss: 1.3763\n",
      "Epoch [7013/10000], Loss: 1.3761\n",
      "Epoch [7014/10000], Loss: 1.3760\n",
      "Epoch [7015/10000], Loss: 1.3759\n",
      "Epoch [7016/10000], Loss: 1.3758\n",
      "Epoch [7017/10000], Loss: 1.3756\n",
      "Epoch [7018/10000], Loss: 1.3755\n",
      "Epoch [7019/10000], Loss: 1.3754\n",
      "Epoch [7020/10000], Loss: 1.3752\n",
      "Epoch [7021/10000], Loss: 1.3751\n",
      "Epoch [7022/10000], Loss: 1.3750\n",
      "Epoch [7023/10000], Loss: 1.3748\n",
      "Epoch [7024/10000], Loss: 1.3747\n",
      "Epoch [7025/10000], Loss: 1.3746\n",
      "Epoch [7026/10000], Loss: 1.3745\n",
      "Epoch [7027/10000], Loss: 1.3743\n",
      "Epoch [7028/10000], Loss: 1.3742\n",
      "Epoch [7029/10000], Loss: 1.3741\n",
      "Epoch [7030/10000], Loss: 1.3739\n",
      "Epoch [7031/10000], Loss: 1.3738\n",
      "Epoch [7032/10000], Loss: 1.3737\n",
      "Epoch [7033/10000], Loss: 1.3735\n",
      "Epoch [7034/10000], Loss: 1.3734\n",
      "Epoch [7035/10000], Loss: 1.3733\n",
      "Epoch [7036/10000], Loss: 1.3732\n",
      "Epoch [7037/10000], Loss: 1.3730\n",
      "Epoch [7038/10000], Loss: 1.3729\n",
      "Epoch [7039/10000], Loss: 1.3728\n",
      "Epoch [7040/10000], Loss: 1.3726\n",
      "Epoch [7041/10000], Loss: 1.3725\n",
      "Epoch [7042/10000], Loss: 1.3724\n",
      "Epoch [7043/10000], Loss: 1.3723\n",
      "Epoch [7044/10000], Loss: 1.3721\n",
      "Epoch [7045/10000], Loss: 1.3720\n",
      "Epoch [7046/10000], Loss: 1.3719\n",
      "Epoch [7047/10000], Loss: 1.3717\n",
      "Epoch [7048/10000], Loss: 1.3716\n",
      "Epoch [7049/10000], Loss: 1.3715\n",
      "Epoch [7050/10000], Loss: 1.3713\n",
      "Epoch [7051/10000], Loss: 1.3712\n",
      "Epoch [7052/10000], Loss: 1.3711\n",
      "Epoch [7053/10000], Loss: 1.3710\n",
      "Epoch [7054/10000], Loss: 1.3708\n",
      "Epoch [7055/10000], Loss: 1.3707\n",
      "Epoch [7056/10000], Loss: 1.3706\n",
      "Epoch [7057/10000], Loss: 1.3704\n",
      "Epoch [7058/10000], Loss: 1.3703\n",
      "Epoch [7059/10000], Loss: 1.3702\n",
      "Epoch [7060/10000], Loss: 1.3700\n",
      "Epoch [7061/10000], Loss: 1.3699\n",
      "Epoch [7062/10000], Loss: 1.3698\n",
      "Epoch [7063/10000], Loss: 1.3697\n",
      "Epoch [7064/10000], Loss: 1.3695\n",
      "Epoch [7065/10000], Loss: 1.3694\n",
      "Epoch [7066/10000], Loss: 1.3693\n",
      "Epoch [7067/10000], Loss: 1.3691\n",
      "Epoch [7068/10000], Loss: 1.3690\n",
      "Epoch [7069/10000], Loss: 1.3689\n",
      "Epoch [7070/10000], Loss: 1.3687\n",
      "Epoch [7071/10000], Loss: 1.3686\n",
      "Epoch [7072/10000], Loss: 1.3685\n",
      "Epoch [7073/10000], Loss: 1.3684\n",
      "Epoch [7074/10000], Loss: 1.3682\n",
      "Epoch [7075/10000], Loss: 1.3681\n",
      "Epoch [7076/10000], Loss: 1.3680\n",
      "Epoch [7077/10000], Loss: 1.3678\n",
      "Epoch [7078/10000], Loss: 1.3677\n",
      "Epoch [7079/10000], Loss: 1.3676\n",
      "Epoch [7080/10000], Loss: 1.3674\n",
      "Epoch [7081/10000], Loss: 1.3673\n",
      "Epoch [7082/10000], Loss: 1.3672\n",
      "Epoch [7083/10000], Loss: 1.3671\n",
      "Epoch [7084/10000], Loss: 1.3669\n",
      "Epoch [7085/10000], Loss: 1.3668\n",
      "Epoch [7086/10000], Loss: 1.3667\n",
      "Epoch [7087/10000], Loss: 1.3665\n",
      "Epoch [7088/10000], Loss: 1.3664\n",
      "Epoch [7089/10000], Loss: 1.3663\n",
      "Epoch [7090/10000], Loss: 1.3662\n",
      "Epoch [7091/10000], Loss: 1.3660\n",
      "Epoch [7092/10000], Loss: 1.3659\n",
      "Epoch [7093/10000], Loss: 1.3658\n",
      "Epoch [7094/10000], Loss: 1.3656\n",
      "Epoch [7095/10000], Loss: 1.3655\n",
      "Epoch [7096/10000], Loss: 1.3654\n",
      "Epoch [7097/10000], Loss: 1.3652\n",
      "Epoch [7098/10000], Loss: 1.3651\n",
      "Epoch [7099/10000], Loss: 1.3650\n",
      "Epoch [7100/10000], Loss: 1.3649\n",
      "Epoch [7101/10000], Loss: 1.3647\n",
      "Epoch [7102/10000], Loss: 1.3646\n",
      "Epoch [7103/10000], Loss: 1.3645\n",
      "Epoch [7104/10000], Loss: 1.3643\n",
      "Epoch [7105/10000], Loss: 1.3642\n",
      "Epoch [7106/10000], Loss: 1.3641\n",
      "Epoch [7107/10000], Loss: 1.3639\n",
      "Epoch [7108/10000], Loss: 1.3638\n",
      "Epoch [7109/10000], Loss: 1.3637\n",
      "Epoch [7110/10000], Loss: 1.3636\n",
      "Epoch [7111/10000], Loss: 1.3634\n",
      "Epoch [7112/10000], Loss: 1.3633\n",
      "Epoch [7113/10000], Loss: 1.3632\n",
      "Epoch [7114/10000], Loss: 1.3630\n",
      "Epoch [7115/10000], Loss: 1.3629\n",
      "Epoch [7116/10000], Loss: 1.3628\n",
      "Epoch [7117/10000], Loss: 1.3626\n",
      "Epoch [7118/10000], Loss: 1.3625\n",
      "Epoch [7119/10000], Loss: 1.3624\n",
      "Epoch [7120/10000], Loss: 1.3623\n",
      "Epoch [7121/10000], Loss: 1.3621\n",
      "Epoch [7122/10000], Loss: 1.3620\n",
      "Epoch [7123/10000], Loss: 1.3619\n",
      "Epoch [7124/10000], Loss: 1.3617\n",
      "Epoch [7125/10000], Loss: 1.3616\n",
      "Epoch [7126/10000], Loss: 1.3615\n",
      "Epoch [7127/10000], Loss: 1.3614\n",
      "Epoch [7128/10000], Loss: 1.3612\n",
      "Epoch [7129/10000], Loss: 1.3611\n",
      "Epoch [7130/10000], Loss: 1.3610\n",
      "Epoch [7131/10000], Loss: 1.3608\n",
      "Epoch [7132/10000], Loss: 1.3607\n",
      "Epoch [7133/10000], Loss: 1.3606\n",
      "Epoch [7134/10000], Loss: 1.3604\n",
      "Epoch [7135/10000], Loss: 1.3603\n",
      "Epoch [7136/10000], Loss: 1.3602\n",
      "Epoch [7137/10000], Loss: 1.3601\n",
      "Epoch [7138/10000], Loss: 1.3599\n",
      "Epoch [7139/10000], Loss: 1.3598\n",
      "Epoch [7140/10000], Loss: 1.3597\n",
      "Epoch [7141/10000], Loss: 1.3595\n",
      "Epoch [7142/10000], Loss: 1.3594\n",
      "Epoch [7143/10000], Loss: 1.3593\n",
      "Epoch [7144/10000], Loss: 1.3592\n",
      "Epoch [7145/10000], Loss: 1.3590\n",
      "Epoch [7146/10000], Loss: 1.3589\n",
      "Epoch [7147/10000], Loss: 1.3588\n",
      "Epoch [7148/10000], Loss: 1.3586\n",
      "Epoch [7149/10000], Loss: 1.3585\n",
      "Epoch [7150/10000], Loss: 1.3584\n",
      "Epoch [7151/10000], Loss: 1.3582\n",
      "Epoch [7152/10000], Loss: 1.3581\n",
      "Epoch [7153/10000], Loss: 1.3580\n",
      "Epoch [7154/10000], Loss: 1.3579\n",
      "Epoch [7155/10000], Loss: 1.3577\n",
      "Epoch [7156/10000], Loss: 1.3576\n",
      "Epoch [7157/10000], Loss: 1.3575\n",
      "Epoch [7158/10000], Loss: 1.3573\n",
      "Epoch [7159/10000], Loss: 1.3572\n",
      "Epoch [7160/10000], Loss: 1.3571\n",
      "Epoch [7161/10000], Loss: 1.3569\n",
      "Epoch [7162/10000], Loss: 1.3568\n",
      "Epoch [7163/10000], Loss: 1.3567\n",
      "Epoch [7164/10000], Loss: 1.3566\n",
      "Epoch [7165/10000], Loss: 1.3564\n",
      "Epoch [7166/10000], Loss: 1.3563\n",
      "Epoch [7167/10000], Loss: 1.3562\n",
      "Epoch [7168/10000], Loss: 1.3560\n",
      "Epoch [7169/10000], Loss: 1.3559\n",
      "Epoch [7170/10000], Loss: 1.3558\n",
      "Epoch [7171/10000], Loss: 1.3557\n",
      "Epoch [7172/10000], Loss: 1.3555\n",
      "Epoch [7173/10000], Loss: 1.3554\n",
      "Epoch [7174/10000], Loss: 1.3553\n",
      "Epoch [7175/10000], Loss: 1.3551\n",
      "Epoch [7176/10000], Loss: 1.3550\n",
      "Epoch [7177/10000], Loss: 1.3549\n",
      "Epoch [7178/10000], Loss: 1.3547\n",
      "Epoch [7179/10000], Loss: 1.3546\n",
      "Epoch [7180/10000], Loss: 1.3545\n",
      "Epoch [7181/10000], Loss: 1.3544\n",
      "Epoch [7182/10000], Loss: 1.3542\n",
      "Epoch [7183/10000], Loss: 1.3541\n",
      "Epoch [7184/10000], Loss: 1.3540\n",
      "Epoch [7185/10000], Loss: 1.3538\n",
      "Epoch [7186/10000], Loss: 1.3537\n",
      "Epoch [7187/10000], Loss: 1.3536\n",
      "Epoch [7188/10000], Loss: 1.3535\n",
      "Epoch [7189/10000], Loss: 1.3533\n",
      "Epoch [7190/10000], Loss: 1.3532\n",
      "Epoch [7191/10000], Loss: 1.3531\n",
      "Epoch [7192/10000], Loss: 1.3529\n",
      "Epoch [7193/10000], Loss: 1.3528\n",
      "Epoch [7194/10000], Loss: 1.3527\n",
      "Epoch [7195/10000], Loss: 1.3525\n",
      "Epoch [7196/10000], Loss: 1.3524\n",
      "Epoch [7197/10000], Loss: 1.3523\n",
      "Epoch [7198/10000], Loss: 1.3522\n",
      "Epoch [7199/10000], Loss: 1.3520\n",
      "Epoch [7200/10000], Loss: 1.3519\n",
      "Epoch [7201/10000], Loss: 1.3518\n",
      "Epoch [7202/10000], Loss: 1.3516\n",
      "Epoch [7203/10000], Loss: 1.3515\n",
      "Epoch [7204/10000], Loss: 1.3514\n",
      "Epoch [7205/10000], Loss: 1.3512\n",
      "Epoch [7206/10000], Loss: 1.3511\n",
      "Epoch [7207/10000], Loss: 1.3510\n",
      "Epoch [7208/10000], Loss: 1.3509\n",
      "Epoch [7209/10000], Loss: 1.3507\n",
      "Epoch [7210/10000], Loss: 1.3506\n",
      "Epoch [7211/10000], Loss: 1.3505\n",
      "Epoch [7212/10000], Loss: 1.3503\n",
      "Epoch [7213/10000], Loss: 1.3502\n",
      "Epoch [7214/10000], Loss: 1.3501\n",
      "Epoch [7215/10000], Loss: 1.3500\n",
      "Epoch [7216/10000], Loss: 1.3498\n",
      "Epoch [7217/10000], Loss: 1.3497\n",
      "Epoch [7218/10000], Loss: 1.3496\n",
      "Epoch [7219/10000], Loss: 1.3494\n",
      "Epoch [7220/10000], Loss: 1.3493\n",
      "Epoch [7221/10000], Loss: 1.3492\n",
      "Epoch [7222/10000], Loss: 1.3490\n",
      "Epoch [7223/10000], Loss: 1.3489\n",
      "Epoch [7224/10000], Loss: 1.3488\n",
      "Epoch [7225/10000], Loss: 1.3487\n",
      "Epoch [7226/10000], Loss: 1.3485\n",
      "Epoch [7227/10000], Loss: 1.3484\n",
      "Epoch [7228/10000], Loss: 1.3483\n",
      "Epoch [7229/10000], Loss: 1.3481\n",
      "Epoch [7230/10000], Loss: 1.3480\n",
      "Epoch [7231/10000], Loss: 1.3479\n",
      "Epoch [7232/10000], Loss: 1.3478\n",
      "Epoch [7233/10000], Loss: 1.3476\n",
      "Epoch [7234/10000], Loss: 1.3475\n",
      "Epoch [7235/10000], Loss: 1.3474\n",
      "Epoch [7236/10000], Loss: 1.3472\n",
      "Epoch [7237/10000], Loss: 1.3471\n",
      "Epoch [7238/10000], Loss: 1.3470\n",
      "Epoch [7239/10000], Loss: 1.3468\n",
      "Epoch [7240/10000], Loss: 1.3467\n",
      "Epoch [7241/10000], Loss: 1.3466\n",
      "Epoch [7242/10000], Loss: 1.3465\n",
      "Epoch [7243/10000], Loss: 1.3463\n",
      "Epoch [7244/10000], Loss: 1.3462\n",
      "Epoch [7245/10000], Loss: 1.3461\n",
      "Epoch [7246/10000], Loss: 1.3459\n",
      "Epoch [7247/10000], Loss: 1.3458\n",
      "Epoch [7248/10000], Loss: 1.3457\n",
      "Epoch [7249/10000], Loss: 1.3456\n",
      "Epoch [7250/10000], Loss: 1.3454\n",
      "Epoch [7251/10000], Loss: 1.3453\n",
      "Epoch [7252/10000], Loss: 1.3452\n",
      "Epoch [7253/10000], Loss: 1.3450\n",
      "Epoch [7254/10000], Loss: 1.3449\n",
      "Epoch [7255/10000], Loss: 1.3448\n",
      "Epoch [7256/10000], Loss: 1.3447\n",
      "Epoch [7257/10000], Loss: 1.3445\n",
      "Epoch [7258/10000], Loss: 1.3444\n",
      "Epoch [7259/10000], Loss: 1.3443\n",
      "Epoch [7260/10000], Loss: 1.3441\n",
      "Epoch [7261/10000], Loss: 1.3440\n",
      "Epoch [7262/10000], Loss: 1.3439\n",
      "Epoch [7263/10000], Loss: 1.3437\n",
      "Epoch [7264/10000], Loss: 1.3436\n",
      "Epoch [7265/10000], Loss: 1.3435\n",
      "Epoch [7266/10000], Loss: 1.3434\n",
      "Epoch [7267/10000], Loss: 1.3432\n",
      "Epoch [7268/10000], Loss: 1.3431\n",
      "Epoch [7269/10000], Loss: 1.3430\n",
      "Epoch [7270/10000], Loss: 1.3428\n",
      "Epoch [7271/10000], Loss: 1.3427\n",
      "Epoch [7272/10000], Loss: 1.3426\n",
      "Epoch [7273/10000], Loss: 1.3425\n",
      "Epoch [7274/10000], Loss: 1.3423\n",
      "Epoch [7275/10000], Loss: 1.3422\n",
      "Epoch [7276/10000], Loss: 1.3421\n",
      "Epoch [7277/10000], Loss: 1.3419\n",
      "Epoch [7278/10000], Loss: 1.3418\n",
      "Epoch [7279/10000], Loss: 1.3417\n",
      "Epoch [7280/10000], Loss: 1.3415\n",
      "Epoch [7281/10000], Loss: 1.3414\n",
      "Epoch [7282/10000], Loss: 1.3413\n",
      "Epoch [7283/10000], Loss: 1.3412\n",
      "Epoch [7284/10000], Loss: 1.3410\n",
      "Epoch [7285/10000], Loss: 1.3409\n",
      "Epoch [7286/10000], Loss: 1.3408\n",
      "Epoch [7287/10000], Loss: 1.3406\n",
      "Epoch [7288/10000], Loss: 1.3405\n",
      "Epoch [7289/10000], Loss: 1.3404\n",
      "Epoch [7290/10000], Loss: 1.3403\n",
      "Epoch [7291/10000], Loss: 1.3401\n",
      "Epoch [7292/10000], Loss: 1.3400\n",
      "Epoch [7293/10000], Loss: 1.3399\n",
      "Epoch [7294/10000], Loss: 1.3397\n",
      "Epoch [7295/10000], Loss: 1.3396\n",
      "Epoch [7296/10000], Loss: 1.3395\n",
      "Epoch [7297/10000], Loss: 1.3394\n",
      "Epoch [7298/10000], Loss: 1.3392\n",
      "Epoch [7299/10000], Loss: 1.3391\n",
      "Epoch [7300/10000], Loss: 1.3390\n",
      "Epoch [7301/10000], Loss: 1.3388\n",
      "Epoch [7302/10000], Loss: 1.3387\n",
      "Epoch [7303/10000], Loss: 1.3386\n",
      "Epoch [7304/10000], Loss: 1.3384\n",
      "Epoch [7305/10000], Loss: 1.3383\n",
      "Epoch [7306/10000], Loss: 1.3382\n",
      "Epoch [7307/10000], Loss: 1.3381\n",
      "Epoch [7308/10000], Loss: 1.3379\n",
      "Epoch [7309/10000], Loss: 1.3378\n",
      "Epoch [7310/10000], Loss: 1.3377\n",
      "Epoch [7311/10000], Loss: 1.3375\n",
      "Epoch [7312/10000], Loss: 1.3374\n",
      "Epoch [7313/10000], Loss: 1.3373\n",
      "Epoch [7314/10000], Loss: 1.3372\n",
      "Epoch [7315/10000], Loss: 1.3370\n",
      "Epoch [7316/10000], Loss: 1.3369\n",
      "Epoch [7317/10000], Loss: 1.3368\n",
      "Epoch [7318/10000], Loss: 1.3366\n",
      "Epoch [7319/10000], Loss: 1.3365\n",
      "Epoch [7320/10000], Loss: 1.3364\n",
      "Epoch [7321/10000], Loss: 1.3362\n",
      "Epoch [7322/10000], Loss: 1.3361\n",
      "Epoch [7323/10000], Loss: 1.3360\n",
      "Epoch [7324/10000], Loss: 1.3359\n",
      "Epoch [7325/10000], Loss: 1.3357\n",
      "Epoch [7326/10000], Loss: 1.3356\n",
      "Epoch [7327/10000], Loss: 1.3355\n",
      "Epoch [7328/10000], Loss: 1.3353\n",
      "Epoch [7329/10000], Loss: 1.3352\n",
      "Epoch [7330/10000], Loss: 1.3351\n",
      "Epoch [7331/10000], Loss: 1.3350\n",
      "Epoch [7332/10000], Loss: 1.3348\n",
      "Epoch [7333/10000], Loss: 1.3347\n",
      "Epoch [7334/10000], Loss: 1.3346\n",
      "Epoch [7335/10000], Loss: 1.3344\n",
      "Epoch [7336/10000], Loss: 1.3343\n",
      "Epoch [7337/10000], Loss: 1.3342\n",
      "Epoch [7338/10000], Loss: 1.3341\n",
      "Epoch [7339/10000], Loss: 1.3339\n",
      "Epoch [7340/10000], Loss: 1.3338\n",
      "Epoch [7341/10000], Loss: 1.3337\n",
      "Epoch [7342/10000], Loss: 1.3335\n",
      "Epoch [7343/10000], Loss: 1.3334\n",
      "Epoch [7344/10000], Loss: 1.3333\n",
      "Epoch [7345/10000], Loss: 1.3331\n",
      "Epoch [7346/10000], Loss: 1.3330\n",
      "Epoch [7347/10000], Loss: 1.3329\n",
      "Epoch [7348/10000], Loss: 1.3328\n",
      "Epoch [7349/10000], Loss: 1.3326\n",
      "Epoch [7350/10000], Loss: 1.3325\n",
      "Epoch [7351/10000], Loss: 1.3324\n",
      "Epoch [7352/10000], Loss: 1.3322\n",
      "Epoch [7353/10000], Loss: 1.3321\n",
      "Epoch [7354/10000], Loss: 1.3320\n",
      "Epoch [7355/10000], Loss: 1.3319\n",
      "Epoch [7356/10000], Loss: 1.3317\n",
      "Epoch [7357/10000], Loss: 1.3316\n",
      "Epoch [7358/10000], Loss: 1.3315\n",
      "Epoch [7359/10000], Loss: 1.3313\n",
      "Epoch [7360/10000], Loss: 1.3312\n",
      "Epoch [7361/10000], Loss: 1.3311\n",
      "Epoch [7362/10000], Loss: 1.3310\n",
      "Epoch [7363/10000], Loss: 1.3308\n",
      "Epoch [7364/10000], Loss: 1.3307\n",
      "Epoch [7365/10000], Loss: 1.3306\n",
      "Epoch [7366/10000], Loss: 1.3304\n",
      "Epoch [7367/10000], Loss: 1.3303\n",
      "Epoch [7368/10000], Loss: 1.3302\n",
      "Epoch [7369/10000], Loss: 1.3300\n",
      "Epoch [7370/10000], Loss: 1.3299\n",
      "Epoch [7371/10000], Loss: 1.3298\n",
      "Epoch [7372/10000], Loss: 1.3297\n",
      "Epoch [7373/10000], Loss: 1.3295\n",
      "Epoch [7374/10000], Loss: 1.3294\n",
      "Epoch [7375/10000], Loss: 1.3293\n",
      "Epoch [7376/10000], Loss: 1.3291\n",
      "Epoch [7377/10000], Loss: 1.3290\n",
      "Epoch [7378/10000], Loss: 1.3289\n",
      "Epoch [7379/10000], Loss: 1.3288\n",
      "Epoch [7380/10000], Loss: 1.3286\n",
      "Epoch [7381/10000], Loss: 1.3285\n",
      "Epoch [7382/10000], Loss: 1.3284\n",
      "Epoch [7383/10000], Loss: 1.3282\n",
      "Epoch [7384/10000], Loss: 1.3281\n",
      "Epoch [7385/10000], Loss: 1.3280\n",
      "Epoch [7386/10000], Loss: 1.3279\n",
      "Epoch [7387/10000], Loss: 1.3277\n",
      "Epoch [7388/10000], Loss: 1.3276\n",
      "Epoch [7389/10000], Loss: 1.3275\n",
      "Epoch [7390/10000], Loss: 1.3273\n",
      "Epoch [7391/10000], Loss: 1.3272\n",
      "Epoch [7392/10000], Loss: 1.3271\n",
      "Epoch [7393/10000], Loss: 1.3270\n",
      "Epoch [7394/10000], Loss: 1.3268\n",
      "Epoch [7395/10000], Loss: 1.3267\n",
      "Epoch [7396/10000], Loss: 1.3266\n",
      "Epoch [7397/10000], Loss: 1.3264\n",
      "Epoch [7398/10000], Loss: 1.3263\n",
      "Epoch [7399/10000], Loss: 1.3262\n",
      "Epoch [7400/10000], Loss: 1.3260\n",
      "Epoch [7401/10000], Loss: 1.3259\n",
      "Epoch [7402/10000], Loss: 1.3258\n",
      "Epoch [7403/10000], Loss: 1.3257\n",
      "Epoch [7404/10000], Loss: 1.3255\n",
      "Epoch [7405/10000], Loss: 1.3254\n",
      "Epoch [7406/10000], Loss: 1.3253\n",
      "Epoch [7407/10000], Loss: 1.3251\n",
      "Epoch [7408/10000], Loss: 1.3250\n",
      "Epoch [7409/10000], Loss: 1.3249\n",
      "Epoch [7410/10000], Loss: 1.3248\n",
      "Epoch [7411/10000], Loss: 1.3246\n",
      "Epoch [7412/10000], Loss: 1.3245\n",
      "Epoch [7413/10000], Loss: 1.3244\n",
      "Epoch [7414/10000], Loss: 1.3242\n",
      "Epoch [7415/10000], Loss: 1.3241\n",
      "Epoch [7416/10000], Loss: 1.3240\n",
      "Epoch [7417/10000], Loss: 1.3239\n",
      "Epoch [7418/10000], Loss: 1.3237\n",
      "Epoch [7419/10000], Loss: 1.3236\n",
      "Epoch [7420/10000], Loss: 1.3235\n",
      "Epoch [7421/10000], Loss: 1.3233\n",
      "Epoch [7422/10000], Loss: 1.3232\n",
      "Epoch [7423/10000], Loss: 1.3231\n",
      "Epoch [7424/10000], Loss: 1.3230\n",
      "Epoch [7425/10000], Loss: 1.3228\n",
      "Epoch [7426/10000], Loss: 1.3227\n",
      "Epoch [7427/10000], Loss: 1.3226\n",
      "Epoch [7428/10000], Loss: 1.3224\n",
      "Epoch [7429/10000], Loss: 1.3223\n",
      "Epoch [7430/10000], Loss: 1.3222\n",
      "Epoch [7431/10000], Loss: 1.3221\n",
      "Epoch [7432/10000], Loss: 1.3219\n",
      "Epoch [7433/10000], Loss: 1.3218\n",
      "Epoch [7434/10000], Loss: 1.3217\n",
      "Epoch [7435/10000], Loss: 1.3215\n",
      "Epoch [7436/10000], Loss: 1.3214\n",
      "Epoch [7437/10000], Loss: 1.3213\n",
      "Epoch [7438/10000], Loss: 1.3211\n",
      "Epoch [7439/10000], Loss: 1.3210\n",
      "Epoch [7440/10000], Loss: 1.3209\n",
      "Epoch [7441/10000], Loss: 1.3208\n",
      "Epoch [7442/10000], Loss: 1.3206\n",
      "Epoch [7443/10000], Loss: 1.3205\n",
      "Epoch [7444/10000], Loss: 1.3204\n",
      "Epoch [7445/10000], Loss: 1.3202\n",
      "Epoch [7446/10000], Loss: 1.3201\n",
      "Epoch [7447/10000], Loss: 1.3200\n",
      "Epoch [7448/10000], Loss: 1.3199\n",
      "Epoch [7449/10000], Loss: 1.3197\n",
      "Epoch [7450/10000], Loss: 1.3196\n",
      "Epoch [7451/10000], Loss: 1.3195\n",
      "Epoch [7452/10000], Loss: 1.3193\n",
      "Epoch [7453/10000], Loss: 1.3192\n",
      "Epoch [7454/10000], Loss: 1.3191\n",
      "Epoch [7455/10000], Loss: 1.3190\n",
      "Epoch [7456/10000], Loss: 1.3188\n",
      "Epoch [7457/10000], Loss: 1.3187\n",
      "Epoch [7458/10000], Loss: 1.3186\n",
      "Epoch [7459/10000], Loss: 1.3184\n",
      "Epoch [7460/10000], Loss: 1.3183\n",
      "Epoch [7461/10000], Loss: 1.3182\n",
      "Epoch [7462/10000], Loss: 1.3181\n",
      "Epoch [7463/10000], Loss: 1.3179\n",
      "Epoch [7464/10000], Loss: 1.3178\n",
      "Epoch [7465/10000], Loss: 1.3177\n",
      "Epoch [7466/10000], Loss: 1.3175\n",
      "Epoch [7467/10000], Loss: 1.3174\n",
      "Epoch [7468/10000], Loss: 1.3173\n",
      "Epoch [7469/10000], Loss: 1.3172\n",
      "Epoch [7470/10000], Loss: 1.3170\n",
      "Epoch [7471/10000], Loss: 1.3169\n",
      "Epoch [7472/10000], Loss: 1.3168\n",
      "Epoch [7473/10000], Loss: 1.3166\n",
      "Epoch [7474/10000], Loss: 1.3165\n",
      "Epoch [7475/10000], Loss: 1.3164\n",
      "Epoch [7476/10000], Loss: 1.3162\n",
      "Epoch [7477/10000], Loss: 1.3161\n",
      "Epoch [7478/10000], Loss: 1.3160\n",
      "Epoch [7479/10000], Loss: 1.3159\n",
      "Epoch [7480/10000], Loss: 1.3157\n",
      "Epoch [7481/10000], Loss: 1.3156\n",
      "Epoch [7482/10000], Loss: 1.3155\n",
      "Epoch [7483/10000], Loss: 1.3153\n",
      "Epoch [7484/10000], Loss: 1.3152\n",
      "Epoch [7485/10000], Loss: 1.3151\n",
      "Epoch [7486/10000], Loss: 1.3150\n",
      "Epoch [7487/10000], Loss: 1.3148\n",
      "Epoch [7488/10000], Loss: 1.3147\n",
      "Epoch [7489/10000], Loss: 1.3146\n",
      "Epoch [7490/10000], Loss: 1.3144\n",
      "Epoch [7491/10000], Loss: 1.3143\n",
      "Epoch [7492/10000], Loss: 1.3142\n",
      "Epoch [7493/10000], Loss: 1.3141\n",
      "Epoch [7494/10000], Loss: 1.3139\n",
      "Epoch [7495/10000], Loss: 1.3138\n",
      "Epoch [7496/10000], Loss: 1.3137\n",
      "Epoch [7497/10000], Loss: 1.3135\n",
      "Epoch [7498/10000], Loss: 1.3134\n",
      "Epoch [7499/10000], Loss: 1.3133\n",
      "Epoch [7500/10000], Loss: 1.3132\n",
      "Epoch [7501/10000], Loss: 1.3130\n",
      "Epoch [7502/10000], Loss: 1.3129\n",
      "Epoch [7503/10000], Loss: 1.3128\n",
      "Epoch [7504/10000], Loss: 1.3126\n",
      "Epoch [7505/10000], Loss: 1.3125\n",
      "Epoch [7506/10000], Loss: 1.3124\n",
      "Epoch [7507/10000], Loss: 1.3123\n",
      "Epoch [7508/10000], Loss: 1.3121\n",
      "Epoch [7509/10000], Loss: 1.3120\n",
      "Epoch [7510/10000], Loss: 1.3119\n",
      "Epoch [7511/10000], Loss: 1.3117\n",
      "Epoch [7512/10000], Loss: 1.3116\n",
      "Epoch [7513/10000], Loss: 1.3115\n",
      "Epoch [7514/10000], Loss: 1.3114\n",
      "Epoch [7515/10000], Loss: 1.3112\n",
      "Epoch [7516/10000], Loss: 1.3111\n",
      "Epoch [7517/10000], Loss: 1.3110\n",
      "Epoch [7518/10000], Loss: 1.3108\n",
      "Epoch [7519/10000], Loss: 1.3107\n",
      "Epoch [7520/10000], Loss: 1.3106\n",
      "Epoch [7521/10000], Loss: 1.3105\n",
      "Epoch [7522/10000], Loss: 1.3103\n",
      "Epoch [7523/10000], Loss: 1.3102\n",
      "Epoch [7524/10000], Loss: 1.3101\n",
      "Epoch [7525/10000], Loss: 1.3099\n",
      "Epoch [7526/10000], Loss: 1.3098\n",
      "Epoch [7527/10000], Loss: 1.3097\n",
      "Epoch [7528/10000], Loss: 1.3096\n",
      "Epoch [7529/10000], Loss: 1.3094\n",
      "Epoch [7530/10000], Loss: 1.3093\n",
      "Epoch [7531/10000], Loss: 1.3092\n",
      "Epoch [7532/10000], Loss: 1.3090\n",
      "Epoch [7533/10000], Loss: 1.3089\n",
      "Epoch [7534/10000], Loss: 1.3088\n",
      "Epoch [7535/10000], Loss: 1.3086\n",
      "Epoch [7536/10000], Loss: 1.3085\n",
      "Epoch [7537/10000], Loss: 1.3084\n",
      "Epoch [7538/10000], Loss: 1.3083\n",
      "Epoch [7539/10000], Loss: 1.3081\n",
      "Epoch [7540/10000], Loss: 1.3080\n",
      "Epoch [7541/10000], Loss: 1.3079\n",
      "Epoch [7542/10000], Loss: 1.3077\n",
      "Epoch [7543/10000], Loss: 1.3076\n",
      "Epoch [7544/10000], Loss: 1.3075\n",
      "Epoch [7545/10000], Loss: 1.3074\n",
      "Epoch [7546/10000], Loss: 1.3072\n",
      "Epoch [7547/10000], Loss: 1.3071\n",
      "Epoch [7548/10000], Loss: 1.3070\n",
      "Epoch [7549/10000], Loss: 1.3068\n",
      "Epoch [7550/10000], Loss: 1.3067\n",
      "Epoch [7551/10000], Loss: 1.3066\n",
      "Epoch [7552/10000], Loss: 1.3065\n",
      "Epoch [7553/10000], Loss: 1.3063\n",
      "Epoch [7554/10000], Loss: 1.3062\n",
      "Epoch [7555/10000], Loss: 1.3061\n",
      "Epoch [7556/10000], Loss: 1.3059\n",
      "Epoch [7557/10000], Loss: 1.3058\n",
      "Epoch [7558/10000], Loss: 1.3057\n",
      "Epoch [7559/10000], Loss: 1.3056\n",
      "Epoch [7560/10000], Loss: 1.3054\n",
      "Epoch [7561/10000], Loss: 1.3053\n",
      "Epoch [7562/10000], Loss: 1.3052\n",
      "Epoch [7563/10000], Loss: 1.3050\n",
      "Epoch [7564/10000], Loss: 1.3049\n",
      "Epoch [7565/10000], Loss: 1.3048\n",
      "Epoch [7566/10000], Loss: 1.3047\n",
      "Epoch [7567/10000], Loss: 1.3045\n",
      "Epoch [7568/10000], Loss: 1.3044\n",
      "Epoch [7569/10000], Loss: 1.3043\n",
      "Epoch [7570/10000], Loss: 1.3041\n",
      "Epoch [7571/10000], Loss: 1.3040\n",
      "Epoch [7572/10000], Loss: 1.3039\n",
      "Epoch [7573/10000], Loss: 1.3038\n",
      "Epoch [7574/10000], Loss: 1.3036\n",
      "Epoch [7575/10000], Loss: 1.3035\n",
      "Epoch [7576/10000], Loss: 1.3034\n",
      "Epoch [7577/10000], Loss: 1.3032\n",
      "Epoch [7578/10000], Loss: 1.3031\n",
      "Epoch [7579/10000], Loss: 1.3030\n",
      "Epoch [7580/10000], Loss: 1.3029\n",
      "Epoch [7581/10000], Loss: 1.3027\n",
      "Epoch [7582/10000], Loss: 1.3026\n",
      "Epoch [7583/10000], Loss: 1.3025\n",
      "Epoch [7584/10000], Loss: 1.3023\n",
      "Epoch [7585/10000], Loss: 1.3022\n",
      "Epoch [7586/10000], Loss: 1.3021\n",
      "Epoch [7587/10000], Loss: 1.3020\n",
      "Epoch [7588/10000], Loss: 1.3018\n",
      "Epoch [7589/10000], Loss: 1.3017\n",
      "Epoch [7590/10000], Loss: 1.3016\n",
      "Epoch [7591/10000], Loss: 1.3014\n",
      "Epoch [7592/10000], Loss: 1.3013\n",
      "Epoch [7593/10000], Loss: 1.3012\n",
      "Epoch [7594/10000], Loss: 1.3011\n",
      "Epoch [7595/10000], Loss: 1.3009\n",
      "Epoch [7596/10000], Loss: 1.3008\n",
      "Epoch [7597/10000], Loss: 1.3007\n",
      "Epoch [7598/10000], Loss: 1.3005\n",
      "Epoch [7599/10000], Loss: 1.3004\n",
      "Epoch [7600/10000], Loss: 1.3003\n",
      "Epoch [7601/10000], Loss: 1.3002\n",
      "Epoch [7602/10000], Loss: 1.3000\n",
      "Epoch [7603/10000], Loss: 1.2999\n",
      "Epoch [7604/10000], Loss: 1.2998\n",
      "Epoch [7605/10000], Loss: 1.2996\n",
      "Epoch [7606/10000], Loss: 1.2995\n",
      "Epoch [7607/10000], Loss: 1.2994\n",
      "Epoch [7608/10000], Loss: 1.2993\n",
      "Epoch [7609/10000], Loss: 1.2991\n",
      "Epoch [7610/10000], Loss: 1.2990\n",
      "Epoch [7611/10000], Loss: 1.2989\n",
      "Epoch [7612/10000], Loss: 1.2987\n",
      "Epoch [7613/10000], Loss: 1.2986\n",
      "Epoch [7614/10000], Loss: 1.2985\n",
      "Epoch [7615/10000], Loss: 1.2984\n",
      "Epoch [7616/10000], Loss: 1.2982\n",
      "Epoch [7617/10000], Loss: 1.2981\n",
      "Epoch [7618/10000], Loss: 1.2980\n",
      "Epoch [7619/10000], Loss: 1.2978\n",
      "Epoch [7620/10000], Loss: 1.2977\n",
      "Epoch [7621/10000], Loss: 1.2976\n",
      "Epoch [7622/10000], Loss: 1.2975\n",
      "Epoch [7623/10000], Loss: 1.2973\n",
      "Epoch [7624/10000], Loss: 1.2972\n",
      "Epoch [7625/10000], Loss: 1.2971\n",
      "Epoch [7626/10000], Loss: 1.2969\n",
      "Epoch [7627/10000], Loss: 1.2968\n",
      "Epoch [7628/10000], Loss: 1.2967\n",
      "Epoch [7629/10000], Loss: 1.2966\n",
      "Epoch [7630/10000], Loss: 1.2964\n",
      "Epoch [7631/10000], Loss: 1.2963\n",
      "Epoch [7632/10000], Loss: 1.2962\n",
      "Epoch [7633/10000], Loss: 1.2960\n",
      "Epoch [7634/10000], Loss: 1.2959\n",
      "Epoch [7635/10000], Loss: 1.2958\n",
      "Epoch [7636/10000], Loss: 1.2957\n",
      "Epoch [7637/10000], Loss: 1.2955\n",
      "Epoch [7638/10000], Loss: 1.2954\n",
      "Epoch [7639/10000], Loss: 1.2953\n",
      "Epoch [7640/10000], Loss: 1.2951\n",
      "Epoch [7641/10000], Loss: 1.2950\n",
      "Epoch [7642/10000], Loss: 1.2949\n",
      "Epoch [7643/10000], Loss: 1.2948\n",
      "Epoch [7644/10000], Loss: 1.2946\n",
      "Epoch [7645/10000], Loss: 1.2945\n",
      "Epoch [7646/10000], Loss: 1.2944\n",
      "Epoch [7647/10000], Loss: 1.2942\n",
      "Epoch [7648/10000], Loss: 1.2941\n",
      "Epoch [7649/10000], Loss: 1.2940\n",
      "Epoch [7650/10000], Loss: 1.2939\n",
      "Epoch [7651/10000], Loss: 1.2937\n",
      "Epoch [7652/10000], Loss: 1.2936\n",
      "Epoch [7653/10000], Loss: 1.2935\n",
      "Epoch [7654/10000], Loss: 1.2933\n",
      "Epoch [7655/10000], Loss: 1.2932\n",
      "Epoch [7656/10000], Loss: 1.2931\n",
      "Epoch [7657/10000], Loss: 1.2930\n",
      "Epoch [7658/10000], Loss: 1.2928\n",
      "Epoch [7659/10000], Loss: 1.2927\n",
      "Epoch [7660/10000], Loss: 1.2926\n",
      "Epoch [7661/10000], Loss: 1.2924\n",
      "Epoch [7662/10000], Loss: 1.2923\n",
      "Epoch [7663/10000], Loss: 1.2922\n",
      "Epoch [7664/10000], Loss: 1.2921\n",
      "Epoch [7665/10000], Loss: 1.2919\n",
      "Epoch [7666/10000], Loss: 1.2918\n",
      "Epoch [7667/10000], Loss: 1.2917\n",
      "Epoch [7668/10000], Loss: 1.2915\n",
      "Epoch [7669/10000], Loss: 1.2914\n",
      "Epoch [7670/10000], Loss: 1.2913\n",
      "Epoch [7671/10000], Loss: 1.2912\n",
      "Epoch [7672/10000], Loss: 1.2910\n",
      "Epoch [7673/10000], Loss: 1.2909\n",
      "Epoch [7674/10000], Loss: 1.2908\n",
      "Epoch [7675/10000], Loss: 1.2906\n",
      "Epoch [7676/10000], Loss: 1.2905\n",
      "Epoch [7677/10000], Loss: 1.2904\n",
      "Epoch [7678/10000], Loss: 1.2903\n",
      "Epoch [7679/10000], Loss: 1.2901\n",
      "Epoch [7680/10000], Loss: 1.2900\n",
      "Epoch [7681/10000], Loss: 1.2899\n",
      "Epoch [7682/10000], Loss: 1.2897\n",
      "Epoch [7683/10000], Loss: 1.2896\n",
      "Epoch [7684/10000], Loss: 1.2895\n",
      "Epoch [7685/10000], Loss: 1.2894\n",
      "Epoch [7686/10000], Loss: 1.2892\n",
      "Epoch [7687/10000], Loss: 1.2891\n",
      "Epoch [7688/10000], Loss: 1.2890\n",
      "Epoch [7689/10000], Loss: 1.2888\n",
      "Epoch [7690/10000], Loss: 1.2887\n",
      "Epoch [7691/10000], Loss: 1.2886\n",
      "Epoch [7692/10000], Loss: 1.2885\n",
      "Epoch [7693/10000], Loss: 1.2883\n",
      "Epoch [7694/10000], Loss: 1.2882\n",
      "Epoch [7695/10000], Loss: 1.2881\n",
      "Epoch [7696/10000], Loss: 1.2879\n",
      "Epoch [7697/10000], Loss: 1.2878\n",
      "Epoch [7698/10000], Loss: 1.2877\n",
      "Epoch [7699/10000], Loss: 1.2876\n",
      "Epoch [7700/10000], Loss: 1.2874\n",
      "Epoch [7701/10000], Loss: 1.2873\n",
      "Epoch [7702/10000], Loss: 1.2872\n",
      "Epoch [7703/10000], Loss: 1.2870\n",
      "Epoch [7704/10000], Loss: 1.2869\n",
      "Epoch [7705/10000], Loss: 1.2868\n",
      "Epoch [7706/10000], Loss: 1.2867\n",
      "Epoch [7707/10000], Loss: 1.2865\n",
      "Epoch [7708/10000], Loss: 1.2864\n",
      "Epoch [7709/10000], Loss: 1.2863\n",
      "Epoch [7710/10000], Loss: 1.2861\n",
      "Epoch [7711/10000], Loss: 1.2860\n",
      "Epoch [7712/10000], Loss: 1.2859\n",
      "Epoch [7713/10000], Loss: 1.2858\n",
      "Epoch [7714/10000], Loss: 1.2856\n",
      "Epoch [7715/10000], Loss: 1.2855\n",
      "Epoch [7716/10000], Loss: 1.2854\n",
      "Epoch [7717/10000], Loss: 1.2852\n",
      "Epoch [7718/10000], Loss: 1.2851\n",
      "Epoch [7719/10000], Loss: 1.2850\n",
      "Epoch [7720/10000], Loss: 1.2849\n",
      "Epoch [7721/10000], Loss: 1.2847\n",
      "Epoch [7722/10000], Loss: 1.2846\n",
      "Epoch [7723/10000], Loss: 1.2845\n",
      "Epoch [7724/10000], Loss: 1.2843\n",
      "Epoch [7725/10000], Loss: 1.2842\n",
      "Epoch [7726/10000], Loss: 1.2841\n",
      "Epoch [7727/10000], Loss: 1.2840\n",
      "Epoch [7728/10000], Loss: 1.2838\n",
      "Epoch [7729/10000], Loss: 1.2837\n",
      "Epoch [7730/10000], Loss: 1.2836\n",
      "Epoch [7731/10000], Loss: 1.2834\n",
      "Epoch [7732/10000], Loss: 1.2833\n",
      "Epoch [7733/10000], Loss: 1.2832\n",
      "Epoch [7734/10000], Loss: 1.2831\n",
      "Epoch [7735/10000], Loss: 1.2829\n",
      "Epoch [7736/10000], Loss: 1.2828\n",
      "Epoch [7737/10000], Loss: 1.2827\n",
      "Epoch [7738/10000], Loss: 1.2825\n",
      "Epoch [7739/10000], Loss: 1.2824\n",
      "Epoch [7740/10000], Loss: 1.2823\n",
      "Epoch [7741/10000], Loss: 1.2822\n",
      "Epoch [7742/10000], Loss: 1.2820\n",
      "Epoch [7743/10000], Loss: 1.2819\n",
      "Epoch [7744/10000], Loss: 1.2818\n",
      "Epoch [7745/10000], Loss: 1.2816\n",
      "Epoch [7746/10000], Loss: 1.2815\n",
      "Epoch [7747/10000], Loss: 1.2814\n",
      "Epoch [7748/10000], Loss: 1.2813\n",
      "Epoch [7749/10000], Loss: 1.2811\n",
      "Epoch [7750/10000], Loss: 1.2810\n",
      "Epoch [7751/10000], Loss: 1.2809\n",
      "Epoch [7752/10000], Loss: 1.2808\n",
      "Epoch [7753/10000], Loss: 1.2806\n",
      "Epoch [7754/10000], Loss: 1.2805\n",
      "Epoch [7755/10000], Loss: 1.2804\n",
      "Epoch [7756/10000], Loss: 1.2802\n",
      "Epoch [7757/10000], Loss: 1.2801\n",
      "Epoch [7758/10000], Loss: 1.2800\n",
      "Epoch [7759/10000], Loss: 1.2799\n",
      "Epoch [7760/10000], Loss: 1.2797\n",
      "Epoch [7761/10000], Loss: 1.2796\n",
      "Epoch [7762/10000], Loss: 1.2795\n",
      "Epoch [7763/10000], Loss: 1.2793\n",
      "Epoch [7764/10000], Loss: 1.2792\n",
      "Epoch [7765/10000], Loss: 1.2791\n",
      "Epoch [7766/10000], Loss: 1.2790\n",
      "Epoch [7767/10000], Loss: 1.2788\n",
      "Epoch [7768/10000], Loss: 1.2787\n",
      "Epoch [7769/10000], Loss: 1.2786\n",
      "Epoch [7770/10000], Loss: 1.2784\n",
      "Epoch [7771/10000], Loss: 1.2783\n",
      "Epoch [7772/10000], Loss: 1.2782\n",
      "Epoch [7773/10000], Loss: 1.2781\n",
      "Epoch [7774/10000], Loss: 1.2779\n",
      "Epoch [7775/10000], Loss: 1.2778\n",
      "Epoch [7776/10000], Loss: 1.2777\n",
      "Epoch [7777/10000], Loss: 1.2775\n",
      "Epoch [7778/10000], Loss: 1.2774\n",
      "Epoch [7779/10000], Loss: 1.2773\n",
      "Epoch [7780/10000], Loss: 1.2772\n",
      "Epoch [7781/10000], Loss: 1.2770\n",
      "Epoch [7782/10000], Loss: 1.2769\n",
      "Epoch [7783/10000], Loss: 1.2768\n",
      "Epoch [7784/10000], Loss: 1.2766\n",
      "Epoch [7785/10000], Loss: 1.2765\n",
      "Epoch [7786/10000], Loss: 1.2764\n",
      "Epoch [7787/10000], Loss: 1.2763\n",
      "Epoch [7788/10000], Loss: 1.2761\n",
      "Epoch [7789/10000], Loss: 1.2760\n",
      "Epoch [7790/10000], Loss: 1.2759\n",
      "Epoch [7791/10000], Loss: 1.2757\n",
      "Epoch [7792/10000], Loss: 1.2756\n",
      "Epoch [7793/10000], Loss: 1.2755\n",
      "Epoch [7794/10000], Loss: 1.2754\n",
      "Epoch [7795/10000], Loss: 1.2752\n",
      "Epoch [7796/10000], Loss: 1.2751\n",
      "Epoch [7797/10000], Loss: 1.2750\n",
      "Epoch [7798/10000], Loss: 1.2748\n",
      "Epoch [7799/10000], Loss: 1.2747\n",
      "Epoch [7800/10000], Loss: 1.2746\n",
      "Epoch [7801/10000], Loss: 1.2745\n",
      "Epoch [7802/10000], Loss: 1.2743\n",
      "Epoch [7803/10000], Loss: 1.2742\n",
      "Epoch [7804/10000], Loss: 1.2741\n",
      "Epoch [7805/10000], Loss: 1.2739\n",
      "Epoch [7806/10000], Loss: 1.2738\n",
      "Epoch [7807/10000], Loss: 1.2737\n",
      "Epoch [7808/10000], Loss: 1.2736\n",
      "Epoch [7809/10000], Loss: 1.2734\n",
      "Epoch [7810/10000], Loss: 1.2733\n",
      "Epoch [7811/10000], Loss: 1.2732\n",
      "Epoch [7812/10000], Loss: 1.2731\n",
      "Epoch [7813/10000], Loss: 1.2729\n",
      "Epoch [7814/10000], Loss: 1.2728\n",
      "Epoch [7815/10000], Loss: 1.2727\n",
      "Epoch [7816/10000], Loss: 1.2725\n",
      "Epoch [7817/10000], Loss: 1.2724\n",
      "Epoch [7818/10000], Loss: 1.2723\n",
      "Epoch [7819/10000], Loss: 1.2722\n",
      "Epoch [7820/10000], Loss: 1.2720\n",
      "Epoch [7821/10000], Loss: 1.2719\n",
      "Epoch [7822/10000], Loss: 1.2718\n",
      "Epoch [7823/10000], Loss: 1.2716\n",
      "Epoch [7824/10000], Loss: 1.2715\n",
      "Epoch [7825/10000], Loss: 1.2714\n",
      "Epoch [7826/10000], Loss: 1.2713\n",
      "Epoch [7827/10000], Loss: 1.2711\n",
      "Epoch [7828/10000], Loss: 1.2710\n",
      "Epoch [7829/10000], Loss: 1.2709\n",
      "Epoch [7830/10000], Loss: 1.2707\n",
      "Epoch [7831/10000], Loss: 1.2706\n",
      "Epoch [7832/10000], Loss: 1.2705\n",
      "Epoch [7833/10000], Loss: 1.2704\n",
      "Epoch [7834/10000], Loss: 1.2702\n",
      "Epoch [7835/10000], Loss: 1.2701\n",
      "Epoch [7836/10000], Loss: 1.2700\n",
      "Epoch [7837/10000], Loss: 1.2698\n",
      "Epoch [7838/10000], Loss: 1.2697\n",
      "Epoch [7839/10000], Loss: 1.2696\n",
      "Epoch [7840/10000], Loss: 1.2695\n",
      "Epoch [7841/10000], Loss: 1.2693\n",
      "Epoch [7842/10000], Loss: 1.2692\n",
      "Epoch [7843/10000], Loss: 1.2691\n",
      "Epoch [7844/10000], Loss: 1.2689\n",
      "Epoch [7845/10000], Loss: 1.2688\n",
      "Epoch [7846/10000], Loss: 1.2687\n",
      "Epoch [7847/10000], Loss: 1.2686\n",
      "Epoch [7848/10000], Loss: 1.2684\n",
      "Epoch [7849/10000], Loss: 1.2683\n",
      "Epoch [7850/10000], Loss: 1.2682\n",
      "Epoch [7851/10000], Loss: 1.2680\n",
      "Epoch [7852/10000], Loss: 1.2679\n",
      "Epoch [7853/10000], Loss: 1.2678\n",
      "Epoch [7854/10000], Loss: 1.2677\n",
      "Epoch [7855/10000], Loss: 1.2675\n",
      "Epoch [7856/10000], Loss: 1.2674\n",
      "Epoch [7857/10000], Loss: 1.2673\n",
      "Epoch [7858/10000], Loss: 1.2672\n",
      "Epoch [7859/10000], Loss: 1.2670\n",
      "Epoch [7860/10000], Loss: 1.2669\n",
      "Epoch [7861/10000], Loss: 1.2668\n",
      "Epoch [7862/10000], Loss: 1.2666\n",
      "Epoch [7863/10000], Loss: 1.2665\n",
      "Epoch [7864/10000], Loss: 1.2664\n",
      "Epoch [7865/10000], Loss: 1.2663\n",
      "Epoch [7866/10000], Loss: 1.2661\n",
      "Epoch [7867/10000], Loss: 1.2660\n",
      "Epoch [7868/10000], Loss: 1.2659\n",
      "Epoch [7869/10000], Loss: 1.2657\n",
      "Epoch [7870/10000], Loss: 1.2656\n",
      "Epoch [7871/10000], Loss: 1.2655\n",
      "Epoch [7872/10000], Loss: 1.2654\n",
      "Epoch [7873/10000], Loss: 1.2652\n",
      "Epoch [7874/10000], Loss: 1.2651\n",
      "Epoch [7875/10000], Loss: 1.2650\n",
      "Epoch [7876/10000], Loss: 1.2648\n",
      "Epoch [7877/10000], Loss: 1.2647\n",
      "Epoch [7878/10000], Loss: 1.2646\n",
      "Epoch [7879/10000], Loss: 1.2645\n",
      "Epoch [7880/10000], Loss: 1.2643\n",
      "Epoch [7881/10000], Loss: 1.2642\n",
      "Epoch [7882/10000], Loss: 1.2641\n",
      "Epoch [7883/10000], Loss: 1.2639\n",
      "Epoch [7884/10000], Loss: 1.2638\n",
      "Epoch [7885/10000], Loss: 1.2637\n",
      "Epoch [7886/10000], Loss: 1.2636\n",
      "Epoch [7887/10000], Loss: 1.2634\n",
      "Epoch [7888/10000], Loss: 1.2633\n",
      "Epoch [7889/10000], Loss: 1.2632\n",
      "Epoch [7890/10000], Loss: 1.2631\n",
      "Epoch [7891/10000], Loss: 1.2629\n",
      "Epoch [7892/10000], Loss: 1.2628\n",
      "Epoch [7893/10000], Loss: 1.2627\n",
      "Epoch [7894/10000], Loss: 1.2625\n",
      "Epoch [7895/10000], Loss: 1.2624\n",
      "Epoch [7896/10000], Loss: 1.2623\n",
      "Epoch [7897/10000], Loss: 1.2622\n",
      "Epoch [7898/10000], Loss: 1.2620\n",
      "Epoch [7899/10000], Loss: 1.2619\n",
      "Epoch [7900/10000], Loss: 1.2618\n",
      "Epoch [7901/10000], Loss: 1.2616\n",
      "Epoch [7902/10000], Loss: 1.2615\n",
      "Epoch [7903/10000], Loss: 1.2614\n",
      "Epoch [7904/10000], Loss: 1.2613\n",
      "Epoch [7905/10000], Loss: 1.2611\n",
      "Epoch [7906/10000], Loss: 1.2610\n",
      "Epoch [7907/10000], Loss: 1.2609\n",
      "Epoch [7908/10000], Loss: 1.2607\n",
      "Epoch [7909/10000], Loss: 1.2606\n",
      "Epoch [7910/10000], Loss: 1.2605\n",
      "Epoch [7911/10000], Loss: 1.2604\n",
      "Epoch [7912/10000], Loss: 1.2602\n",
      "Epoch [7913/10000], Loss: 1.2601\n",
      "Epoch [7914/10000], Loss: 1.2600\n",
      "Epoch [7915/10000], Loss: 1.2598\n",
      "Epoch [7916/10000], Loss: 1.2597\n",
      "Epoch [7917/10000], Loss: 1.2596\n",
      "Epoch [7918/10000], Loss: 1.2595\n",
      "Epoch [7919/10000], Loss: 1.2593\n",
      "Epoch [7920/10000], Loss: 1.2592\n",
      "Epoch [7921/10000], Loss: 1.2591\n",
      "Epoch [7922/10000], Loss: 1.2590\n",
      "Epoch [7923/10000], Loss: 1.2588\n",
      "Epoch [7924/10000], Loss: 1.2587\n",
      "Epoch [7925/10000], Loss: 1.2586\n",
      "Epoch [7926/10000], Loss: 1.2584\n",
      "Epoch [7927/10000], Loss: 1.2583\n",
      "Epoch [7928/10000], Loss: 1.2582\n",
      "Epoch [7929/10000], Loss: 1.2581\n",
      "Epoch [7930/10000], Loss: 1.2579\n",
      "Epoch [7931/10000], Loss: 1.2578\n",
      "Epoch [7932/10000], Loss: 1.2577\n",
      "Epoch [7933/10000], Loss: 1.2575\n",
      "Epoch [7934/10000], Loss: 1.2574\n",
      "Epoch [7935/10000], Loss: 1.2573\n",
      "Epoch [7936/10000], Loss: 1.2572\n",
      "Epoch [7937/10000], Loss: 1.2570\n",
      "Epoch [7938/10000], Loss: 1.2569\n",
      "Epoch [7939/10000], Loss: 1.2568\n",
      "Epoch [7940/10000], Loss: 1.2566\n",
      "Epoch [7941/10000], Loss: 1.2565\n",
      "Epoch [7942/10000], Loss: 1.2564\n",
      "Epoch [7943/10000], Loss: 1.2563\n",
      "Epoch [7944/10000], Loss: 1.2561\n",
      "Epoch [7945/10000], Loss: 1.2560\n",
      "Epoch [7946/10000], Loss: 1.2559\n",
      "Epoch [7947/10000], Loss: 1.2557\n",
      "Epoch [7948/10000], Loss: 1.2556\n",
      "Epoch [7949/10000], Loss: 1.2555\n",
      "Epoch [7950/10000], Loss: 1.2554\n",
      "Epoch [7951/10000], Loss: 1.2552\n",
      "Epoch [7952/10000], Loss: 1.2551\n",
      "Epoch [7953/10000], Loss: 1.2550\n",
      "Epoch [7954/10000], Loss: 1.2549\n",
      "Epoch [7955/10000], Loss: 1.2547\n",
      "Epoch [7956/10000], Loss: 1.2546\n",
      "Epoch [7957/10000], Loss: 1.2545\n",
      "Epoch [7958/10000], Loss: 1.2543\n",
      "Epoch [7959/10000], Loss: 1.2542\n",
      "Epoch [7960/10000], Loss: 1.2541\n",
      "Epoch [7961/10000], Loss: 1.2540\n",
      "Epoch [7962/10000], Loss: 1.2538\n",
      "Epoch [7963/10000], Loss: 1.2537\n",
      "Epoch [7964/10000], Loss: 1.2536\n",
      "Epoch [7965/10000], Loss: 1.2534\n",
      "Epoch [7966/10000], Loss: 1.2533\n",
      "Epoch [7967/10000], Loss: 1.2532\n",
      "Epoch [7968/10000], Loss: 1.2531\n",
      "Epoch [7969/10000], Loss: 1.2529\n",
      "Epoch [7970/10000], Loss: 1.2528\n",
      "Epoch [7971/10000], Loss: 1.2527\n",
      "Epoch [7972/10000], Loss: 1.2525\n",
      "Epoch [7973/10000], Loss: 1.2524\n",
      "Epoch [7974/10000], Loss: 1.2523\n",
      "Epoch [7975/10000], Loss: 1.2522\n",
      "Epoch [7976/10000], Loss: 1.2520\n",
      "Epoch [7977/10000], Loss: 1.2519\n",
      "Epoch [7978/10000], Loss: 1.2518\n",
      "Epoch [7979/10000], Loss: 1.2517\n",
      "Epoch [7980/10000], Loss: 1.2515\n",
      "Epoch [7981/10000], Loss: 1.2514\n",
      "Epoch [7982/10000], Loss: 1.2513\n",
      "Epoch [7983/10000], Loss: 1.2511\n",
      "Epoch [7984/10000], Loss: 1.2510\n",
      "Epoch [7985/10000], Loss: 1.2509\n",
      "Epoch [7986/10000], Loss: 1.2508\n",
      "Epoch [7987/10000], Loss: 1.2506\n",
      "Epoch [7988/10000], Loss: 1.2505\n",
      "Epoch [7989/10000], Loss: 1.2504\n",
      "Epoch [7990/10000], Loss: 1.2502\n",
      "Epoch [7991/10000], Loss: 1.2501\n",
      "Epoch [7992/10000], Loss: 1.2500\n",
      "Epoch [7993/10000], Loss: 1.2499\n",
      "Epoch [7994/10000], Loss: 1.2497\n",
      "Epoch [7995/10000], Loss: 1.2496\n",
      "Epoch [7996/10000], Loss: 1.2495\n",
      "Epoch [7997/10000], Loss: 1.2493\n",
      "Epoch [7998/10000], Loss: 1.2492\n",
      "Epoch [7999/10000], Loss: 1.2491\n",
      "Epoch [8000/10000], Loss: 1.2490\n",
      "Epoch [8001/10000], Loss: 1.2488\n",
      "Epoch [8002/10000], Loss: 1.2487\n",
      "Epoch [8003/10000], Loss: 1.2486\n",
      "Epoch [8004/10000], Loss: 1.2485\n",
      "Epoch [8005/10000], Loss: 1.2483\n",
      "Epoch [8006/10000], Loss: 1.2482\n",
      "Epoch [8007/10000], Loss: 1.2481\n",
      "Epoch [8008/10000], Loss: 1.2479\n",
      "Epoch [8009/10000], Loss: 1.2478\n",
      "Epoch [8010/10000], Loss: 1.2477\n",
      "Epoch [8011/10000], Loss: 1.2476\n",
      "Epoch [8012/10000], Loss: 1.2474\n",
      "Epoch [8013/10000], Loss: 1.2473\n",
      "Epoch [8014/10000], Loss: 1.2472\n",
      "Epoch [8015/10000], Loss: 1.2470\n",
      "Epoch [8016/10000], Loss: 1.2469\n",
      "Epoch [8017/10000], Loss: 1.2468\n",
      "Epoch [8018/10000], Loss: 1.2467\n",
      "Epoch [8019/10000], Loss: 1.2465\n",
      "Epoch [8020/10000], Loss: 1.2464\n",
      "Epoch [8021/10000], Loss: 1.2463\n",
      "Epoch [8022/10000], Loss: 1.2461\n",
      "Epoch [8023/10000], Loss: 1.2460\n",
      "Epoch [8024/10000], Loss: 1.2459\n",
      "Epoch [8025/10000], Loss: 1.2458\n",
      "Epoch [8026/10000], Loss: 1.2456\n",
      "Epoch [8027/10000], Loss: 1.2455\n",
      "Epoch [8028/10000], Loss: 1.2454\n",
      "Epoch [8029/10000], Loss: 1.2453\n",
      "Epoch [8030/10000], Loss: 1.2451\n",
      "Epoch [8031/10000], Loss: 1.2450\n",
      "Epoch [8032/10000], Loss: 1.2449\n",
      "Epoch [8033/10000], Loss: 1.2447\n",
      "Epoch [8034/10000], Loss: 1.2446\n",
      "Epoch [8035/10000], Loss: 1.2445\n",
      "Epoch [8036/10000], Loss: 1.2444\n",
      "Epoch [8037/10000], Loss: 1.2442\n",
      "Epoch [8038/10000], Loss: 1.2441\n",
      "Epoch [8039/10000], Loss: 1.2440\n",
      "Epoch [8040/10000], Loss: 1.2438\n",
      "Epoch [8041/10000], Loss: 1.2437\n",
      "Epoch [8042/10000], Loss: 1.2436\n",
      "Epoch [8043/10000], Loss: 1.2435\n",
      "Epoch [8044/10000], Loss: 1.2433\n",
      "Epoch [8045/10000], Loss: 1.2432\n",
      "Epoch [8046/10000], Loss: 1.2431\n",
      "Epoch [8047/10000], Loss: 1.2430\n",
      "Epoch [8048/10000], Loss: 1.2428\n",
      "Epoch [8049/10000], Loss: 1.2427\n",
      "Epoch [8050/10000], Loss: 1.2426\n",
      "Epoch [8051/10000], Loss: 1.2424\n",
      "Epoch [8052/10000], Loss: 1.2423\n",
      "Epoch [8053/10000], Loss: 1.2422\n",
      "Epoch [8054/10000], Loss: 1.2421\n",
      "Epoch [8055/10000], Loss: 1.2419\n",
      "Epoch [8056/10000], Loss: 1.2418\n",
      "Epoch [8057/10000], Loss: 1.2417\n",
      "Epoch [8058/10000], Loss: 1.2415\n",
      "Epoch [8059/10000], Loss: 1.2414\n",
      "Epoch [8060/10000], Loss: 1.2413\n",
      "Epoch [8061/10000], Loss: 1.2412\n",
      "Epoch [8062/10000], Loss: 1.2410\n",
      "Epoch [8063/10000], Loss: 1.2409\n",
      "Epoch [8064/10000], Loss: 1.2408\n",
      "Epoch [8065/10000], Loss: 1.2406\n",
      "Epoch [8066/10000], Loss: 1.2405\n",
      "Epoch [8067/10000], Loss: 1.2404\n",
      "Epoch [8068/10000], Loss: 1.2403\n",
      "Epoch [8069/10000], Loss: 1.2401\n",
      "Epoch [8070/10000], Loss: 1.2400\n",
      "Epoch [8071/10000], Loss: 1.2399\n",
      "Epoch [8072/10000], Loss: 1.2398\n",
      "Epoch [8073/10000], Loss: 1.2396\n",
      "Epoch [8074/10000], Loss: 1.2395\n",
      "Epoch [8075/10000], Loss: 1.2394\n",
      "Epoch [8076/10000], Loss: 1.2392\n",
      "Epoch [8077/10000], Loss: 1.2391\n",
      "Epoch [8078/10000], Loss: 1.2390\n",
      "Epoch [8079/10000], Loss: 1.2389\n",
      "Epoch [8080/10000], Loss: 1.2387\n",
      "Epoch [8081/10000], Loss: 1.2386\n",
      "Epoch [8082/10000], Loss: 1.2385\n",
      "Epoch [8083/10000], Loss: 1.2383\n",
      "Epoch [8084/10000], Loss: 1.2382\n",
      "Epoch [8085/10000], Loss: 1.2381\n",
      "Epoch [8086/10000], Loss: 1.2380\n",
      "Epoch [8087/10000], Loss: 1.2378\n",
      "Epoch [8088/10000], Loss: 1.2377\n",
      "Epoch [8089/10000], Loss: 1.2376\n",
      "Epoch [8090/10000], Loss: 1.2375\n",
      "Epoch [8091/10000], Loss: 1.2373\n",
      "Epoch [8092/10000], Loss: 1.2372\n",
      "Epoch [8093/10000], Loss: 1.2371\n",
      "Epoch [8094/10000], Loss: 1.2369\n",
      "Epoch [8095/10000], Loss: 1.2368\n",
      "Epoch [8096/10000], Loss: 1.2367\n",
      "Epoch [8097/10000], Loss: 1.2366\n",
      "Epoch [8098/10000], Loss: 1.2364\n",
      "Epoch [8099/10000], Loss: 1.2363\n",
      "Epoch [8100/10000], Loss: 1.2362\n",
      "Epoch [8101/10000], Loss: 1.2360\n",
      "Epoch [8102/10000], Loss: 1.2359\n",
      "Epoch [8103/10000], Loss: 1.2358\n",
      "Epoch [8104/10000], Loss: 1.2357\n",
      "Epoch [8105/10000], Loss: 1.2355\n",
      "Epoch [8106/10000], Loss: 1.2354\n",
      "Epoch [8107/10000], Loss: 1.2353\n",
      "Epoch [8108/10000], Loss: 1.2352\n",
      "Epoch [8109/10000], Loss: 1.2350\n",
      "Epoch [8110/10000], Loss: 1.2349\n",
      "Epoch [8111/10000], Loss: 1.2348\n",
      "Epoch [8112/10000], Loss: 1.2346\n",
      "Epoch [8113/10000], Loss: 1.2345\n",
      "Epoch [8114/10000], Loss: 1.2344\n",
      "Epoch [8115/10000], Loss: 1.2343\n",
      "Epoch [8116/10000], Loss: 1.2341\n",
      "Epoch [8117/10000], Loss: 1.2340\n",
      "Epoch [8118/10000], Loss: 1.2339\n",
      "Epoch [8119/10000], Loss: 1.2337\n",
      "Epoch [8120/10000], Loss: 1.2336\n",
      "Epoch [8121/10000], Loss: 1.2335\n",
      "Epoch [8122/10000], Loss: 1.2334\n",
      "Epoch [8123/10000], Loss: 1.2332\n",
      "Epoch [8124/10000], Loss: 1.2331\n",
      "Epoch [8125/10000], Loss: 1.2330\n",
      "Epoch [8126/10000], Loss: 1.2329\n",
      "Epoch [8127/10000], Loss: 1.2327\n",
      "Epoch [8128/10000], Loss: 1.2326\n",
      "Epoch [8129/10000], Loss: 1.2325\n",
      "Epoch [8130/10000], Loss: 1.2323\n",
      "Epoch [8131/10000], Loss: 1.2322\n",
      "Epoch [8132/10000], Loss: 1.2321\n",
      "Epoch [8133/10000], Loss: 1.2320\n",
      "Epoch [8134/10000], Loss: 1.2318\n",
      "Epoch [8135/10000], Loss: 1.2317\n",
      "Epoch [8136/10000], Loss: 1.2316\n",
      "Epoch [8137/10000], Loss: 1.2314\n",
      "Epoch [8138/10000], Loss: 1.2313\n",
      "Epoch [8139/10000], Loss: 1.2312\n",
      "Epoch [8140/10000], Loss: 1.2311\n",
      "Epoch [8141/10000], Loss: 1.2309\n",
      "Epoch [8142/10000], Loss: 1.2308\n",
      "Epoch [8143/10000], Loss: 1.2307\n",
      "Epoch [8144/10000], Loss: 1.2306\n",
      "Epoch [8145/10000], Loss: 1.2304\n",
      "Epoch [8146/10000], Loss: 1.2303\n",
      "Epoch [8147/10000], Loss: 1.2302\n",
      "Epoch [8148/10000], Loss: 1.2300\n",
      "Epoch [8149/10000], Loss: 1.2299\n",
      "Epoch [8150/10000], Loss: 1.2298\n",
      "Epoch [8151/10000], Loss: 1.2297\n",
      "Epoch [8152/10000], Loss: 1.2295\n",
      "Epoch [8153/10000], Loss: 1.2294\n",
      "Epoch [8154/10000], Loss: 1.2293\n",
      "Epoch [8155/10000], Loss: 1.2291\n",
      "Epoch [8156/10000], Loss: 1.2290\n",
      "Epoch [8157/10000], Loss: 1.2289\n",
      "Epoch [8158/10000], Loss: 1.2288\n",
      "Epoch [8159/10000], Loss: 1.2286\n",
      "Epoch [8160/10000], Loss: 1.2285\n",
      "Epoch [8161/10000], Loss: 1.2284\n",
      "Epoch [8162/10000], Loss: 1.2283\n",
      "Epoch [8163/10000], Loss: 1.2281\n",
      "Epoch [8164/10000], Loss: 1.2280\n",
      "Epoch [8165/10000], Loss: 1.2279\n",
      "Epoch [8166/10000], Loss: 1.2277\n",
      "Epoch [8167/10000], Loss: 1.2276\n",
      "Epoch [8168/10000], Loss: 1.2275\n",
      "Epoch [8169/10000], Loss: 1.2274\n",
      "Epoch [8170/10000], Loss: 1.2272\n",
      "Epoch [8171/10000], Loss: 1.2271\n",
      "Epoch [8172/10000], Loss: 1.2270\n",
      "Epoch [8173/10000], Loss: 1.2268\n",
      "Epoch [8174/10000], Loss: 1.2267\n",
      "Epoch [8175/10000], Loss: 1.2266\n",
      "Epoch [8176/10000], Loss: 1.2265\n",
      "Epoch [8177/10000], Loss: 1.2263\n",
      "Epoch [8178/10000], Loss: 1.2262\n",
      "Epoch [8179/10000], Loss: 1.2261\n",
      "Epoch [8180/10000], Loss: 1.2260\n",
      "Epoch [8181/10000], Loss: 1.2258\n",
      "Epoch [8182/10000], Loss: 1.2257\n",
      "Epoch [8183/10000], Loss: 1.2256\n",
      "Epoch [8184/10000], Loss: 1.2254\n",
      "Epoch [8185/10000], Loss: 1.2253\n",
      "Epoch [8186/10000], Loss: 1.2252\n",
      "Epoch [8187/10000], Loss: 1.2251\n",
      "Epoch [8188/10000], Loss: 1.2249\n",
      "Epoch [8189/10000], Loss: 1.2248\n",
      "Epoch [8190/10000], Loss: 1.2247\n",
      "Epoch [8191/10000], Loss: 1.2245\n",
      "Epoch [8192/10000], Loss: 1.2244\n",
      "Epoch [8193/10000], Loss: 1.2243\n",
      "Epoch [8194/10000], Loss: 1.2242\n",
      "Epoch [8195/10000], Loss: 1.2240\n",
      "Epoch [8196/10000], Loss: 1.2239\n",
      "Epoch [8197/10000], Loss: 1.2238\n",
      "Epoch [8198/10000], Loss: 1.2237\n",
      "Epoch [8199/10000], Loss: 1.2235\n",
      "Epoch [8200/10000], Loss: 1.2234\n",
      "Epoch [8201/10000], Loss: 1.2233\n",
      "Epoch [8202/10000], Loss: 1.2231\n",
      "Epoch [8203/10000], Loss: 1.2230\n",
      "Epoch [8204/10000], Loss: 1.2229\n",
      "Epoch [8205/10000], Loss: 1.2228\n",
      "Epoch [8206/10000], Loss: 1.2226\n",
      "Epoch [8207/10000], Loss: 1.2225\n",
      "Epoch [8208/10000], Loss: 1.2224\n",
      "Epoch [8209/10000], Loss: 1.2222\n",
      "Epoch [8210/10000], Loss: 1.2221\n",
      "Epoch [8211/10000], Loss: 1.2220\n",
      "Epoch [8212/10000], Loss: 1.2219\n",
      "Epoch [8213/10000], Loss: 1.2217\n",
      "Epoch [8214/10000], Loss: 1.2216\n",
      "Epoch [8215/10000], Loss: 1.2215\n",
      "Epoch [8216/10000], Loss: 1.2214\n",
      "Epoch [8217/10000], Loss: 1.2212\n",
      "Epoch [8218/10000], Loss: 1.2211\n",
      "Epoch [8219/10000], Loss: 1.2210\n",
      "Epoch [8220/10000], Loss: 1.2208\n",
      "Epoch [8221/10000], Loss: 1.2207\n",
      "Epoch [8222/10000], Loss: 1.2206\n",
      "Epoch [8223/10000], Loss: 1.2205\n",
      "Epoch [8224/10000], Loss: 1.2203\n",
      "Epoch [8225/10000], Loss: 1.2202\n",
      "Epoch [8226/10000], Loss: 1.2201\n",
      "Epoch [8227/10000], Loss: 1.2200\n",
      "Epoch [8228/10000], Loss: 1.2198\n",
      "Epoch [8229/10000], Loss: 1.2197\n",
      "Epoch [8230/10000], Loss: 1.2196\n",
      "Epoch [8231/10000], Loss: 1.2194\n",
      "Epoch [8232/10000], Loss: 1.2193\n",
      "Epoch [8233/10000], Loss: 1.2192\n",
      "Epoch [8234/10000], Loss: 1.2191\n",
      "Epoch [8235/10000], Loss: 1.2189\n",
      "Epoch [8236/10000], Loss: 1.2188\n",
      "Epoch [8237/10000], Loss: 1.2187\n",
      "Epoch [8238/10000], Loss: 1.2185\n",
      "Epoch [8239/10000], Loss: 1.2184\n",
      "Epoch [8240/10000], Loss: 1.2183\n",
      "Epoch [8241/10000], Loss: 1.2182\n",
      "Epoch [8242/10000], Loss: 1.2180\n",
      "Epoch [8243/10000], Loss: 1.2179\n",
      "Epoch [8244/10000], Loss: 1.2178\n",
      "Epoch [8245/10000], Loss: 1.2177\n",
      "Epoch [8246/10000], Loss: 1.2175\n",
      "Epoch [8247/10000], Loss: 1.2174\n",
      "Epoch [8248/10000], Loss: 1.2173\n",
      "Epoch [8249/10000], Loss: 1.2171\n",
      "Epoch [8250/10000], Loss: 1.2170\n",
      "Epoch [8251/10000], Loss: 1.2169\n",
      "Epoch [8252/10000], Loss: 1.2168\n",
      "Epoch [8253/10000], Loss: 1.2166\n",
      "Epoch [8254/10000], Loss: 1.2165\n",
      "Epoch [8255/10000], Loss: 1.2164\n",
      "Epoch [8256/10000], Loss: 1.2162\n",
      "Epoch [8257/10000], Loss: 1.2161\n",
      "Epoch [8258/10000], Loss: 1.2160\n",
      "Epoch [8259/10000], Loss: 1.2159\n",
      "Epoch [8260/10000], Loss: 1.2157\n",
      "Epoch [8261/10000], Loss: 1.2156\n",
      "Epoch [8262/10000], Loss: 1.2155\n",
      "Epoch [8263/10000], Loss: 1.2154\n",
      "Epoch [8264/10000], Loss: 1.2152\n",
      "Epoch [8265/10000], Loss: 1.2151\n",
      "Epoch [8266/10000], Loss: 1.2150\n",
      "Epoch [8267/10000], Loss: 1.2148\n",
      "Epoch [8268/10000], Loss: 1.2147\n",
      "Epoch [8269/10000], Loss: 1.2146\n",
      "Epoch [8270/10000], Loss: 1.2145\n",
      "Epoch [8271/10000], Loss: 1.2143\n",
      "Epoch [8272/10000], Loss: 1.2142\n",
      "Epoch [8273/10000], Loss: 1.2141\n",
      "Epoch [8274/10000], Loss: 1.2140\n",
      "Epoch [8275/10000], Loss: 1.2138\n",
      "Epoch [8276/10000], Loss: 1.2137\n",
      "Epoch [8277/10000], Loss: 1.2136\n",
      "Epoch [8278/10000], Loss: 1.2134\n",
      "Epoch [8279/10000], Loss: 1.2133\n",
      "Epoch [8280/10000], Loss: 1.2132\n",
      "Epoch [8281/10000], Loss: 1.2131\n",
      "Epoch [8282/10000], Loss: 1.2129\n",
      "Epoch [8283/10000], Loss: 1.2128\n",
      "Epoch [8284/10000], Loss: 1.2127\n",
      "Epoch [8285/10000], Loss: 1.2125\n",
      "Epoch [8286/10000], Loss: 1.2124\n",
      "Epoch [8287/10000], Loss: 1.2123\n",
      "Epoch [8288/10000], Loss: 1.2122\n",
      "Epoch [8289/10000], Loss: 1.2120\n",
      "Epoch [8290/10000], Loss: 1.2119\n",
      "Epoch [8291/10000], Loss: 1.2118\n",
      "Epoch [8292/10000], Loss: 1.2117\n",
      "Epoch [8293/10000], Loss: 1.2115\n",
      "Epoch [8294/10000], Loss: 1.2114\n",
      "Epoch [8295/10000], Loss: 1.2113\n",
      "Epoch [8296/10000], Loss: 1.2111\n",
      "Epoch [8297/10000], Loss: 1.2110\n",
      "Epoch [8298/10000], Loss: 1.2109\n",
      "Epoch [8299/10000], Loss: 1.2108\n",
      "Epoch [8300/10000], Loss: 1.2106\n",
      "Epoch [8301/10000], Loss: 1.2105\n",
      "Epoch [8302/10000], Loss: 1.2104\n",
      "Epoch [8303/10000], Loss: 1.2103\n",
      "Epoch [8304/10000], Loss: 1.2101\n",
      "Epoch [8305/10000], Loss: 1.2100\n",
      "Epoch [8306/10000], Loss: 1.2099\n",
      "Epoch [8307/10000], Loss: 1.2097\n",
      "Epoch [8308/10000], Loss: 1.2096\n",
      "Epoch [8309/10000], Loss: 1.2095\n",
      "Epoch [8310/10000], Loss: 1.2094\n",
      "Epoch [8311/10000], Loss: 1.2092\n",
      "Epoch [8312/10000], Loss: 1.2091\n",
      "Epoch [8313/10000], Loss: 1.2090\n",
      "Epoch [8314/10000], Loss: 1.2088\n",
      "Epoch [8315/10000], Loss: 1.2087\n",
      "Epoch [8316/10000], Loss: 1.2086\n",
      "Epoch [8317/10000], Loss: 1.2085\n",
      "Epoch [8318/10000], Loss: 1.2083\n",
      "Epoch [8319/10000], Loss: 1.2082\n",
      "Epoch [8320/10000], Loss: 1.2081\n",
      "Epoch [8321/10000], Loss: 1.2080\n",
      "Epoch [8322/10000], Loss: 1.2078\n",
      "Epoch [8323/10000], Loss: 1.2077\n",
      "Epoch [8324/10000], Loss: 1.2076\n",
      "Epoch [8325/10000], Loss: 1.2074\n",
      "Epoch [8326/10000], Loss: 1.2073\n",
      "Epoch [8327/10000], Loss: 1.2072\n",
      "Epoch [8328/10000], Loss: 1.2071\n",
      "Epoch [8329/10000], Loss: 1.2069\n",
      "Epoch [8330/10000], Loss: 1.2068\n",
      "Epoch [8331/10000], Loss: 1.2067\n",
      "Epoch [8332/10000], Loss: 1.2066\n",
      "Epoch [8333/10000], Loss: 1.2064\n",
      "Epoch [8334/10000], Loss: 1.2063\n",
      "Epoch [8335/10000], Loss: 1.2062\n",
      "Epoch [8336/10000], Loss: 1.2060\n",
      "Epoch [8337/10000], Loss: 1.2059\n",
      "Epoch [8338/10000], Loss: 1.2058\n",
      "Epoch [8339/10000], Loss: 1.2057\n",
      "Epoch [8340/10000], Loss: 1.2055\n",
      "Epoch [8341/10000], Loss: 1.2054\n",
      "Epoch [8342/10000], Loss: 1.2053\n",
      "Epoch [8343/10000], Loss: 1.2052\n",
      "Epoch [8344/10000], Loss: 1.2050\n",
      "Epoch [8345/10000], Loss: 1.2049\n",
      "Epoch [8346/10000], Loss: 1.2048\n",
      "Epoch [8347/10000], Loss: 1.2046\n",
      "Epoch [8348/10000], Loss: 1.2045\n",
      "Epoch [8349/10000], Loss: 1.2044\n",
      "Epoch [8350/10000], Loss: 1.2043\n",
      "Epoch [8351/10000], Loss: 1.2041\n",
      "Epoch [8352/10000], Loss: 1.2040\n",
      "Epoch [8353/10000], Loss: 1.2039\n",
      "Epoch [8354/10000], Loss: 1.2037\n",
      "Epoch [8355/10000], Loss: 1.2036\n",
      "Epoch [8356/10000], Loss: 1.2035\n",
      "Epoch [8357/10000], Loss: 1.2034\n",
      "Epoch [8358/10000], Loss: 1.2032\n",
      "Epoch [8359/10000], Loss: 1.2031\n",
      "Epoch [8360/10000], Loss: 1.2030\n",
      "Epoch [8361/10000], Loss: 1.2029\n",
      "Epoch [8362/10000], Loss: 1.2027\n",
      "Epoch [8363/10000], Loss: 1.2026\n",
      "Epoch [8364/10000], Loss: 1.2025\n",
      "Epoch [8365/10000], Loss: 1.2023\n",
      "Epoch [8366/10000], Loss: 1.2022\n",
      "Epoch [8367/10000], Loss: 1.2021\n",
      "Epoch [8368/10000], Loss: 1.2020\n",
      "Epoch [8369/10000], Loss: 1.2018\n",
      "Epoch [8370/10000], Loss: 1.2017\n",
      "Epoch [8371/10000], Loss: 1.2016\n",
      "Epoch [8372/10000], Loss: 1.2015\n",
      "Epoch [8373/10000], Loss: 1.2013\n",
      "Epoch [8374/10000], Loss: 1.2012\n",
      "Epoch [8375/10000], Loss: 1.2011\n",
      "Epoch [8376/10000], Loss: 1.2009\n",
      "Epoch [8377/10000], Loss: 1.2008\n",
      "Epoch [8378/10000], Loss: 1.2007\n",
      "Epoch [8379/10000], Loss: 1.2006\n",
      "Epoch [8380/10000], Loss: 1.2004\n",
      "Epoch [8381/10000], Loss: 1.2003\n",
      "Epoch [8382/10000], Loss: 1.2002\n",
      "Epoch [8383/10000], Loss: 1.2001\n",
      "Epoch [8384/10000], Loss: 1.1999\n",
      "Epoch [8385/10000], Loss: 1.1998\n",
      "Epoch [8386/10000], Loss: 1.1997\n",
      "Epoch [8387/10000], Loss: 1.1995\n",
      "Epoch [8388/10000], Loss: 1.1994\n",
      "Epoch [8389/10000], Loss: 1.1993\n",
      "Epoch [8390/10000], Loss: 1.1992\n",
      "Epoch [8391/10000], Loss: 1.1990\n",
      "Epoch [8392/10000], Loss: 1.1989\n",
      "Epoch [8393/10000], Loss: 1.1988\n",
      "Epoch [8394/10000], Loss: 1.1987\n",
      "Epoch [8395/10000], Loss: 1.1985\n",
      "Epoch [8396/10000], Loss: 1.1984\n",
      "Epoch [8397/10000], Loss: 1.1983\n",
      "Epoch [8398/10000], Loss: 1.1981\n",
      "Epoch [8399/10000], Loss: 1.1980\n",
      "Epoch [8400/10000], Loss: 1.1979\n",
      "Epoch [8401/10000], Loss: 1.1978\n",
      "Epoch [8402/10000], Loss: 1.1976\n",
      "Epoch [8403/10000], Loss: 1.1975\n",
      "Epoch [8404/10000], Loss: 1.1974\n",
      "Epoch [8405/10000], Loss: 1.1972\n",
      "Epoch [8406/10000], Loss: 1.1971\n",
      "Epoch [8407/10000], Loss: 1.1970\n",
      "Epoch [8408/10000], Loss: 1.1969\n",
      "Epoch [8409/10000], Loss: 1.1967\n",
      "Epoch [8410/10000], Loss: 1.1966\n",
      "Epoch [8411/10000], Loss: 1.1965\n",
      "Epoch [8412/10000], Loss: 1.1964\n",
      "Epoch [8413/10000], Loss: 1.1962\n",
      "Epoch [8414/10000], Loss: 1.1961\n",
      "Epoch [8415/10000], Loss: 1.1960\n",
      "Epoch [8416/10000], Loss: 1.1958\n",
      "Epoch [8417/10000], Loss: 1.1957\n",
      "Epoch [8418/10000], Loss: 1.1956\n",
      "Epoch [8419/10000], Loss: 1.1955\n",
      "Epoch [8420/10000], Loss: 1.1953\n",
      "Epoch [8421/10000], Loss: 1.1952\n",
      "Epoch [8422/10000], Loss: 1.1951\n",
      "Epoch [8423/10000], Loss: 1.1950\n",
      "Epoch [8424/10000], Loss: 1.1948\n",
      "Epoch [8425/10000], Loss: 1.1947\n",
      "Epoch [8426/10000], Loss: 1.1946\n",
      "Epoch [8427/10000], Loss: 1.1944\n",
      "Epoch [8428/10000], Loss: 1.1943\n",
      "Epoch [8429/10000], Loss: 1.1942\n",
      "Epoch [8430/10000], Loss: 1.1941\n",
      "Epoch [8431/10000], Loss: 1.1939\n",
      "Epoch [8432/10000], Loss: 1.1938\n",
      "Epoch [8433/10000], Loss: 1.1937\n",
      "Epoch [8434/10000], Loss: 1.1936\n",
      "Epoch [8435/10000], Loss: 1.1934\n",
      "Epoch [8436/10000], Loss: 1.1933\n",
      "Epoch [8437/10000], Loss: 1.1932\n",
      "Epoch [8438/10000], Loss: 1.1930\n",
      "Epoch [8439/10000], Loss: 1.1929\n",
      "Epoch [8440/10000], Loss: 1.1928\n",
      "Epoch [8441/10000], Loss: 1.1927\n",
      "Epoch [8442/10000], Loss: 1.1925\n",
      "Epoch [8443/10000], Loss: 1.1924\n",
      "Epoch [8444/10000], Loss: 1.1923\n",
      "Epoch [8445/10000], Loss: 1.1922\n",
      "Epoch [8446/10000], Loss: 1.1920\n",
      "Epoch [8447/10000], Loss: 1.1919\n",
      "Epoch [8448/10000], Loss: 1.1918\n",
      "Epoch [8449/10000], Loss: 1.1916\n",
      "Epoch [8450/10000], Loss: 1.1915\n",
      "Epoch [8451/10000], Loss: 1.1914\n",
      "Epoch [8452/10000], Loss: 1.1913\n",
      "Epoch [8453/10000], Loss: 1.1911\n",
      "Epoch [8454/10000], Loss: 1.1910\n",
      "Epoch [8455/10000], Loss: 1.1909\n",
      "Epoch [8456/10000], Loss: 1.1908\n",
      "Epoch [8457/10000], Loss: 1.1906\n",
      "Epoch [8458/10000], Loss: 1.1905\n",
      "Epoch [8459/10000], Loss: 1.1904\n",
      "Epoch [8460/10000], Loss: 1.1902\n",
      "Epoch [8461/10000], Loss: 1.1901\n",
      "Epoch [8462/10000], Loss: 1.1900\n",
      "Epoch [8463/10000], Loss: 1.1899\n",
      "Epoch [8464/10000], Loss: 1.1897\n",
      "Epoch [8465/10000], Loss: 1.1896\n",
      "Epoch [8466/10000], Loss: 1.1895\n",
      "Epoch [8467/10000], Loss: 1.1894\n",
      "Epoch [8468/10000], Loss: 1.1892\n",
      "Epoch [8469/10000], Loss: 1.1891\n",
      "Epoch [8470/10000], Loss: 1.1890\n",
      "Epoch [8471/10000], Loss: 1.1888\n",
      "Epoch [8472/10000], Loss: 1.1887\n",
      "Epoch [8473/10000], Loss: 1.1886\n",
      "Epoch [8474/10000], Loss: 1.1885\n",
      "Epoch [8475/10000], Loss: 1.1883\n",
      "Epoch [8476/10000], Loss: 1.1882\n",
      "Epoch [8477/10000], Loss: 1.1881\n",
      "Epoch [8478/10000], Loss: 1.1880\n",
      "Epoch [8479/10000], Loss: 1.1878\n",
      "Epoch [8480/10000], Loss: 1.1877\n",
      "Epoch [8481/10000], Loss: 1.1876\n",
      "Epoch [8482/10000], Loss: 1.1874\n",
      "Epoch [8483/10000], Loss: 1.1873\n",
      "Epoch [8484/10000], Loss: 1.1872\n",
      "Epoch [8485/10000], Loss: 1.1871\n",
      "Epoch [8486/10000], Loss: 1.1869\n",
      "Epoch [8487/10000], Loss: 1.1868\n",
      "Epoch [8488/10000], Loss: 1.1867\n",
      "Epoch [8489/10000], Loss: 1.1865\n",
      "Epoch [8490/10000], Loss: 1.1864\n",
      "Epoch [8491/10000], Loss: 1.1863\n",
      "Epoch [8492/10000], Loss: 1.1862\n",
      "Epoch [8493/10000], Loss: 1.1860\n",
      "Epoch [8494/10000], Loss: 1.1859\n",
      "Epoch [8495/10000], Loss: 1.1858\n",
      "Epoch [8496/10000], Loss: 1.1857\n",
      "Epoch [8497/10000], Loss: 1.1855\n",
      "Epoch [8498/10000], Loss: 1.1854\n",
      "Epoch [8499/10000], Loss: 1.1853\n",
      "Epoch [8500/10000], Loss: 1.1851\n",
      "Epoch [8501/10000], Loss: 1.1850\n",
      "Epoch [8502/10000], Loss: 1.1849\n",
      "Epoch [8503/10000], Loss: 1.1848\n",
      "Epoch [8504/10000], Loss: 1.1846\n",
      "Epoch [8505/10000], Loss: 1.1845\n",
      "Epoch [8506/10000], Loss: 1.1844\n",
      "Epoch [8507/10000], Loss: 1.1843\n",
      "Epoch [8508/10000], Loss: 1.1841\n",
      "Epoch [8509/10000], Loss: 1.1840\n",
      "Epoch [8510/10000], Loss: 1.1839\n",
      "Epoch [8511/10000], Loss: 1.1837\n",
      "Epoch [8512/10000], Loss: 1.1836\n",
      "Epoch [8513/10000], Loss: 1.1835\n",
      "Epoch [8514/10000], Loss: 1.1834\n",
      "Epoch [8515/10000], Loss: 1.1832\n",
      "Epoch [8516/10000], Loss: 1.1831\n",
      "Epoch [8517/10000], Loss: 1.1830\n",
      "Epoch [8518/10000], Loss: 1.1829\n",
      "Epoch [8519/10000], Loss: 1.1827\n",
      "Epoch [8520/10000], Loss: 1.1826\n",
      "Epoch [8521/10000], Loss: 1.1825\n",
      "Epoch [8522/10000], Loss: 1.1823\n",
      "Epoch [8523/10000], Loss: 1.1822\n",
      "Epoch [8524/10000], Loss: 1.1821\n",
      "Epoch [8525/10000], Loss: 1.1820\n",
      "Epoch [8526/10000], Loss: 1.1818\n",
      "Epoch [8527/10000], Loss: 1.1817\n",
      "Epoch [8528/10000], Loss: 1.1816\n",
      "Epoch [8529/10000], Loss: 1.1815\n",
      "Epoch [8530/10000], Loss: 1.1813\n",
      "Epoch [8531/10000], Loss: 1.1812\n",
      "Epoch [8532/10000], Loss: 1.1811\n",
      "Epoch [8533/10000], Loss: 1.1809\n",
      "Epoch [8534/10000], Loss: 1.1808\n",
      "Epoch [8535/10000], Loss: 1.1807\n",
      "Epoch [8536/10000], Loss: 1.1806\n",
      "Epoch [8537/10000], Loss: 1.1804\n",
      "Epoch [8538/10000], Loss: 1.1803\n",
      "Epoch [8539/10000], Loss: 1.1802\n",
      "Epoch [8540/10000], Loss: 1.1801\n",
      "Epoch [8541/10000], Loss: 1.1799\n",
      "Epoch [8542/10000], Loss: 1.1798\n",
      "Epoch [8543/10000], Loss: 1.1797\n",
      "Epoch [8544/10000], Loss: 1.1795\n",
      "Epoch [8545/10000], Loss: 1.1794\n",
      "Epoch [8546/10000], Loss: 1.1793\n",
      "Epoch [8547/10000], Loss: 1.1792\n",
      "Epoch [8548/10000], Loss: 1.1790\n",
      "Epoch [8549/10000], Loss: 1.1789\n",
      "Epoch [8550/10000], Loss: 1.1788\n",
      "Epoch [8551/10000], Loss: 1.1787\n",
      "Epoch [8552/10000], Loss: 1.1785\n",
      "Epoch [8553/10000], Loss: 1.1784\n",
      "Epoch [8554/10000], Loss: 1.1783\n",
      "Epoch [8555/10000], Loss: 1.1781\n",
      "Epoch [8556/10000], Loss: 1.1780\n",
      "Epoch [8557/10000], Loss: 1.1779\n",
      "Epoch [8558/10000], Loss: 1.1778\n",
      "Epoch [8559/10000], Loss: 1.1776\n",
      "Epoch [8560/10000], Loss: 1.1775\n",
      "Epoch [8561/10000], Loss: 1.1774\n",
      "Epoch [8562/10000], Loss: 1.1773\n",
      "Epoch [8563/10000], Loss: 1.1771\n",
      "Epoch [8564/10000], Loss: 1.1770\n",
      "Epoch [8565/10000], Loss: 1.1769\n",
      "Epoch [8566/10000], Loss: 1.1767\n",
      "Epoch [8567/10000], Loss: 1.1766\n",
      "Epoch [8568/10000], Loss: 1.1765\n",
      "Epoch [8569/10000], Loss: 1.1764\n",
      "Epoch [8570/10000], Loss: 1.1762\n",
      "Epoch [8571/10000], Loss: 1.1761\n",
      "Epoch [8572/10000], Loss: 1.1760\n",
      "Epoch [8573/10000], Loss: 1.1759\n",
      "Epoch [8574/10000], Loss: 1.1757\n",
      "Epoch [8575/10000], Loss: 1.1756\n",
      "Epoch [8576/10000], Loss: 1.1755\n",
      "Epoch [8577/10000], Loss: 1.1753\n",
      "Epoch [8578/10000], Loss: 1.1752\n",
      "Epoch [8579/10000], Loss: 1.1751\n",
      "Epoch [8580/10000], Loss: 1.1750\n",
      "Epoch [8581/10000], Loss: 1.1748\n",
      "Epoch [8582/10000], Loss: 1.1747\n",
      "Epoch [8583/10000], Loss: 1.1746\n",
      "Epoch [8584/10000], Loss: 1.1745\n",
      "Epoch [8585/10000], Loss: 1.1743\n",
      "Epoch [8586/10000], Loss: 1.1742\n",
      "Epoch [8587/10000], Loss: 1.1741\n",
      "Epoch [8588/10000], Loss: 1.1740\n",
      "Epoch [8589/10000], Loss: 1.1738\n",
      "Epoch [8590/10000], Loss: 1.1737\n",
      "Epoch [8591/10000], Loss: 1.1736\n",
      "Epoch [8592/10000], Loss: 1.1734\n",
      "Epoch [8593/10000], Loss: 1.1733\n",
      "Epoch [8594/10000], Loss: 1.1732\n",
      "Epoch [8595/10000], Loss: 1.1731\n",
      "Epoch [8596/10000], Loss: 1.1729\n",
      "Epoch [8597/10000], Loss: 1.1728\n",
      "Epoch [8598/10000], Loss: 1.1727\n",
      "Epoch [8599/10000], Loss: 1.1726\n",
      "Epoch [8600/10000], Loss: 1.1724\n",
      "Epoch [8601/10000], Loss: 1.1723\n",
      "Epoch [8602/10000], Loss: 1.1722\n",
      "Epoch [8603/10000], Loss: 1.1720\n",
      "Epoch [8604/10000], Loss: 1.1719\n",
      "Epoch [8605/10000], Loss: 1.1718\n",
      "Epoch [8606/10000], Loss: 1.1717\n",
      "Epoch [8607/10000], Loss: 1.1715\n",
      "Epoch [8608/10000], Loss: 1.1714\n",
      "Epoch [8609/10000], Loss: 1.1713\n",
      "Epoch [8610/10000], Loss: 1.1712\n",
      "Epoch [8611/10000], Loss: 1.1710\n",
      "Epoch [8612/10000], Loss: 1.1709\n",
      "Epoch [8613/10000], Loss: 1.1708\n",
      "Epoch [8614/10000], Loss: 1.1706\n",
      "Epoch [8615/10000], Loss: 1.1705\n",
      "Epoch [8616/10000], Loss: 1.1704\n",
      "Epoch [8617/10000], Loss: 1.1703\n",
      "Epoch [8618/10000], Loss: 1.1701\n",
      "Epoch [8619/10000], Loss: 1.1700\n",
      "Epoch [8620/10000], Loss: 1.1699\n",
      "Epoch [8621/10000], Loss: 1.1698\n",
      "Epoch [8622/10000], Loss: 1.1696\n",
      "Epoch [8623/10000], Loss: 1.1695\n",
      "Epoch [8624/10000], Loss: 1.1694\n",
      "Epoch [8625/10000], Loss: 1.1692\n",
      "Epoch [8626/10000], Loss: 1.1691\n",
      "Epoch [8627/10000], Loss: 1.1690\n",
      "Epoch [8628/10000], Loss: 1.1689\n",
      "Epoch [8629/10000], Loss: 1.1687\n",
      "Epoch [8630/10000], Loss: 1.1686\n",
      "Epoch [8631/10000], Loss: 1.1685\n",
      "Epoch [8632/10000], Loss: 1.1684\n",
      "Epoch [8633/10000], Loss: 1.1682\n",
      "Epoch [8634/10000], Loss: 1.1681\n",
      "Epoch [8635/10000], Loss: 1.1680\n",
      "Epoch [8636/10000], Loss: 1.1678\n",
      "Epoch [8637/10000], Loss: 1.1677\n",
      "Epoch [8638/10000], Loss: 1.1676\n",
      "Epoch [8639/10000], Loss: 1.1675\n",
      "Epoch [8640/10000], Loss: 1.1673\n",
      "Epoch [8641/10000], Loss: 1.1672\n",
      "Epoch [8642/10000], Loss: 1.1671\n",
      "Epoch [8643/10000], Loss: 1.1670\n",
      "Epoch [8644/10000], Loss: 1.1668\n",
      "Epoch [8645/10000], Loss: 1.1667\n",
      "Epoch [8646/10000], Loss: 1.1666\n",
      "Epoch [8647/10000], Loss: 1.1664\n",
      "Epoch [8648/10000], Loss: 1.1663\n",
      "Epoch [8649/10000], Loss: 1.1662\n",
      "Epoch [8650/10000], Loss: 1.1661\n",
      "Epoch [8651/10000], Loss: 1.1659\n",
      "Epoch [8652/10000], Loss: 1.1658\n",
      "Epoch [8653/10000], Loss: 1.1657\n",
      "Epoch [8654/10000], Loss: 1.1656\n",
      "Epoch [8655/10000], Loss: 1.1654\n",
      "Epoch [8656/10000], Loss: 1.1653\n",
      "Epoch [8657/10000], Loss: 1.1652\n",
      "Epoch [8658/10000], Loss: 1.1650\n",
      "Epoch [8659/10000], Loss: 1.1649\n",
      "Epoch [8660/10000], Loss: 1.1648\n",
      "Epoch [8661/10000], Loss: 1.1647\n",
      "Epoch [8662/10000], Loss: 1.1645\n",
      "Epoch [8663/10000], Loss: 1.1644\n",
      "Epoch [8664/10000], Loss: 1.1643\n",
      "Epoch [8665/10000], Loss: 1.1642\n",
      "Epoch [8666/10000], Loss: 1.1640\n",
      "Epoch [8667/10000], Loss: 1.1639\n",
      "Epoch [8668/10000], Loss: 1.1638\n",
      "Epoch [8669/10000], Loss: 1.1636\n",
      "Epoch [8670/10000], Loss: 1.1635\n",
      "Epoch [8671/10000], Loss: 1.1634\n",
      "Epoch [8672/10000], Loss: 1.1633\n",
      "Epoch [8673/10000], Loss: 1.1631\n",
      "Epoch [8674/10000], Loss: 1.1630\n",
      "Epoch [8675/10000], Loss: 1.1629\n",
      "Epoch [8676/10000], Loss: 1.1628\n",
      "Epoch [8677/10000], Loss: 1.1626\n",
      "Epoch [8678/10000], Loss: 1.1625\n",
      "Epoch [8679/10000], Loss: 1.1624\n",
      "Epoch [8680/10000], Loss: 1.1623\n",
      "Epoch [8681/10000], Loss: 1.1621\n",
      "Epoch [8682/10000], Loss: 1.1620\n",
      "Epoch [8683/10000], Loss: 1.1619\n",
      "Epoch [8684/10000], Loss: 1.1617\n",
      "Epoch [8685/10000], Loss: 1.1616\n",
      "Epoch [8686/10000], Loss: 1.1615\n",
      "Epoch [8687/10000], Loss: 1.1614\n",
      "Epoch [8688/10000], Loss: 1.1612\n",
      "Epoch [8689/10000], Loss: 1.1611\n",
      "Epoch [8690/10000], Loss: 1.1610\n",
      "Epoch [8691/10000], Loss: 1.1609\n",
      "Epoch [8692/10000], Loss: 1.1607\n",
      "Epoch [8693/10000], Loss: 1.1606\n",
      "Epoch [8694/10000], Loss: 1.1605\n",
      "Epoch [8695/10000], Loss: 1.1603\n",
      "Epoch [8696/10000], Loss: 1.1602\n",
      "Epoch [8697/10000], Loss: 1.1601\n",
      "Epoch [8698/10000], Loss: 1.1600\n",
      "Epoch [8699/10000], Loss: 1.1598\n",
      "Epoch [8700/10000], Loss: 1.1597\n",
      "Epoch [8701/10000], Loss: 1.1596\n",
      "Epoch [8702/10000], Loss: 1.1595\n",
      "Epoch [8703/10000], Loss: 1.1593\n",
      "Epoch [8704/10000], Loss: 1.1592\n",
      "Epoch [8705/10000], Loss: 1.1591\n",
      "Epoch [8706/10000], Loss: 1.1589\n",
      "Epoch [8707/10000], Loss: 1.1588\n",
      "Epoch [8708/10000], Loss: 1.1587\n",
      "Epoch [8709/10000], Loss: 1.1586\n",
      "Epoch [8710/10000], Loss: 1.1584\n",
      "Epoch [8711/10000], Loss: 1.1583\n",
      "Epoch [8712/10000], Loss: 1.1582\n",
      "Epoch [8713/10000], Loss: 1.1581\n",
      "Epoch [8714/10000], Loss: 1.1579\n",
      "Epoch [8715/10000], Loss: 1.1578\n",
      "Epoch [8716/10000], Loss: 1.1577\n",
      "Epoch [8717/10000], Loss: 1.1575\n",
      "Epoch [8718/10000], Loss: 1.1574\n",
      "Epoch [8719/10000], Loss: 1.1573\n",
      "Epoch [8720/10000], Loss: 1.1572\n",
      "Epoch [8721/10000], Loss: 1.1570\n",
      "Epoch [8722/10000], Loss: 1.1569\n",
      "Epoch [8723/10000], Loss: 1.1568\n",
      "Epoch [8724/10000], Loss: 1.1567\n",
      "Epoch [8725/10000], Loss: 1.1565\n",
      "Epoch [8726/10000], Loss: 1.1564\n",
      "Epoch [8727/10000], Loss: 1.1563\n",
      "Epoch [8728/10000], Loss: 1.1562\n",
      "Epoch [8729/10000], Loss: 1.1560\n",
      "Epoch [8730/10000], Loss: 1.1559\n",
      "Epoch [8731/10000], Loss: 1.1558\n",
      "Epoch [8732/10000], Loss: 1.1556\n",
      "Epoch [8733/10000], Loss: 1.1555\n",
      "Epoch [8734/10000], Loss: 1.1554\n",
      "Epoch [8735/10000], Loss: 1.1553\n",
      "Epoch [8736/10000], Loss: 1.1551\n",
      "Epoch [8737/10000], Loss: 1.1550\n",
      "Epoch [8738/10000], Loss: 1.1549\n",
      "Epoch [8739/10000], Loss: 1.1548\n",
      "Epoch [8740/10000], Loss: 1.1546\n",
      "Epoch [8741/10000], Loss: 1.1545\n",
      "Epoch [8742/10000], Loss: 1.1544\n",
      "Epoch [8743/10000], Loss: 1.1542\n",
      "Epoch [8744/10000], Loss: 1.1541\n",
      "Epoch [8745/10000], Loss: 1.1540\n",
      "Epoch [8746/10000], Loss: 1.1539\n",
      "Epoch [8747/10000], Loss: 1.1537\n",
      "Epoch [8748/10000], Loss: 1.1536\n",
      "Epoch [8749/10000], Loss: 1.1535\n",
      "Epoch [8750/10000], Loss: 1.1534\n",
      "Epoch [8751/10000], Loss: 1.1532\n",
      "Epoch [8752/10000], Loss: 1.1531\n",
      "Epoch [8753/10000], Loss: 1.1530\n",
      "Epoch [8754/10000], Loss: 1.1528\n",
      "Epoch [8755/10000], Loss: 1.1527\n",
      "Epoch [8756/10000], Loss: 1.1526\n",
      "Epoch [8757/10000], Loss: 1.1525\n",
      "Epoch [8758/10000], Loss: 1.1523\n",
      "Epoch [8759/10000], Loss: 1.1522\n",
      "Epoch [8760/10000], Loss: 1.1521\n",
      "Epoch [8761/10000], Loss: 1.1520\n",
      "Epoch [8762/10000], Loss: 1.1518\n",
      "Epoch [8763/10000], Loss: 1.1517\n",
      "Epoch [8764/10000], Loss: 1.1516\n",
      "Epoch [8765/10000], Loss: 1.1515\n",
      "Epoch [8766/10000], Loss: 1.1513\n",
      "Epoch [8767/10000], Loss: 1.1512\n",
      "Epoch [8768/10000], Loss: 1.1511\n",
      "Epoch [8769/10000], Loss: 1.1509\n",
      "Epoch [8770/10000], Loss: 1.1508\n",
      "Epoch [8771/10000], Loss: 1.1507\n",
      "Epoch [8772/10000], Loss: 1.1506\n",
      "Epoch [8773/10000], Loss: 1.1504\n",
      "Epoch [8774/10000], Loss: 1.1503\n",
      "Epoch [8775/10000], Loss: 1.1502\n",
      "Epoch [8776/10000], Loss: 1.1501\n",
      "Epoch [8777/10000], Loss: 1.1499\n",
      "Epoch [8778/10000], Loss: 1.1498\n",
      "Epoch [8779/10000], Loss: 1.1497\n",
      "Epoch [8780/10000], Loss: 1.1495\n",
      "Epoch [8781/10000], Loss: 1.1494\n",
      "Epoch [8782/10000], Loss: 1.1493\n",
      "Epoch [8783/10000], Loss: 1.1492\n",
      "Epoch [8784/10000], Loss: 1.1490\n",
      "Epoch [8785/10000], Loss: 1.1489\n",
      "Epoch [8786/10000], Loss: 1.1488\n",
      "Epoch [8787/10000], Loss: 1.1487\n",
      "Epoch [8788/10000], Loss: 1.1485\n",
      "Epoch [8789/10000], Loss: 1.1484\n",
      "Epoch [8790/10000], Loss: 1.1483\n",
      "Epoch [8791/10000], Loss: 1.1481\n",
      "Epoch [8792/10000], Loss: 1.1480\n",
      "Epoch [8793/10000], Loss: 1.1479\n",
      "Epoch [8794/10000], Loss: 1.1478\n",
      "Epoch [8795/10000], Loss: 1.1476\n",
      "Epoch [8796/10000], Loss: 1.1475\n",
      "Epoch [8797/10000], Loss: 1.1474\n",
      "Epoch [8798/10000], Loss: 1.1473\n",
      "Epoch [8799/10000], Loss: 1.1471\n",
      "Epoch [8800/10000], Loss: 1.1470\n",
      "Epoch [8801/10000], Loss: 1.1469\n",
      "Epoch [8802/10000], Loss: 1.1468\n",
      "Epoch [8803/10000], Loss: 1.1466\n",
      "Epoch [8804/10000], Loss: 1.1465\n",
      "Epoch [8805/10000], Loss: 1.1464\n",
      "Epoch [8806/10000], Loss: 1.1462\n",
      "Epoch [8807/10000], Loss: 1.1461\n",
      "Epoch [8808/10000], Loss: 1.1460\n",
      "Epoch [8809/10000], Loss: 1.1459\n",
      "Epoch [8810/10000], Loss: 1.1457\n",
      "Epoch [8811/10000], Loss: 1.1456\n",
      "Epoch [8812/10000], Loss: 1.1455\n",
      "Epoch [8813/10000], Loss: 1.1454\n",
      "Epoch [8814/10000], Loss: 1.1452\n",
      "Epoch [8815/10000], Loss: 1.1451\n",
      "Epoch [8816/10000], Loss: 1.1450\n",
      "Epoch [8817/10000], Loss: 1.1448\n",
      "Epoch [8818/10000], Loss: 1.1447\n",
      "Epoch [8819/10000], Loss: 1.1446\n",
      "Epoch [8820/10000], Loss: 1.1445\n",
      "Epoch [8821/10000], Loss: 1.1443\n",
      "Epoch [8822/10000], Loss: 1.1442\n",
      "Epoch [8823/10000], Loss: 1.1441\n",
      "Epoch [8824/10000], Loss: 1.1440\n",
      "Epoch [8825/10000], Loss: 1.1438\n",
      "Epoch [8826/10000], Loss: 1.1437\n",
      "Epoch [8827/10000], Loss: 1.1436\n",
      "Epoch [8828/10000], Loss: 1.1435\n",
      "Epoch [8829/10000], Loss: 1.1433\n",
      "Epoch [8830/10000], Loss: 1.1432\n",
      "Epoch [8831/10000], Loss: 1.1431\n",
      "Epoch [8832/10000], Loss: 1.1429\n",
      "Epoch [8833/10000], Loss: 1.1428\n",
      "Epoch [8834/10000], Loss: 1.1427\n",
      "Epoch [8835/10000], Loss: 1.1426\n",
      "Epoch [8836/10000], Loss: 1.1424\n",
      "Epoch [8837/10000], Loss: 1.1423\n",
      "Epoch [8838/10000], Loss: 1.1422\n",
      "Epoch [8839/10000], Loss: 1.1421\n",
      "Epoch [8840/10000], Loss: 1.1419\n",
      "Epoch [8841/10000], Loss: 1.1418\n",
      "Epoch [8842/10000], Loss: 1.1417\n",
      "Epoch [8843/10000], Loss: 1.1415\n",
      "Epoch [8844/10000], Loss: 1.1414\n",
      "Epoch [8845/10000], Loss: 1.1413\n",
      "Epoch [8846/10000], Loss: 1.1412\n",
      "Epoch [8847/10000], Loss: 1.1410\n",
      "Epoch [8848/10000], Loss: 1.1409\n",
      "Epoch [8849/10000], Loss: 1.1408\n",
      "Epoch [8850/10000], Loss: 1.1407\n",
      "Epoch [8851/10000], Loss: 1.1405\n",
      "Epoch [8852/10000], Loss: 1.1404\n",
      "Epoch [8853/10000], Loss: 1.1403\n",
      "Epoch [8854/10000], Loss: 1.1402\n",
      "Epoch [8855/10000], Loss: 1.1400\n",
      "Epoch [8856/10000], Loss: 1.1399\n",
      "Epoch [8857/10000], Loss: 1.1398\n",
      "Epoch [8858/10000], Loss: 1.1396\n",
      "Epoch [8859/10000], Loss: 1.1395\n",
      "Epoch [8860/10000], Loss: 1.1394\n",
      "Epoch [8861/10000], Loss: 1.1393\n",
      "Epoch [8862/10000], Loss: 1.1391\n",
      "Epoch [8863/10000], Loss: 1.1390\n",
      "Epoch [8864/10000], Loss: 1.1389\n",
      "Epoch [8865/10000], Loss: 1.1388\n",
      "Epoch [8866/10000], Loss: 1.1386\n",
      "Epoch [8867/10000], Loss: 1.1385\n",
      "Epoch [8868/10000], Loss: 1.1384\n",
      "Epoch [8869/10000], Loss: 1.1382\n",
      "Epoch [8870/10000], Loss: 1.1381\n",
      "Epoch [8871/10000], Loss: 1.1380\n",
      "Epoch [8872/10000], Loss: 1.1379\n",
      "Epoch [8873/10000], Loss: 1.1377\n",
      "Epoch [8874/10000], Loss: 1.1376\n",
      "Epoch [8875/10000], Loss: 1.1375\n",
      "Epoch [8876/10000], Loss: 1.1374\n",
      "Epoch [8877/10000], Loss: 1.1372\n",
      "Epoch [8878/10000], Loss: 1.1371\n",
      "Epoch [8879/10000], Loss: 1.1370\n",
      "Epoch [8880/10000], Loss: 1.1369\n",
      "Epoch [8881/10000], Loss: 1.1367\n",
      "Epoch [8882/10000], Loss: 1.1366\n",
      "Epoch [8883/10000], Loss: 1.1365\n",
      "Epoch [8884/10000], Loss: 1.1363\n",
      "Epoch [8885/10000], Loss: 1.1362\n",
      "Epoch [8886/10000], Loss: 1.1361\n",
      "Epoch [8887/10000], Loss: 1.1360\n",
      "Epoch [8888/10000], Loss: 1.1358\n",
      "Epoch [8889/10000], Loss: 1.1357\n",
      "Epoch [8890/10000], Loss: 1.1356\n",
      "Epoch [8891/10000], Loss: 1.1355\n",
      "Epoch [8892/10000], Loss: 1.1353\n",
      "Epoch [8893/10000], Loss: 1.1352\n",
      "Epoch [8894/10000], Loss: 1.1351\n",
      "Epoch [8895/10000], Loss: 1.1349\n",
      "Epoch [8896/10000], Loss: 1.1348\n",
      "Epoch [8897/10000], Loss: 1.1347\n",
      "Epoch [8898/10000], Loss: 1.1346\n",
      "Epoch [8899/10000], Loss: 1.1344\n",
      "Epoch [8900/10000], Loss: 1.1343\n",
      "Epoch [8901/10000], Loss: 1.1342\n",
      "Epoch [8902/10000], Loss: 1.1341\n",
      "Epoch [8903/10000], Loss: 1.1339\n",
      "Epoch [8904/10000], Loss: 1.1338\n",
      "Epoch [8905/10000], Loss: 1.1337\n",
      "Epoch [8906/10000], Loss: 1.1336\n",
      "Epoch [8907/10000], Loss: 1.1334\n",
      "Epoch [8908/10000], Loss: 1.1333\n",
      "Epoch [8909/10000], Loss: 1.1332\n",
      "Epoch [8910/10000], Loss: 1.1330\n",
      "Epoch [8911/10000], Loss: 1.1329\n",
      "Epoch [8912/10000], Loss: 1.1328\n",
      "Epoch [8913/10000], Loss: 1.1327\n",
      "Epoch [8914/10000], Loss: 1.1325\n",
      "Epoch [8915/10000], Loss: 1.1324\n",
      "Epoch [8916/10000], Loss: 1.1323\n",
      "Epoch [8917/10000], Loss: 1.1322\n",
      "Epoch [8918/10000], Loss: 1.1320\n",
      "Epoch [8919/10000], Loss: 1.1319\n",
      "Epoch [8920/10000], Loss: 1.1318\n",
      "Epoch [8921/10000], Loss: 1.1316\n",
      "Epoch [8922/10000], Loss: 1.1315\n",
      "Epoch [8923/10000], Loss: 1.1314\n",
      "Epoch [8924/10000], Loss: 1.1313\n",
      "Epoch [8925/10000], Loss: 1.1311\n",
      "Epoch [8926/10000], Loss: 1.1310\n",
      "Epoch [8927/10000], Loss: 1.1309\n",
      "Epoch [8928/10000], Loss: 1.1308\n",
      "Epoch [8929/10000], Loss: 1.1306\n",
      "Epoch [8930/10000], Loss: 1.1305\n",
      "Epoch [8931/10000], Loss: 1.1304\n",
      "Epoch [8932/10000], Loss: 1.1303\n",
      "Epoch [8933/10000], Loss: 1.1301\n",
      "Epoch [8934/10000], Loss: 1.1300\n",
      "Epoch [8935/10000], Loss: 1.1299\n",
      "Epoch [8936/10000], Loss: 1.1297\n",
      "Epoch [8937/10000], Loss: 1.1296\n",
      "Epoch [8938/10000], Loss: 1.1295\n",
      "Epoch [8939/10000], Loss: 1.1294\n",
      "Epoch [8940/10000], Loss: 1.1292\n",
      "Epoch [8941/10000], Loss: 1.1291\n",
      "Epoch [8942/10000], Loss: 1.1290\n",
      "Epoch [8943/10000], Loss: 1.1289\n",
      "Epoch [8944/10000], Loss: 1.1287\n",
      "Epoch [8945/10000], Loss: 1.1286\n",
      "Epoch [8946/10000], Loss: 1.1285\n",
      "Epoch [8947/10000], Loss: 1.1284\n",
      "Epoch [8948/10000], Loss: 1.1282\n",
      "Epoch [8949/10000], Loss: 1.1281\n",
      "Epoch [8950/10000], Loss: 1.1280\n",
      "Epoch [8951/10000], Loss: 1.1278\n",
      "Epoch [8952/10000], Loss: 1.1277\n",
      "Epoch [8953/10000], Loss: 1.1276\n",
      "Epoch [8954/10000], Loss: 1.1275\n",
      "Epoch [8955/10000], Loss: 1.1273\n",
      "Epoch [8956/10000], Loss: 1.1272\n",
      "Epoch [8957/10000], Loss: 1.1271\n",
      "Epoch [8958/10000], Loss: 1.1270\n",
      "Epoch [8959/10000], Loss: 1.1268\n",
      "Epoch [8960/10000], Loss: 1.1267\n",
      "Epoch [8961/10000], Loss: 1.1266\n",
      "Epoch [8962/10000], Loss: 1.1264\n",
      "Epoch [8963/10000], Loss: 1.1263\n",
      "Epoch [8964/10000], Loss: 1.1262\n",
      "Epoch [8965/10000], Loss: 1.1261\n",
      "Epoch [8966/10000], Loss: 1.1259\n",
      "Epoch [8967/10000], Loss: 1.1258\n",
      "Epoch [8968/10000], Loss: 1.1257\n",
      "Epoch [8969/10000], Loss: 1.1256\n",
      "Epoch [8970/10000], Loss: 1.1254\n",
      "Epoch [8971/10000], Loss: 1.1253\n",
      "Epoch [8972/10000], Loss: 1.1252\n",
      "Epoch [8973/10000], Loss: 1.1251\n",
      "Epoch [8974/10000], Loss: 1.1249\n",
      "Epoch [8975/10000], Loss: 1.1248\n",
      "Epoch [8976/10000], Loss: 1.1247\n",
      "Epoch [8977/10000], Loss: 1.1245\n",
      "Epoch [8978/10000], Loss: 1.1244\n",
      "Epoch [8979/10000], Loss: 1.1243\n",
      "Epoch [8980/10000], Loss: 1.1242\n",
      "Epoch [8981/10000], Loss: 1.1240\n",
      "Epoch [8982/10000], Loss: 1.1239\n",
      "Epoch [8983/10000], Loss: 1.1238\n",
      "Epoch [8984/10000], Loss: 1.1237\n",
      "Epoch [8985/10000], Loss: 1.1235\n",
      "Epoch [8986/10000], Loss: 1.1234\n",
      "Epoch [8987/10000], Loss: 1.1233\n",
      "Epoch [8988/10000], Loss: 1.1232\n",
      "Epoch [8989/10000], Loss: 1.1230\n",
      "Epoch [8990/10000], Loss: 1.1229\n",
      "Epoch [8991/10000], Loss: 1.1228\n",
      "Epoch [8992/10000], Loss: 1.1226\n",
      "Epoch [8993/10000], Loss: 1.1225\n",
      "Epoch [8994/10000], Loss: 1.1224\n",
      "Epoch [8995/10000], Loss: 1.1223\n",
      "Epoch [8996/10000], Loss: 1.1221\n",
      "Epoch [8997/10000], Loss: 1.1220\n",
      "Epoch [8998/10000], Loss: 1.1219\n",
      "Epoch [8999/10000], Loss: 1.1218\n",
      "Epoch [9000/10000], Loss: 1.1216\n",
      "Epoch [9001/10000], Loss: 1.1215\n",
      "Epoch [9002/10000], Loss: 1.1214\n",
      "Epoch [9003/10000], Loss: 1.1212\n",
      "Epoch [9004/10000], Loss: 1.1211\n",
      "Epoch [9005/10000], Loss: 1.1210\n",
      "Epoch [9006/10000], Loss: 1.1209\n",
      "Epoch [9007/10000], Loss: 1.1207\n",
      "Epoch [9008/10000], Loss: 1.1206\n",
      "Epoch [9009/10000], Loss: 1.1205\n",
      "Epoch [9010/10000], Loss: 1.1204\n",
      "Epoch [9011/10000], Loss: 1.1202\n",
      "Epoch [9012/10000], Loss: 1.1201\n",
      "Epoch [9013/10000], Loss: 1.1200\n",
      "Epoch [9014/10000], Loss: 1.1199\n",
      "Epoch [9015/10000], Loss: 1.1197\n",
      "Epoch [9016/10000], Loss: 1.1196\n",
      "Epoch [9017/10000], Loss: 1.1195\n",
      "Epoch [9018/10000], Loss: 1.1193\n",
      "Epoch [9019/10000], Loss: 1.1192\n",
      "Epoch [9020/10000], Loss: 1.1191\n",
      "Epoch [9021/10000], Loss: 1.1190\n",
      "Epoch [9022/10000], Loss: 1.1188\n",
      "Epoch [9023/10000], Loss: 1.1187\n",
      "Epoch [9024/10000], Loss: 1.1186\n",
      "Epoch [9025/10000], Loss: 1.1185\n",
      "Epoch [9026/10000], Loss: 1.1183\n",
      "Epoch [9027/10000], Loss: 1.1182\n",
      "Epoch [9028/10000], Loss: 1.1181\n",
      "Epoch [9029/10000], Loss: 1.1180\n",
      "Epoch [9030/10000], Loss: 1.1178\n",
      "Epoch [9031/10000], Loss: 1.1177\n",
      "Epoch [9032/10000], Loss: 1.1176\n",
      "Epoch [9033/10000], Loss: 1.1174\n",
      "Epoch [9034/10000], Loss: 1.1173\n",
      "Epoch [9035/10000], Loss: 1.1172\n",
      "Epoch [9036/10000], Loss: 1.1171\n",
      "Epoch [9037/10000], Loss: 1.1169\n",
      "Epoch [9038/10000], Loss: 1.1168\n",
      "Epoch [9039/10000], Loss: 1.1167\n",
      "Epoch [9040/10000], Loss: 1.1166\n",
      "Epoch [9041/10000], Loss: 1.1164\n",
      "Epoch [9042/10000], Loss: 1.1163\n",
      "Epoch [9043/10000], Loss: 1.1162\n",
      "Epoch [9044/10000], Loss: 1.1161\n",
      "Epoch [9045/10000], Loss: 1.1159\n",
      "Epoch [9046/10000], Loss: 1.1158\n",
      "Epoch [9047/10000], Loss: 1.1157\n",
      "Epoch [9048/10000], Loss: 1.1155\n",
      "Epoch [9049/10000], Loss: 1.1154\n",
      "Epoch [9050/10000], Loss: 1.1153\n",
      "Epoch [9051/10000], Loss: 1.1152\n",
      "Epoch [9052/10000], Loss: 1.1150\n",
      "Epoch [9053/10000], Loss: 1.1149\n",
      "Epoch [9054/10000], Loss: 1.1148\n",
      "Epoch [9055/10000], Loss: 1.1147\n",
      "Epoch [9056/10000], Loss: 1.1145\n",
      "Epoch [9057/10000], Loss: 1.1144\n",
      "Epoch [9058/10000], Loss: 1.1143\n",
      "Epoch [9059/10000], Loss: 1.1142\n",
      "Epoch [9060/10000], Loss: 1.1140\n",
      "Epoch [9061/10000], Loss: 1.1139\n",
      "Epoch [9062/10000], Loss: 1.1138\n",
      "Epoch [9063/10000], Loss: 1.1136\n",
      "Epoch [9064/10000], Loss: 1.1135\n",
      "Epoch [9065/10000], Loss: 1.1134\n",
      "Epoch [9066/10000], Loss: 1.1133\n",
      "Epoch [9067/10000], Loss: 1.1131\n",
      "Epoch [9068/10000], Loss: 1.1130\n",
      "Epoch [9069/10000], Loss: 1.1129\n",
      "Epoch [9070/10000], Loss: 1.1128\n",
      "Epoch [9071/10000], Loss: 1.1126\n",
      "Epoch [9072/10000], Loss: 1.1125\n",
      "Epoch [9073/10000], Loss: 1.1124\n",
      "Epoch [9074/10000], Loss: 1.1122\n",
      "Epoch [9075/10000], Loss: 1.1121\n",
      "Epoch [9076/10000], Loss: 1.1120\n",
      "Epoch [9077/10000], Loss: 1.1119\n",
      "Epoch [9078/10000], Loss: 1.1117\n",
      "Epoch [9079/10000], Loss: 1.1116\n",
      "Epoch [9080/10000], Loss: 1.1115\n",
      "Epoch [9081/10000], Loss: 1.1114\n",
      "Epoch [9082/10000], Loss: 1.1112\n",
      "Epoch [9083/10000], Loss: 1.1111\n",
      "Epoch [9084/10000], Loss: 1.1110\n",
      "Epoch [9085/10000], Loss: 1.1109\n",
      "Epoch [9086/10000], Loss: 1.1107\n",
      "Epoch [9087/10000], Loss: 1.1106\n",
      "Epoch [9088/10000], Loss: 1.1105\n",
      "Epoch [9089/10000], Loss: 1.1103\n",
      "Epoch [9090/10000], Loss: 1.1102\n",
      "Epoch [9091/10000], Loss: 1.1101\n",
      "Epoch [9092/10000], Loss: 1.1100\n",
      "Epoch [9093/10000], Loss: 1.1098\n",
      "Epoch [9094/10000], Loss: 1.1097\n",
      "Epoch [9095/10000], Loss: 1.1096\n",
      "Epoch [9096/10000], Loss: 1.1095\n",
      "Epoch [9097/10000], Loss: 1.1093\n",
      "Epoch [9098/10000], Loss: 1.1092\n",
      "Epoch [9099/10000], Loss: 1.1091\n",
      "Epoch [9100/10000], Loss: 1.1090\n",
      "Epoch [9101/10000], Loss: 1.1088\n",
      "Epoch [9102/10000], Loss: 1.1087\n",
      "Epoch [9103/10000], Loss: 1.1086\n",
      "Epoch [9104/10000], Loss: 1.1084\n",
      "Epoch [9105/10000], Loss: 1.1083\n",
      "Epoch [9106/10000], Loss: 1.1082\n",
      "Epoch [9107/10000], Loss: 1.1081\n",
      "Epoch [9108/10000], Loss: 1.1079\n",
      "Epoch [9109/10000], Loss: 1.1078\n",
      "Epoch [9110/10000], Loss: 1.1077\n",
      "Epoch [9111/10000], Loss: 1.1076\n",
      "Epoch [9112/10000], Loss: 1.1074\n",
      "Epoch [9113/10000], Loss: 1.1073\n",
      "Epoch [9114/10000], Loss: 1.1072\n",
      "Epoch [9115/10000], Loss: 1.1071\n",
      "Epoch [9116/10000], Loss: 1.1069\n",
      "Epoch [9117/10000], Loss: 1.1068\n",
      "Epoch [9118/10000], Loss: 1.1067\n",
      "Epoch [9119/10000], Loss: 1.1065\n",
      "Epoch [9120/10000], Loss: 1.1064\n",
      "Epoch [9121/10000], Loss: 1.1063\n",
      "Epoch [9122/10000], Loss: 1.1062\n",
      "Epoch [9123/10000], Loss: 1.1060\n",
      "Epoch [9124/10000], Loss: 1.1059\n",
      "Epoch [9125/10000], Loss: 1.1058\n",
      "Epoch [9126/10000], Loss: 1.1057\n",
      "Epoch [9127/10000], Loss: 1.1055\n",
      "Epoch [9128/10000], Loss: 1.1054\n",
      "Epoch [9129/10000], Loss: 1.1053\n",
      "Epoch [9130/10000], Loss: 1.1052\n",
      "Epoch [9131/10000], Loss: 1.1050\n",
      "Epoch [9132/10000], Loss: 1.1049\n",
      "Epoch [9133/10000], Loss: 1.1048\n",
      "Epoch [9134/10000], Loss: 1.1046\n",
      "Epoch [9135/10000], Loss: 1.1045\n",
      "Epoch [9136/10000], Loss: 1.1044\n",
      "Epoch [9137/10000], Loss: 1.1043\n",
      "Epoch [9138/10000], Loss: 1.1041\n",
      "Epoch [9139/10000], Loss: 1.1040\n",
      "Epoch [9140/10000], Loss: 1.1039\n",
      "Epoch [9141/10000], Loss: 1.1038\n",
      "Epoch [9142/10000], Loss: 1.1036\n",
      "Epoch [9143/10000], Loss: 1.1035\n",
      "Epoch [9144/10000], Loss: 1.1034\n",
      "Epoch [9145/10000], Loss: 1.1033\n",
      "Epoch [9146/10000], Loss: 1.1031\n",
      "Epoch [9147/10000], Loss: 1.1030\n",
      "Epoch [9148/10000], Loss: 1.1029\n",
      "Epoch [9149/10000], Loss: 1.1027\n",
      "Epoch [9150/10000], Loss: 1.1026\n",
      "Epoch [9151/10000], Loss: 1.1025\n",
      "Epoch [9152/10000], Loss: 1.1024\n",
      "Epoch [9153/10000], Loss: 1.1022\n",
      "Epoch [9154/10000], Loss: 1.1021\n",
      "Epoch [9155/10000], Loss: 1.1020\n",
      "Epoch [9156/10000], Loss: 1.1019\n",
      "Epoch [9157/10000], Loss: 1.1017\n",
      "Epoch [9158/10000], Loss: 1.1016\n",
      "Epoch [9159/10000], Loss: 1.1015\n",
      "Epoch [9160/10000], Loss: 1.1014\n",
      "Epoch [9161/10000], Loss: 1.1012\n",
      "Epoch [9162/10000], Loss: 1.1011\n",
      "Epoch [9163/10000], Loss: 1.1010\n",
      "Epoch [9164/10000], Loss: 1.1008\n",
      "Epoch [9165/10000], Loss: 1.1007\n",
      "Epoch [9166/10000], Loss: 1.1006\n",
      "Epoch [9167/10000], Loss: 1.1005\n",
      "Epoch [9168/10000], Loss: 1.1003\n",
      "Epoch [9169/10000], Loss: 1.1002\n",
      "Epoch [9170/10000], Loss: 1.1001\n",
      "Epoch [9171/10000], Loss: 1.1000\n",
      "Epoch [9172/10000], Loss: 1.0998\n",
      "Epoch [9173/10000], Loss: 1.0997\n",
      "Epoch [9174/10000], Loss: 1.0996\n",
      "Epoch [9175/10000], Loss: 1.0995\n",
      "Epoch [9176/10000], Loss: 1.0993\n",
      "Epoch [9177/10000], Loss: 1.0992\n",
      "Epoch [9178/10000], Loss: 1.0991\n",
      "Epoch [9179/10000], Loss: 1.0989\n",
      "Epoch [9180/10000], Loss: 1.0988\n",
      "Epoch [9181/10000], Loss: 1.0987\n",
      "Epoch [9182/10000], Loss: 1.0986\n",
      "Epoch [9183/10000], Loss: 1.0984\n",
      "Epoch [9184/10000], Loss: 1.0983\n",
      "Epoch [9185/10000], Loss: 1.0982\n",
      "Epoch [9186/10000], Loss: 1.0981\n",
      "Epoch [9187/10000], Loss: 1.0979\n",
      "Epoch [9188/10000], Loss: 1.0978\n",
      "Epoch [9189/10000], Loss: 1.0977\n",
      "Epoch [9190/10000], Loss: 1.0976\n",
      "Epoch [9191/10000], Loss: 1.0974\n",
      "Epoch [9192/10000], Loss: 1.0973\n",
      "Epoch [9193/10000], Loss: 1.0972\n",
      "Epoch [9194/10000], Loss: 1.0970\n",
      "Epoch [9195/10000], Loss: 1.0969\n",
      "Epoch [9196/10000], Loss: 1.0968\n",
      "Epoch [9197/10000], Loss: 1.0967\n",
      "Epoch [9198/10000], Loss: 1.0965\n",
      "Epoch [9199/10000], Loss: 1.0964\n",
      "Epoch [9200/10000], Loss: 1.0963\n",
      "Epoch [9201/10000], Loss: 1.0962\n",
      "Epoch [9202/10000], Loss: 1.0960\n",
      "Epoch [9203/10000], Loss: 1.0959\n",
      "Epoch [9204/10000], Loss: 1.0958\n",
      "Epoch [9205/10000], Loss: 1.0957\n",
      "Epoch [9206/10000], Loss: 1.0955\n",
      "Epoch [9207/10000], Loss: 1.0954\n",
      "Epoch [9208/10000], Loss: 1.0953\n",
      "Epoch [9209/10000], Loss: 1.0951\n",
      "Epoch [9210/10000], Loss: 1.0950\n",
      "Epoch [9211/10000], Loss: 1.0949\n",
      "Epoch [9212/10000], Loss: 1.0948\n",
      "Epoch [9213/10000], Loss: 1.0946\n",
      "Epoch [9214/10000], Loss: 1.0945\n",
      "Epoch [9215/10000], Loss: 1.0944\n",
      "Epoch [9216/10000], Loss: 1.0943\n",
      "Epoch [9217/10000], Loss: 1.0941\n",
      "Epoch [9218/10000], Loss: 1.0940\n",
      "Epoch [9219/10000], Loss: 1.0939\n",
      "Epoch [9220/10000], Loss: 1.0938\n",
      "Epoch [9221/10000], Loss: 1.0936\n",
      "Epoch [9222/10000], Loss: 1.0935\n",
      "Epoch [9223/10000], Loss: 1.0934\n",
      "Epoch [9224/10000], Loss: 1.0933\n",
      "Epoch [9225/10000], Loss: 1.0931\n",
      "Epoch [9226/10000], Loss: 1.0930\n",
      "Epoch [9227/10000], Loss: 1.0929\n",
      "Epoch [9228/10000], Loss: 1.0927\n",
      "Epoch [9229/10000], Loss: 1.0926\n",
      "Epoch [9230/10000], Loss: 1.0925\n",
      "Epoch [9231/10000], Loss: 1.0924\n",
      "Epoch [9232/10000], Loss: 1.0922\n",
      "Epoch [9233/10000], Loss: 1.0921\n",
      "Epoch [9234/10000], Loss: 1.0920\n",
      "Epoch [9235/10000], Loss: 1.0919\n",
      "Epoch [9236/10000], Loss: 1.0917\n",
      "Epoch [9237/10000], Loss: 1.0916\n",
      "Epoch [9238/10000], Loss: 1.0915\n",
      "Epoch [9239/10000], Loss: 1.0914\n",
      "Epoch [9240/10000], Loss: 1.0912\n",
      "Epoch [9241/10000], Loss: 1.0911\n",
      "Epoch [9242/10000], Loss: 1.0910\n",
      "Epoch [9243/10000], Loss: 1.0908\n",
      "Epoch [9244/10000], Loss: 1.0907\n",
      "Epoch [9245/10000], Loss: 1.0906\n",
      "Epoch [9246/10000], Loss: 1.0905\n",
      "Epoch [9247/10000], Loss: 1.0903\n",
      "Epoch [9248/10000], Loss: 1.0902\n",
      "Epoch [9249/10000], Loss: 1.0901\n",
      "Epoch [9250/10000], Loss: 1.0900\n",
      "Epoch [9251/10000], Loss: 1.0898\n",
      "Epoch [9252/10000], Loss: 1.0897\n",
      "Epoch [9253/10000], Loss: 1.0896\n",
      "Epoch [9254/10000], Loss: 1.0895\n",
      "Epoch [9255/10000], Loss: 1.0893\n",
      "Epoch [9256/10000], Loss: 1.0892\n",
      "Epoch [9257/10000], Loss: 1.0891\n",
      "Epoch [9258/10000], Loss: 1.0889\n",
      "Epoch [9259/10000], Loss: 1.0888\n",
      "Epoch [9260/10000], Loss: 1.0887\n",
      "Epoch [9261/10000], Loss: 1.0886\n",
      "Epoch [9262/10000], Loss: 1.0884\n",
      "Epoch [9263/10000], Loss: 1.0883\n",
      "Epoch [9264/10000], Loss: 1.0882\n",
      "Epoch [9265/10000], Loss: 1.0881\n",
      "Epoch [9266/10000], Loss: 1.0879\n",
      "Epoch [9267/10000], Loss: 1.0878\n",
      "Epoch [9268/10000], Loss: 1.0877\n",
      "Epoch [9269/10000], Loss: 1.0876\n",
      "Epoch [9270/10000], Loss: 1.0874\n",
      "Epoch [9271/10000], Loss: 1.0873\n",
      "Epoch [9272/10000], Loss: 1.0872\n",
      "Epoch [9273/10000], Loss: 1.0870\n",
      "Epoch [9274/10000], Loss: 1.0869\n",
      "Epoch [9275/10000], Loss: 1.0868\n",
      "Epoch [9276/10000], Loss: 1.0867\n",
      "Epoch [9277/10000], Loss: 1.0865\n",
      "Epoch [9278/10000], Loss: 1.0864\n",
      "Epoch [9279/10000], Loss: 1.0863\n",
      "Epoch [9280/10000], Loss: 1.0862\n",
      "Epoch [9281/10000], Loss: 1.0860\n",
      "Epoch [9282/10000], Loss: 1.0859\n",
      "Epoch [9283/10000], Loss: 1.0858\n",
      "Epoch [9284/10000], Loss: 1.0857\n",
      "Epoch [9285/10000], Loss: 1.0855\n",
      "Epoch [9286/10000], Loss: 1.0854\n",
      "Epoch [9287/10000], Loss: 1.0853\n",
      "Epoch [9288/10000], Loss: 1.0851\n",
      "Epoch [9289/10000], Loss: 1.0850\n",
      "Epoch [9290/10000], Loss: 1.0849\n",
      "Epoch [9291/10000], Loss: 1.0848\n",
      "Epoch [9292/10000], Loss: 1.0846\n",
      "Epoch [9293/10000], Loss: 1.0845\n",
      "Epoch [9294/10000], Loss: 1.0844\n",
      "Epoch [9295/10000], Loss: 1.0843\n",
      "Epoch [9296/10000], Loss: 1.0841\n",
      "Epoch [9297/10000], Loss: 1.0840\n",
      "Epoch [9298/10000], Loss: 1.0839\n",
      "Epoch [9299/10000], Loss: 1.0838\n",
      "Epoch [9300/10000], Loss: 1.0836\n",
      "Epoch [9301/10000], Loss: 1.0835\n",
      "Epoch [9302/10000], Loss: 1.0834\n",
      "Epoch [9303/10000], Loss: 1.0833\n",
      "Epoch [9304/10000], Loss: 1.0831\n",
      "Epoch [9305/10000], Loss: 1.0830\n",
      "Epoch [9306/10000], Loss: 1.0829\n",
      "Epoch [9307/10000], Loss: 1.0827\n",
      "Epoch [9308/10000], Loss: 1.0826\n",
      "Epoch [9309/10000], Loss: 1.0825\n",
      "Epoch [9310/10000], Loss: 1.0824\n",
      "Epoch [9311/10000], Loss: 1.0822\n",
      "Epoch [9312/10000], Loss: 1.0821\n",
      "Epoch [9313/10000], Loss: 1.0820\n",
      "Epoch [9314/10000], Loss: 1.0819\n",
      "Epoch [9315/10000], Loss: 1.0817\n",
      "Epoch [9316/10000], Loss: 1.0816\n",
      "Epoch [9317/10000], Loss: 1.0815\n",
      "Epoch [9318/10000], Loss: 1.0814\n",
      "Epoch [9319/10000], Loss: 1.0812\n",
      "Epoch [9320/10000], Loss: 1.0811\n",
      "Epoch [9321/10000], Loss: 1.0810\n",
      "Epoch [9322/10000], Loss: 1.0808\n",
      "Epoch [9323/10000], Loss: 1.0807\n",
      "Epoch [9324/10000], Loss: 1.0806\n",
      "Epoch [9325/10000], Loss: 1.0805\n",
      "Epoch [9326/10000], Loss: 1.0803\n",
      "Epoch [9327/10000], Loss: 1.0802\n",
      "Epoch [9328/10000], Loss: 1.0801\n",
      "Epoch [9329/10000], Loss: 1.0800\n",
      "Epoch [9330/10000], Loss: 1.0798\n",
      "Epoch [9331/10000], Loss: 1.0797\n",
      "Epoch [9332/10000], Loss: 1.0796\n",
      "Epoch [9333/10000], Loss: 1.0795\n",
      "Epoch [9334/10000], Loss: 1.0793\n",
      "Epoch [9335/10000], Loss: 1.0792\n",
      "Epoch [9336/10000], Loss: 1.0791\n",
      "Epoch [9337/10000], Loss: 1.0789\n",
      "Epoch [9338/10000], Loss: 1.0788\n",
      "Epoch [9339/10000], Loss: 1.0787\n",
      "Epoch [9340/10000], Loss: 1.0786\n",
      "Epoch [9341/10000], Loss: 1.0784\n",
      "Epoch [9342/10000], Loss: 1.0783\n",
      "Epoch [9343/10000], Loss: 1.0782\n",
      "Epoch [9344/10000], Loss: 1.0781\n",
      "Epoch [9345/10000], Loss: 1.0779\n",
      "Epoch [9346/10000], Loss: 1.0778\n",
      "Epoch [9347/10000], Loss: 1.0777\n",
      "Epoch [9348/10000], Loss: 1.0776\n",
      "Epoch [9349/10000], Loss: 1.0774\n",
      "Epoch [9350/10000], Loss: 1.0773\n",
      "Epoch [9351/10000], Loss: 1.0772\n",
      "Epoch [9352/10000], Loss: 1.0771\n",
      "Epoch [9353/10000], Loss: 1.0769\n",
      "Epoch [9354/10000], Loss: 1.0768\n",
      "Epoch [9355/10000], Loss: 1.0767\n",
      "Epoch [9356/10000], Loss: 1.0765\n",
      "Epoch [9357/10000], Loss: 1.0764\n",
      "Epoch [9358/10000], Loss: 1.0763\n",
      "Epoch [9359/10000], Loss: 1.0762\n",
      "Epoch [9360/10000], Loss: 1.0760\n",
      "Epoch [9361/10000], Loss: 1.0759\n",
      "Epoch [9362/10000], Loss: 1.0758\n",
      "Epoch [9363/10000], Loss: 1.0757\n",
      "Epoch [9364/10000], Loss: 1.0755\n",
      "Epoch [9365/10000], Loss: 1.0754\n",
      "Epoch [9366/10000], Loss: 1.0753\n",
      "Epoch [9367/10000], Loss: 1.0752\n",
      "Epoch [9368/10000], Loss: 1.0750\n",
      "Epoch [9369/10000], Loss: 1.0749\n",
      "Epoch [9370/10000], Loss: 1.0748\n",
      "Epoch [9371/10000], Loss: 1.0746\n",
      "Epoch [9372/10000], Loss: 1.0745\n",
      "Epoch [9373/10000], Loss: 1.0744\n",
      "Epoch [9374/10000], Loss: 1.0743\n",
      "Epoch [9375/10000], Loss: 1.0741\n",
      "Epoch [9376/10000], Loss: 1.0740\n",
      "Epoch [9377/10000], Loss: 1.0739\n",
      "Epoch [9378/10000], Loss: 1.0738\n",
      "Epoch [9379/10000], Loss: 1.0736\n",
      "Epoch [9380/10000], Loss: 1.0735\n",
      "Epoch [9381/10000], Loss: 1.0734\n",
      "Epoch [9382/10000], Loss: 1.0733\n",
      "Epoch [9383/10000], Loss: 1.0731\n",
      "Epoch [9384/10000], Loss: 1.0730\n",
      "Epoch [9385/10000], Loss: 1.0729\n",
      "Epoch [9386/10000], Loss: 1.0728\n",
      "Epoch [9387/10000], Loss: 1.0726\n",
      "Epoch [9388/10000], Loss: 1.0725\n",
      "Epoch [9389/10000], Loss: 1.0724\n",
      "Epoch [9390/10000], Loss: 1.0722\n",
      "Epoch [9391/10000], Loss: 1.0721\n",
      "Epoch [9392/10000], Loss: 1.0720\n",
      "Epoch [9393/10000], Loss: 1.0719\n",
      "Epoch [9394/10000], Loss: 1.0717\n",
      "Epoch [9395/10000], Loss: 1.0716\n",
      "Epoch [9396/10000], Loss: 1.0715\n",
      "Epoch [9397/10000], Loss: 1.0714\n",
      "Epoch [9398/10000], Loss: 1.0712\n",
      "Epoch [9399/10000], Loss: 1.0711\n",
      "Epoch [9400/10000], Loss: 1.0710\n",
      "Epoch [9401/10000], Loss: 1.0709\n",
      "Epoch [9402/10000], Loss: 1.0707\n",
      "Epoch [9403/10000], Loss: 1.0706\n",
      "Epoch [9404/10000], Loss: 1.0705\n",
      "Epoch [9405/10000], Loss: 1.0703\n",
      "Epoch [9406/10000], Loss: 1.0702\n",
      "Epoch [9407/10000], Loss: 1.0701\n",
      "Epoch [9408/10000], Loss: 1.0700\n",
      "Epoch [9409/10000], Loss: 1.0698\n",
      "Epoch [9410/10000], Loss: 1.0697\n",
      "Epoch [9411/10000], Loss: 1.0696\n",
      "Epoch [9412/10000], Loss: 1.0695\n",
      "Epoch [9413/10000], Loss: 1.0693\n",
      "Epoch [9414/10000], Loss: 1.0692\n",
      "Epoch [9415/10000], Loss: 1.0691\n",
      "Epoch [9416/10000], Loss: 1.0690\n",
      "Epoch [9417/10000], Loss: 1.0688\n",
      "Epoch [9418/10000], Loss: 1.0687\n",
      "Epoch [9419/10000], Loss: 1.0686\n",
      "Epoch [9420/10000], Loss: 1.0685\n",
      "Epoch [9421/10000], Loss: 1.0683\n",
      "Epoch [9422/10000], Loss: 1.0682\n",
      "Epoch [9423/10000], Loss: 1.0681\n",
      "Epoch [9424/10000], Loss: 1.0679\n",
      "Epoch [9425/10000], Loss: 1.0678\n",
      "Epoch [9426/10000], Loss: 1.0677\n",
      "Epoch [9427/10000], Loss: 1.0676\n",
      "Epoch [9428/10000], Loss: 1.0674\n",
      "Epoch [9429/10000], Loss: 1.0673\n",
      "Epoch [9430/10000], Loss: 1.0672\n",
      "Epoch [9431/10000], Loss: 1.0671\n",
      "Epoch [9432/10000], Loss: 1.0669\n",
      "Epoch [9433/10000], Loss: 1.0668\n",
      "Epoch [9434/10000], Loss: 1.0667\n",
      "Epoch [9435/10000], Loss: 1.0666\n",
      "Epoch [9436/10000], Loss: 1.0664\n",
      "Epoch [9437/10000], Loss: 1.0663\n",
      "Epoch [9438/10000], Loss: 1.0662\n",
      "Epoch [9439/10000], Loss: 1.0661\n",
      "Epoch [9440/10000], Loss: 1.0659\n",
      "Epoch [9441/10000], Loss: 1.0658\n",
      "Epoch [9442/10000], Loss: 1.0657\n",
      "Epoch [9443/10000], Loss: 1.0655\n",
      "Epoch [9444/10000], Loss: 1.0654\n",
      "Epoch [9445/10000], Loss: 1.0653\n",
      "Epoch [9446/10000], Loss: 1.0652\n",
      "Epoch [9447/10000], Loss: 1.0650\n",
      "Epoch [9448/10000], Loss: 1.0649\n",
      "Epoch [9449/10000], Loss: 1.0648\n",
      "Epoch [9450/10000], Loss: 1.0647\n",
      "Epoch [9451/10000], Loss: 1.0645\n",
      "Epoch [9452/10000], Loss: 1.0644\n",
      "Epoch [9453/10000], Loss: 1.0643\n",
      "Epoch [9454/10000], Loss: 1.0642\n",
      "Epoch [9455/10000], Loss: 1.0640\n",
      "Epoch [9456/10000], Loss: 1.0639\n",
      "Epoch [9457/10000], Loss: 1.0638\n",
      "Epoch [9458/10000], Loss: 1.0636\n",
      "Epoch [9459/10000], Loss: 1.0635\n",
      "Epoch [9460/10000], Loss: 1.0634\n",
      "Epoch [9461/10000], Loss: 1.0633\n",
      "Epoch [9462/10000], Loss: 1.0631\n",
      "Epoch [9463/10000], Loss: 1.0630\n",
      "Epoch [9464/10000], Loss: 1.0629\n",
      "Epoch [9465/10000], Loss: 1.0628\n",
      "Epoch [9466/10000], Loss: 1.0626\n",
      "Epoch [9467/10000], Loss: 1.0625\n",
      "Epoch [9468/10000], Loss: 1.0624\n",
      "Epoch [9469/10000], Loss: 1.0623\n",
      "Epoch [9470/10000], Loss: 1.0621\n",
      "Epoch [9471/10000], Loss: 1.0620\n",
      "Epoch [9472/10000], Loss: 1.0619\n",
      "Epoch [9473/10000], Loss: 1.0618\n",
      "Epoch [9474/10000], Loss: 1.0616\n",
      "Epoch [9475/10000], Loss: 1.0615\n",
      "Epoch [9476/10000], Loss: 1.0614\n",
      "Epoch [9477/10000], Loss: 1.0612\n",
      "Epoch [9478/10000], Loss: 1.0611\n",
      "Epoch [9479/10000], Loss: 1.0610\n",
      "Epoch [9480/10000], Loss: 1.0609\n",
      "Epoch [9481/10000], Loss: 1.0607\n",
      "Epoch [9482/10000], Loss: 1.0606\n",
      "Epoch [9483/10000], Loss: 1.0605\n",
      "Epoch [9484/10000], Loss: 1.0604\n",
      "Epoch [9485/10000], Loss: 1.0602\n",
      "Epoch [9486/10000], Loss: 1.0601\n",
      "Epoch [9487/10000], Loss: 1.0600\n",
      "Epoch [9488/10000], Loss: 1.0599\n",
      "Epoch [9489/10000], Loss: 1.0597\n",
      "Epoch [9490/10000], Loss: 1.0596\n",
      "Epoch [9491/10000], Loss: 1.0595\n",
      "Epoch [9492/10000], Loss: 1.0594\n",
      "Epoch [9493/10000], Loss: 1.0592\n",
      "Epoch [9494/10000], Loss: 1.0591\n",
      "Epoch [9495/10000], Loss: 1.0590\n",
      "Epoch [9496/10000], Loss: 1.0588\n",
      "Epoch [9497/10000], Loss: 1.0587\n",
      "Epoch [9498/10000], Loss: 1.0586\n",
      "Epoch [9499/10000], Loss: 1.0585\n",
      "Epoch [9500/10000], Loss: 1.0583\n",
      "Epoch [9501/10000], Loss: 1.0582\n",
      "Epoch [9502/10000], Loss: 1.0581\n",
      "Epoch [9503/10000], Loss: 1.0580\n",
      "Epoch [9504/10000], Loss: 1.0578\n",
      "Epoch [9505/10000], Loss: 1.0577\n",
      "Epoch [9506/10000], Loss: 1.0576\n",
      "Epoch [9507/10000], Loss: 1.0575\n",
      "Epoch [9508/10000], Loss: 1.0573\n",
      "Epoch [9509/10000], Loss: 1.0572\n",
      "Epoch [9510/10000], Loss: 1.0571\n",
      "Epoch [9511/10000], Loss: 1.0569\n",
      "Epoch [9512/10000], Loss: 1.0568\n",
      "Epoch [9513/10000], Loss: 1.0567\n",
      "Epoch [9514/10000], Loss: 1.0566\n",
      "Epoch [9515/10000], Loss: 1.0564\n",
      "Epoch [9516/10000], Loss: 1.0563\n",
      "Epoch [9517/10000], Loss: 1.0562\n",
      "Epoch [9518/10000], Loss: 1.0561\n",
      "Epoch [9519/10000], Loss: 1.0559\n",
      "Epoch [9520/10000], Loss: 1.0558\n",
      "Epoch [9521/10000], Loss: 1.0557\n",
      "Epoch [9522/10000], Loss: 1.0556\n",
      "Epoch [9523/10000], Loss: 1.0554\n",
      "Epoch [9524/10000], Loss: 1.0553\n",
      "Epoch [9525/10000], Loss: 1.0552\n",
      "Epoch [9526/10000], Loss: 1.0551\n",
      "Epoch [9527/10000], Loss: 1.0549\n",
      "Epoch [9528/10000], Loss: 1.0548\n",
      "Epoch [9529/10000], Loss: 1.0547\n",
      "Epoch [9530/10000], Loss: 1.0545\n",
      "Epoch [9531/10000], Loss: 1.0544\n",
      "Epoch [9532/10000], Loss: 1.0543\n",
      "Epoch [9533/10000], Loss: 1.0542\n",
      "Epoch [9534/10000], Loss: 1.0540\n",
      "Epoch [9535/10000], Loss: 1.0539\n",
      "Epoch [9536/10000], Loss: 1.0538\n",
      "Epoch [9537/10000], Loss: 1.0537\n",
      "Epoch [9538/10000], Loss: 1.0535\n",
      "Epoch [9539/10000], Loss: 1.0534\n",
      "Epoch [9540/10000], Loss: 1.0533\n",
      "Epoch [9541/10000], Loss: 1.0532\n",
      "Epoch [9542/10000], Loss: 1.0530\n",
      "Epoch [9543/10000], Loss: 1.0529\n",
      "Epoch [9544/10000], Loss: 1.0528\n",
      "Epoch [9545/10000], Loss: 1.0527\n",
      "Epoch [9546/10000], Loss: 1.0525\n",
      "Epoch [9547/10000], Loss: 1.0524\n",
      "Epoch [9548/10000], Loss: 1.0523\n",
      "Epoch [9549/10000], Loss: 1.0521\n",
      "Epoch [9550/10000], Loss: 1.0520\n",
      "Epoch [9551/10000], Loss: 1.0519\n",
      "Epoch [9552/10000], Loss: 1.0518\n",
      "Epoch [9553/10000], Loss: 1.0516\n",
      "Epoch [9554/10000], Loss: 1.0515\n",
      "Epoch [9555/10000], Loss: 1.0514\n",
      "Epoch [9556/10000], Loss: 1.0513\n",
      "Epoch [9557/10000], Loss: 1.0511\n",
      "Epoch [9558/10000], Loss: 1.0510\n",
      "Epoch [9559/10000], Loss: 1.0509\n",
      "Epoch [9560/10000], Loss: 1.0508\n",
      "Epoch [9561/10000], Loss: 1.0506\n",
      "Epoch [9562/10000], Loss: 1.0505\n",
      "Epoch [9563/10000], Loss: 1.0504\n",
      "Epoch [9564/10000], Loss: 1.0503\n",
      "Epoch [9565/10000], Loss: 1.0501\n",
      "Epoch [9566/10000], Loss: 1.0500\n",
      "Epoch [9567/10000], Loss: 1.0499\n",
      "Epoch [9568/10000], Loss: 1.0497\n",
      "Epoch [9569/10000], Loss: 1.0496\n",
      "Epoch [9570/10000], Loss: 1.0495\n",
      "Epoch [9571/10000], Loss: 1.0494\n",
      "Epoch [9572/10000], Loss: 1.0492\n",
      "Epoch [9573/10000], Loss: 1.0491\n",
      "Epoch [9574/10000], Loss: 1.0490\n",
      "Epoch [9575/10000], Loss: 1.0489\n",
      "Epoch [9576/10000], Loss: 1.0487\n",
      "Epoch [9577/10000], Loss: 1.0486\n",
      "Epoch [9578/10000], Loss: 1.0485\n",
      "Epoch [9579/10000], Loss: 1.0484\n",
      "Epoch [9580/10000], Loss: 1.0482\n",
      "Epoch [9581/10000], Loss: 1.0481\n",
      "Epoch [9582/10000], Loss: 1.0480\n",
      "Epoch [9583/10000], Loss: 1.0479\n",
      "Epoch [9584/10000], Loss: 1.0477\n",
      "Epoch [9585/10000], Loss: 1.0476\n",
      "Epoch [9586/10000], Loss: 1.0475\n",
      "Epoch [9587/10000], Loss: 1.0473\n",
      "Epoch [9588/10000], Loss: 1.0472\n",
      "Epoch [9589/10000], Loss: 1.0471\n",
      "Epoch [9590/10000], Loss: 1.0470\n",
      "Epoch [9591/10000], Loss: 1.0468\n",
      "Epoch [9592/10000], Loss: 1.0467\n",
      "Epoch [9593/10000], Loss: 1.0466\n",
      "Epoch [9594/10000], Loss: 1.0465\n",
      "Epoch [9595/10000], Loss: 1.0463\n",
      "Epoch [9596/10000], Loss: 1.0462\n",
      "Epoch [9597/10000], Loss: 1.0461\n",
      "Epoch [9598/10000], Loss: 1.0460\n",
      "Epoch [9599/10000], Loss: 1.0458\n",
      "Epoch [9600/10000], Loss: 1.0457\n",
      "Epoch [9601/10000], Loss: 1.0456\n",
      "Epoch [9602/10000], Loss: 1.0455\n",
      "Epoch [9603/10000], Loss: 1.0453\n",
      "Epoch [9604/10000], Loss: 1.0452\n",
      "Epoch [9605/10000], Loss: 1.0451\n",
      "Epoch [9606/10000], Loss: 1.0449\n",
      "Epoch [9607/10000], Loss: 1.0448\n",
      "Epoch [9608/10000], Loss: 1.0447\n",
      "Epoch [9609/10000], Loss: 1.0446\n",
      "Epoch [9610/10000], Loss: 1.0444\n",
      "Epoch [9611/10000], Loss: 1.0443\n",
      "Epoch [9612/10000], Loss: 1.0442\n",
      "Epoch [9613/10000], Loss: 1.0441\n",
      "Epoch [9614/10000], Loss: 1.0439\n",
      "Epoch [9615/10000], Loss: 1.0438\n",
      "Epoch [9616/10000], Loss: 1.0437\n",
      "Epoch [9617/10000], Loss: 1.0436\n",
      "Epoch [9618/10000], Loss: 1.0434\n",
      "Epoch [9619/10000], Loss: 1.0433\n",
      "Epoch [9620/10000], Loss: 1.0432\n",
      "Epoch [9621/10000], Loss: 1.0431\n",
      "Epoch [9622/10000], Loss: 1.0429\n",
      "Epoch [9623/10000], Loss: 1.0428\n",
      "Epoch [9624/10000], Loss: 1.0427\n",
      "Epoch [9625/10000], Loss: 1.0425\n",
      "Epoch [9626/10000], Loss: 1.0424\n",
      "Epoch [9627/10000], Loss: 1.0423\n",
      "Epoch [9628/10000], Loss: 1.0422\n",
      "Epoch [9629/10000], Loss: 1.0420\n",
      "Epoch [9630/10000], Loss: 1.0419\n",
      "Epoch [9631/10000], Loss: 1.0418\n",
      "Epoch [9632/10000], Loss: 1.0417\n",
      "Epoch [9633/10000], Loss: 1.0415\n",
      "Epoch [9634/10000], Loss: 1.0414\n",
      "Epoch [9635/10000], Loss: 1.0413\n",
      "Epoch [9636/10000], Loss: 1.0412\n",
      "Epoch [9637/10000], Loss: 1.0410\n",
      "Epoch [9638/10000], Loss: 1.0409\n",
      "Epoch [9639/10000], Loss: 1.0408\n",
      "Epoch [9640/10000], Loss: 1.0407\n",
      "Epoch [9641/10000], Loss: 1.0405\n",
      "Epoch [9642/10000], Loss: 1.0404\n",
      "Epoch [9643/10000], Loss: 1.0403\n",
      "Epoch [9644/10000], Loss: 1.0401\n",
      "Epoch [9645/10000], Loss: 1.0400\n",
      "Epoch [9646/10000], Loss: 1.0399\n",
      "Epoch [9647/10000], Loss: 1.0398\n",
      "Epoch [9648/10000], Loss: 1.0396\n",
      "Epoch [9649/10000], Loss: 1.0395\n",
      "Epoch [9650/10000], Loss: 1.0394\n",
      "Epoch [9651/10000], Loss: 1.0393\n",
      "Epoch [9652/10000], Loss: 1.0391\n",
      "Epoch [9653/10000], Loss: 1.0390\n",
      "Epoch [9654/10000], Loss: 1.0389\n",
      "Epoch [9655/10000], Loss: 1.0388\n",
      "Epoch [9656/10000], Loss: 1.0386\n",
      "Epoch [9657/10000], Loss: 1.0385\n",
      "Epoch [9658/10000], Loss: 1.0384\n",
      "Epoch [9659/10000], Loss: 1.0383\n",
      "Epoch [9660/10000], Loss: 1.0381\n",
      "Epoch [9661/10000], Loss: 1.0380\n",
      "Epoch [9662/10000], Loss: 1.0379\n",
      "Epoch [9663/10000], Loss: 1.0377\n",
      "Epoch [9664/10000], Loss: 1.0376\n",
      "Epoch [9665/10000], Loss: 1.0375\n",
      "Epoch [9666/10000], Loss: 1.0374\n",
      "Epoch [9667/10000], Loss: 1.0372\n",
      "Epoch [9668/10000], Loss: 1.0371\n",
      "Epoch [9669/10000], Loss: 1.0370\n",
      "Epoch [9670/10000], Loss: 1.0369\n",
      "Epoch [9671/10000], Loss: 1.0367\n",
      "Epoch [9672/10000], Loss: 1.0366\n",
      "Epoch [9673/10000], Loss: 1.0365\n",
      "Epoch [9674/10000], Loss: 1.0364\n",
      "Epoch [9675/10000], Loss: 1.0362\n",
      "Epoch [9676/10000], Loss: 1.0361\n",
      "Epoch [9677/10000], Loss: 1.0360\n",
      "Epoch [9678/10000], Loss: 1.0359\n",
      "Epoch [9679/10000], Loss: 1.0357\n",
      "Epoch [9680/10000], Loss: 1.0356\n",
      "Epoch [9681/10000], Loss: 1.0355\n",
      "Epoch [9682/10000], Loss: 1.0353\n",
      "Epoch [9683/10000], Loss: 1.0352\n",
      "Epoch [9684/10000], Loss: 1.0351\n",
      "Epoch [9685/10000], Loss: 1.0350\n",
      "Epoch [9686/10000], Loss: 1.0348\n",
      "Epoch [9687/10000], Loss: 1.0347\n",
      "Epoch [9688/10000], Loss: 1.0346\n",
      "Epoch [9689/10000], Loss: 1.0345\n",
      "Epoch [9690/10000], Loss: 1.0343\n",
      "Epoch [9691/10000], Loss: 1.0342\n",
      "Epoch [9692/10000], Loss: 1.0341\n",
      "Epoch [9693/10000], Loss: 1.0340\n",
      "Epoch [9694/10000], Loss: 1.0338\n",
      "Epoch [9695/10000], Loss: 1.0337\n",
      "Epoch [9696/10000], Loss: 1.0336\n",
      "Epoch [9697/10000], Loss: 1.0335\n",
      "Epoch [9698/10000], Loss: 1.0333\n",
      "Epoch [9699/10000], Loss: 1.0332\n",
      "Epoch [9700/10000], Loss: 1.0331\n",
      "Epoch [9701/10000], Loss: 1.0330\n",
      "Epoch [9702/10000], Loss: 1.0328\n",
      "Epoch [9703/10000], Loss: 1.0327\n",
      "Epoch [9704/10000], Loss: 1.0326\n",
      "Epoch [9705/10000], Loss: 1.0324\n",
      "Epoch [9706/10000], Loss: 1.0323\n",
      "Epoch [9707/10000], Loss: 1.0322\n",
      "Epoch [9708/10000], Loss: 1.0321\n",
      "Epoch [9709/10000], Loss: 1.0319\n",
      "Epoch [9710/10000], Loss: 1.0318\n",
      "Epoch [9711/10000], Loss: 1.0317\n",
      "Epoch [9712/10000], Loss: 1.0316\n",
      "Epoch [9713/10000], Loss: 1.0314\n",
      "Epoch [9714/10000], Loss: 1.0313\n",
      "Epoch [9715/10000], Loss: 1.0312\n",
      "Epoch [9716/10000], Loss: 1.0311\n",
      "Epoch [9717/10000], Loss: 1.0309\n",
      "Epoch [9718/10000], Loss: 1.0308\n",
      "Epoch [9719/10000], Loss: 1.0307\n",
      "Epoch [9720/10000], Loss: 1.0306\n",
      "Epoch [9721/10000], Loss: 1.0304\n",
      "Epoch [9722/10000], Loss: 1.0303\n",
      "Epoch [9723/10000], Loss: 1.0302\n",
      "Epoch [9724/10000], Loss: 1.0300\n",
      "Epoch [9725/10000], Loss: 1.0299\n",
      "Epoch [9726/10000], Loss: 1.0298\n",
      "Epoch [9727/10000], Loss: 1.0297\n",
      "Epoch [9728/10000], Loss: 1.0295\n",
      "Epoch [9729/10000], Loss: 1.0294\n",
      "Epoch [9730/10000], Loss: 1.0293\n",
      "Epoch [9731/10000], Loss: 1.0292\n",
      "Epoch [9732/10000], Loss: 1.0290\n",
      "Epoch [9733/10000], Loss: 1.0289\n",
      "Epoch [9734/10000], Loss: 1.0288\n",
      "Epoch [9735/10000], Loss: 1.0287\n",
      "Epoch [9736/10000], Loss: 1.0285\n",
      "Epoch [9737/10000], Loss: 1.0284\n",
      "Epoch [9738/10000], Loss: 1.0283\n",
      "Epoch [9739/10000], Loss: 1.0282\n",
      "Epoch [9740/10000], Loss: 1.0280\n",
      "Epoch [9741/10000], Loss: 1.0279\n",
      "Epoch [9742/10000], Loss: 1.0278\n",
      "Epoch [9743/10000], Loss: 1.0276\n",
      "Epoch [9744/10000], Loss: 1.0275\n",
      "Epoch [9745/10000], Loss: 1.0274\n",
      "Epoch [9746/10000], Loss: 1.0273\n",
      "Epoch [9747/10000], Loss: 1.0271\n",
      "Epoch [9748/10000], Loss: 1.0270\n",
      "Epoch [9749/10000], Loss: 1.0269\n",
      "Epoch [9750/10000], Loss: 1.0268\n",
      "Epoch [9751/10000], Loss: 1.0266\n",
      "Epoch [9752/10000], Loss: 1.0265\n",
      "Epoch [9753/10000], Loss: 1.0264\n",
      "Epoch [9754/10000], Loss: 1.0263\n",
      "Epoch [9755/10000], Loss: 1.0261\n",
      "Epoch [9756/10000], Loss: 1.0260\n",
      "Epoch [9757/10000], Loss: 1.0259\n",
      "Epoch [9758/10000], Loss: 1.0258\n",
      "Epoch [9759/10000], Loss: 1.0256\n",
      "Epoch [9760/10000], Loss: 1.0255\n",
      "Epoch [9761/10000], Loss: 1.0254\n",
      "Epoch [9762/10000], Loss: 1.0253\n",
      "Epoch [9763/10000], Loss: 1.0251\n",
      "Epoch [9764/10000], Loss: 1.0250\n",
      "Epoch [9765/10000], Loss: 1.0249\n",
      "Epoch [9766/10000], Loss: 1.0247\n",
      "Epoch [9767/10000], Loss: 1.0246\n",
      "Epoch [9768/10000], Loss: 1.0245\n",
      "Epoch [9769/10000], Loss: 1.0244\n",
      "Epoch [9770/10000], Loss: 1.0242\n",
      "Epoch [9771/10000], Loss: 1.0241\n",
      "Epoch [9772/10000], Loss: 1.0240\n",
      "Epoch [9773/10000], Loss: 1.0239\n",
      "Epoch [9774/10000], Loss: 1.0237\n",
      "Epoch [9775/10000], Loss: 1.0236\n",
      "Epoch [9776/10000], Loss: 1.0235\n",
      "Epoch [9777/10000], Loss: 1.0234\n",
      "Epoch [9778/10000], Loss: 1.0232\n",
      "Epoch [9779/10000], Loss: 1.0231\n",
      "Epoch [9780/10000], Loss: 1.0230\n",
      "Epoch [9781/10000], Loss: 1.0229\n",
      "Epoch [9782/10000], Loss: 1.0227\n",
      "Epoch [9783/10000], Loss: 1.0226\n",
      "Epoch [9784/10000], Loss: 1.0225\n",
      "Epoch [9785/10000], Loss: 1.0223\n",
      "Epoch [9786/10000], Loss: 1.0222\n",
      "Epoch [9787/10000], Loss: 1.0221\n",
      "Epoch [9788/10000], Loss: 1.0220\n",
      "Epoch [9789/10000], Loss: 1.0218\n",
      "Epoch [9790/10000], Loss: 1.0217\n",
      "Epoch [9791/10000], Loss: 1.0216\n",
      "Epoch [9792/10000], Loss: 1.0215\n",
      "Epoch [9793/10000], Loss: 1.0213\n",
      "Epoch [9794/10000], Loss: 1.0212\n",
      "Epoch [9795/10000], Loss: 1.0211\n",
      "Epoch [9796/10000], Loss: 1.0210\n",
      "Epoch [9797/10000], Loss: 1.0208\n",
      "Epoch [9798/10000], Loss: 1.0207\n",
      "Epoch [9799/10000], Loss: 1.0206\n",
      "Epoch [9800/10000], Loss: 1.0205\n",
      "Epoch [9801/10000], Loss: 1.0203\n",
      "Epoch [9802/10000], Loss: 1.0202\n",
      "Epoch [9803/10000], Loss: 1.0201\n",
      "Epoch [9804/10000], Loss: 1.0200\n",
      "Epoch [9805/10000], Loss: 1.0198\n",
      "Epoch [9806/10000], Loss: 1.0197\n",
      "Epoch [9807/10000], Loss: 1.0196\n",
      "Epoch [9808/10000], Loss: 1.0194\n",
      "Epoch [9809/10000], Loss: 1.0193\n",
      "Epoch [9810/10000], Loss: 1.0192\n",
      "Epoch [9811/10000], Loss: 1.0191\n",
      "Epoch [9812/10000], Loss: 1.0189\n",
      "Epoch [9813/10000], Loss: 1.0188\n",
      "Epoch [9814/10000], Loss: 1.0187\n",
      "Epoch [9815/10000], Loss: 1.0186\n",
      "Epoch [9816/10000], Loss: 1.0184\n",
      "Epoch [9817/10000], Loss: 1.0183\n",
      "Epoch [9818/10000], Loss: 1.0182\n",
      "Epoch [9819/10000], Loss: 1.0181\n",
      "Epoch [9820/10000], Loss: 1.0179\n",
      "Epoch [9821/10000], Loss: 1.0178\n",
      "Epoch [9822/10000], Loss: 1.0177\n",
      "Epoch [9823/10000], Loss: 1.0176\n",
      "Epoch [9824/10000], Loss: 1.0174\n",
      "Epoch [9825/10000], Loss: 1.0173\n",
      "Epoch [9826/10000], Loss: 1.0172\n",
      "Epoch [9827/10000], Loss: 1.0170\n",
      "Epoch [9828/10000], Loss: 1.0169\n",
      "Epoch [9829/10000], Loss: 1.0168\n",
      "Epoch [9830/10000], Loss: 1.0167\n",
      "Epoch [9831/10000], Loss: 1.0165\n",
      "Epoch [9832/10000], Loss: 1.0164\n",
      "Epoch [9833/10000], Loss: 1.0163\n",
      "Epoch [9834/10000], Loss: 1.0162\n",
      "Epoch [9835/10000], Loss: 1.0160\n",
      "Epoch [9836/10000], Loss: 1.0159\n",
      "Epoch [9837/10000], Loss: 1.0158\n",
      "Epoch [9838/10000], Loss: 1.0157\n",
      "Epoch [9839/10000], Loss: 1.0155\n",
      "Epoch [9840/10000], Loss: 1.0154\n",
      "Epoch [9841/10000], Loss: 1.0153\n",
      "Epoch [9842/10000], Loss: 1.0152\n",
      "Epoch [9843/10000], Loss: 1.0150\n",
      "Epoch [9844/10000], Loss: 1.0149\n",
      "Epoch [9845/10000], Loss: 1.0148\n",
      "Epoch [9846/10000], Loss: 1.0147\n",
      "Epoch [9847/10000], Loss: 1.0145\n",
      "Epoch [9848/10000], Loss: 1.0144\n",
      "Epoch [9849/10000], Loss: 1.0143\n",
      "Epoch [9850/10000], Loss: 1.0141\n",
      "Epoch [9851/10000], Loss: 1.0140\n",
      "Epoch [9852/10000], Loss: 1.0139\n",
      "Epoch [9853/10000], Loss: 1.0138\n",
      "Epoch [9854/10000], Loss: 1.0136\n",
      "Epoch [9855/10000], Loss: 1.0135\n",
      "Epoch [9856/10000], Loss: 1.0134\n",
      "Epoch [9857/10000], Loss: 1.0133\n",
      "Epoch [9858/10000], Loss: 1.0131\n",
      "Epoch [9859/10000], Loss: 1.0130\n",
      "Epoch [9860/10000], Loss: 1.0129\n",
      "Epoch [9861/10000], Loss: 1.0128\n",
      "Epoch [9862/10000], Loss: 1.0126\n",
      "Epoch [9863/10000], Loss: 1.0125\n",
      "Epoch [9864/10000], Loss: 1.0124\n",
      "Epoch [9865/10000], Loss: 1.0123\n",
      "Epoch [9866/10000], Loss: 1.0121\n",
      "Epoch [9867/10000], Loss: 1.0120\n",
      "Epoch [9868/10000], Loss: 1.0119\n",
      "Epoch [9869/10000], Loss: 1.0118\n",
      "Epoch [9870/10000], Loss: 1.0116\n",
      "Epoch [9871/10000], Loss: 1.0115\n",
      "Epoch [9872/10000], Loss: 1.0114\n",
      "Epoch [9873/10000], Loss: 1.0112\n",
      "Epoch [9874/10000], Loss: 1.0111\n",
      "Epoch [9875/10000], Loss: 1.0110\n",
      "Epoch [9876/10000], Loss: 1.0109\n",
      "Epoch [9877/10000], Loss: 1.0107\n",
      "Epoch [9878/10000], Loss: 1.0106\n",
      "Epoch [9879/10000], Loss: 1.0105\n",
      "Epoch [9880/10000], Loss: 1.0104\n",
      "Epoch [9881/10000], Loss: 1.0102\n",
      "Epoch [9882/10000], Loss: 1.0101\n",
      "Epoch [9883/10000], Loss: 1.0100\n",
      "Epoch [9884/10000], Loss: 1.0099\n",
      "Epoch [9885/10000], Loss: 1.0097\n",
      "Epoch [9886/10000], Loss: 1.0096\n",
      "Epoch [9887/10000], Loss: 1.0095\n",
      "Epoch [9888/10000], Loss: 1.0094\n",
      "Epoch [9889/10000], Loss: 1.0092\n",
      "Epoch [9890/10000], Loss: 1.0091\n",
      "Epoch [9891/10000], Loss: 1.0090\n",
      "Epoch [9892/10000], Loss: 1.0088\n",
      "Epoch [9893/10000], Loss: 1.0087\n",
      "Epoch [9894/10000], Loss: 1.0086\n",
      "Epoch [9895/10000], Loss: 1.0085\n",
      "Epoch [9896/10000], Loss: 1.0083\n",
      "Epoch [9897/10000], Loss: 1.0082\n",
      "Epoch [9898/10000], Loss: 1.0081\n",
      "Epoch [9899/10000], Loss: 1.0080\n",
      "Epoch [9900/10000], Loss: 1.0078\n",
      "Epoch [9901/10000], Loss: 1.0077\n",
      "Epoch [9902/10000], Loss: 1.0076\n",
      "Epoch [9903/10000], Loss: 1.0075\n",
      "Epoch [9904/10000], Loss: 1.0073\n",
      "Epoch [9905/10000], Loss: 1.0072\n",
      "Epoch [9906/10000], Loss: 1.0071\n",
      "Epoch [9907/10000], Loss: 1.0070\n",
      "Epoch [9908/10000], Loss: 1.0068\n",
      "Epoch [9909/10000], Loss: 1.0067\n",
      "Epoch [9910/10000], Loss: 1.0066\n",
      "Epoch [9911/10000], Loss: 1.0065\n",
      "Epoch [9912/10000], Loss: 1.0063\n",
      "Epoch [9913/10000], Loss: 1.0062\n",
      "Epoch [9914/10000], Loss: 1.0061\n",
      "Epoch [9915/10000], Loss: 1.0059\n",
      "Epoch [9916/10000], Loss: 1.0058\n",
      "Epoch [9917/10000], Loss: 1.0057\n",
      "Epoch [9918/10000], Loss: 1.0056\n",
      "Epoch [9919/10000], Loss: 1.0054\n",
      "Epoch [9920/10000], Loss: 1.0053\n",
      "Epoch [9921/10000], Loss: 1.0052\n",
      "Epoch [9922/10000], Loss: 1.0051\n",
      "Epoch [9923/10000], Loss: 1.0049\n",
      "Epoch [9924/10000], Loss: 1.0048\n",
      "Epoch [9925/10000], Loss: 1.0047\n",
      "Epoch [9926/10000], Loss: 1.0046\n",
      "Epoch [9927/10000], Loss: 1.0044\n",
      "Epoch [9928/10000], Loss: 1.0043\n",
      "Epoch [9929/10000], Loss: 1.0042\n",
      "Epoch [9930/10000], Loss: 1.0041\n",
      "Epoch [9931/10000], Loss: 1.0039\n",
      "Epoch [9932/10000], Loss: 1.0038\n",
      "Epoch [9933/10000], Loss: 1.0037\n",
      "Epoch [9934/10000], Loss: 1.0036\n",
      "Epoch [9935/10000], Loss: 1.0034\n",
      "Epoch [9936/10000], Loss: 1.0033\n",
      "Epoch [9937/10000], Loss: 1.0032\n",
      "Epoch [9938/10000], Loss: 1.0030\n",
      "Epoch [9939/10000], Loss: 1.0029\n",
      "Epoch [9940/10000], Loss: 1.0028\n",
      "Epoch [9941/10000], Loss: 1.0027\n",
      "Epoch [9942/10000], Loss: 1.0025\n",
      "Epoch [9943/10000], Loss: 1.0024\n",
      "Epoch [9944/10000], Loss: 1.0023\n",
      "Epoch [9945/10000], Loss: 1.0022\n",
      "Epoch [9946/10000], Loss: 1.0020\n",
      "Epoch [9947/10000], Loss: 1.0019\n",
      "Epoch [9948/10000], Loss: 1.0018\n",
      "Epoch [9949/10000], Loss: 1.0017\n",
      "Epoch [9950/10000], Loss: 1.0015\n",
      "Epoch [9951/10000], Loss: 1.0014\n",
      "Epoch [9952/10000], Loss: 1.0013\n",
      "Epoch [9953/10000], Loss: 1.0012\n",
      "Epoch [9954/10000], Loss: 1.0010\n",
      "Epoch [9955/10000], Loss: 1.0009\n",
      "Epoch [9956/10000], Loss: 1.0008\n",
      "Epoch [9957/10000], Loss: 1.0007\n",
      "Epoch [9958/10000], Loss: 1.0005\n",
      "Epoch [9959/10000], Loss: 1.0004\n",
      "Epoch [9960/10000], Loss: 1.0003\n",
      "Epoch [9961/10000], Loss: 1.0001\n",
      "Epoch [9962/10000], Loss: 1.0000\n",
      "Epoch [9963/10000], Loss: 0.9999\n",
      "Epoch [9964/10000], Loss: 0.9998\n",
      "Epoch [9965/10000], Loss: 0.9996\n",
      "Epoch [9966/10000], Loss: 0.9995\n",
      "Epoch [9967/10000], Loss: 0.9994\n",
      "Epoch [9968/10000], Loss: 0.9993\n",
      "Epoch [9969/10000], Loss: 0.9991\n",
      "Epoch [9970/10000], Loss: 0.9990\n",
      "Epoch [9971/10000], Loss: 0.9989\n",
      "Epoch [9972/10000], Loss: 0.9988\n",
      "Epoch [9973/10000], Loss: 0.9986\n",
      "Epoch [9974/10000], Loss: 0.9985\n",
      "Epoch [9975/10000], Loss: 0.9984\n",
      "Epoch [9976/10000], Loss: 0.9983\n",
      "Epoch [9977/10000], Loss: 0.9981\n",
      "Epoch [9978/10000], Loss: 0.9980\n",
      "Epoch [9979/10000], Loss: 0.9979\n",
      "Epoch [9980/10000], Loss: 0.9978\n",
      "Epoch [9981/10000], Loss: 0.9976\n",
      "Epoch [9982/10000], Loss: 0.9975\n",
      "Epoch [9983/10000], Loss: 0.9974\n",
      "Epoch [9984/10000], Loss: 0.9972\n",
      "Epoch [9985/10000], Loss: 0.9971\n",
      "Epoch [9986/10000], Loss: 0.9970\n",
      "Epoch [9987/10000], Loss: 0.9969\n",
      "Epoch [9988/10000], Loss: 0.9967\n",
      "Epoch [9989/10000], Loss: 0.9966\n",
      "Epoch [9990/10000], Loss: 0.9965\n",
      "Epoch [9991/10000], Loss: 0.9964\n",
      "Epoch [9992/10000], Loss: 0.9962\n",
      "Epoch [9993/10000], Loss: 0.9961\n",
      "Epoch [9994/10000], Loss: 0.9960\n",
      "Epoch [9995/10000], Loss: 0.9959\n",
      "Epoch [9996/10000], Loss: 0.9957\n",
      "Epoch [9997/10000], Loss: 0.9956\n",
      "Epoch [9998/10000], Loss: 0.9955\n",
      "Epoch [9999/10000], Loss: 0.9954\n",
      "Epoch [10000/10000], Loss: 0.9952\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loss function đã cho\n",
    "def compute_loss_Task(pi_star, y_true):\n",
    "    loss_Task = 0.0\n",
    "    batch_size = len(pi_star)\n",
    "    for i in range(batch_size):\n",
    "        # Chỉ lấy xác suất tương ứng với nhãn đúng\n",
    "        sentence_loss = -torch.log(pi_star[i, y_true[i]])\n",
    "        loss_Task += sentence_loss\n",
    "    return loss_Task / batch_size\n",
    "\n",
    "\n",
    "# Khởi tạo các tham số\n",
    "batch_size = 4\n",
    "output_size = 10  # số lớp (số nhãn)\n",
    "learning_rate = 0.001\n",
    "epochs = 10000\n",
    "\n",
    "# Tạo tensor pi_star (có thể tính gradient) và y_true\n",
    "# pi_star = torch.randn(batch_size, output_size, requires_grad=True)  # Các xác suất ngẫu nhiên\n",
    "# y_true = torch.randint(0, output_size, (batch_size,))  # Các nhãn ngẫu nhiên\n",
    "\n",
    "# Optimizer để tối ưu hóa pi_star\n",
    "optimizer = optim.Adam([pi_star], lr=learning_rate)\n",
    "\n",
    "# Huấn luyện\n",
    "for epoch in range(epochs):\n",
    "    pi_star.data = torch.clamp(pi_star.data, min=1e-6, max=1-1e-6)  # Đảm bảo pi_star trong khoảng [1e-6, 1-1e-6]\n",
    "    \n",
    "    optimizer.zero_grad()  # Đặt gradient về 0\n",
    "    \n",
    "    # Tính loss\n",
    "    loss = compute_loss_Task(pi_star, y_true)\n",
    "    \n",
    "    # Tính gradient và cập nhật\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # In ra loss mỗi epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 4, 9, 4])"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010],\n",
       "        [1.0010],\n",
       "        [1.1000],\n",
       "        [0.1367]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star.gather(1,y_true.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_star.data[2][4] = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-06, 1.0000e+00, 1.0000e+00, 4.2360e-01, 1.0000e-06, 1.0000e-06,\n",
       "         1.0010e+00, 1.0000e-06, 4.6763e-01, 3.6268e-01],\n",
       "        [1.0000e-06, 1.0000e-06, 1.0000e-06, 3.9432e-01, 1.0010e+00, 1.0000e+00,\n",
       "         1.0000e-06, 4.5628e-01, 7.8573e-01, 1.0000e-06],\n",
       "        [1.0000e-06, 1.0000e+00, 1.0000e+00, 1.9780e-01, 1.1000e+00, 1.0000e-06,\n",
       "         1.0000e-06, 1.0000e-06, 1.5413e-01, 1.1000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e-06, 1.0000e-06, 1.3667e-01, 1.0000e-06,\n",
       "         1.0000e-06, 1.0000e-06, 8.6441e-01, 3.7974e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
