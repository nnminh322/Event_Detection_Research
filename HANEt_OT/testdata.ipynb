{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "args_perm_id = 0\n",
    "args_task_num = 5\n",
    "args_class_num = 10\n",
    "args_shot_num = 5\n",
    "def collect_from_json(dataset, root, split):\n",
    "    if split == \"train\":\n",
    "        pth = os.path.join(\n",
    "            root,\n",
    "            dataset,\n",
    "            \"perm\" + str(args_perm_id),\n",
    "            f\"{dataset}_{args_task_num}task_{args_class_num // args_task_num}way_{args_shot_num}shot.{split}.jsonl\",\n",
    "        )\n",
    "    elif split in [\"dev\", \"test\"]:\n",
    "        pth = os.path.join(root, dataset, f\"{dataset}.{split}.jsonl\")\n",
    "    elif split == \"stream\":\n",
    "        pth = os.path.join(\n",
    "            root,\n",
    "            dataset,\n",
    "            f\"stream_label_{args_task_num}task_{args_class_num // args_task_num}way.json\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Split \"{split}\" value wrong!')\n",
    "    if not os.path.exists(pth):\n",
    "        raise FileNotFoundError(f\"Path {pth} do not exist!\")\n",
    "    else:\n",
    "        with open(pth) as f:\n",
    "            if pth.endswith(\".jsonl\"):\n",
    "                data = [json.loads(line) for line in f]\n",
    "                if split == \"train\":\n",
    "                    data = [list(i.values()) for i in data]\n",
    "            else:\n",
    "                data = json.load(f)\n",
    "    # if split == \"train\":\n",
    "    #     data = extract_single_dict(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ACE'\n",
    "root = './data_incremental'\n",
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [instance for t in collect_from_json(dataset, root, split)[1] for instance in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    trigger_word = []\n",
    "    seq_len = len(data_instance[\"piece_ids\"]) + 1 # because start_index of piece_ids is 1 instead of 0\n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            trigger_word.append(data_instance[\"span\"][i])\n",
    "\n",
    "    set_label_in_one_sentence = set(true_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(num_label)\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(num_label)[i]\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(seq_len)\n",
    "    trigger = []\n",
    "    for i in trigger_word:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig = set(trigger)\n",
    "    for i in set_trig:\n",
    "        true_one_hot_label_vector += torch.eye(seq_len)[i]\n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_one_hot_true_label_and_true_trigger(data[0],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "47\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(data[0]['piece_ids']))\n",
    "print(len(data[0]['label']))\n",
    "print(len(data[0]['span']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in data:\n",
    "    true_one_hot_trigger_vector, true_one_hot_label_vector = get_one_hot_true_label_and_true_trigger(instance, 10)\n",
    "    instance['true_one_hot_trigger_vector'] = true_one_hot_trigger_vector.tolist()\n",
    "    instance['true_one_hot_label_vector'] = true_one_hot_label_vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'piece_ids': [101,\n",
       "  3960,\n",
       "  1010,\n",
       "  1045,\n",
       "  2228,\n",
       "  2008,\n",
       "  1996,\n",
       "  3114,\n",
       "  7955,\n",
       "  1999,\n",
       "  1996,\n",
       "  2148,\n",
       "  1011,\n",
       "  1011,\n",
       "  2017,\n",
       "  2113,\n",
       "  1010,\n",
       "  2034,\n",
       "  1997,\n",
       "  2035,\n",
       "  1010,\n",
       "  2057,\n",
       "  2020,\n",
       "  1011,\n",
       "  1011,\n",
       "  2043,\n",
       "  5951,\n",
       "  8573,\n",
       "  2001,\n",
       "  2700,\n",
       "  2343,\n",
       "  1010,\n",
       "  2057,\n",
       "  2018,\n",
       "  2042,\n",
       "  2542,\n",
       "  2054,\n",
       "  2057,\n",
       "  2245,\n",
       "  2001,\n",
       "  2145,\n",
       "  1037,\n",
       "  11438,\n",
       "  3842,\n",
       "  2044,\n",
       "  1996,\n",
       "  2942,\n",
       "  2162,\n",
       "  1012,\n",
       "  102],\n",
       " 'label': [6,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'span': [[29, 29],\n",
       "  [46, 47],\n",
       "  [1, 1],\n",
       "  [2, 2],\n",
       "  [3, 3],\n",
       "  [4, 4],\n",
       "  [5, 5],\n",
       "  [6, 6],\n",
       "  [7, 7],\n",
       "  [8, 8],\n",
       "  [9, 9],\n",
       "  [10, 10],\n",
       "  [11, 11],\n",
       "  [12, 12],\n",
       "  [13, 13],\n",
       "  [14, 14],\n",
       "  [15, 15],\n",
       "  [16, 16],\n",
       "  [17, 17],\n",
       "  [18, 18],\n",
       "  [19, 19],\n",
       "  [20, 20],\n",
       "  [21, 21],\n",
       "  [22, 22],\n",
       "  [23, 23],\n",
       "  [24, 24],\n",
       "  [25, 25],\n",
       "  [26, 26],\n",
       "  [27, 27],\n",
       "  [28, 28],\n",
       "  [30, 30],\n",
       "  [31, 31],\n",
       "  [32, 32],\n",
       "  [33, 33],\n",
       "  [34, 34],\n",
       "  [35, 35],\n",
       "  [36, 36],\n",
       "  [37, 37],\n",
       "  [38, 38],\n",
       "  [39, 39],\n",
       "  [40, 40],\n",
       "  [41, 41],\n",
       "  [42, 42],\n",
       "  [43, 43],\n",
       "  [44, 44],\n",
       "  [45, 45],\n",
       "  [48, 48]],\n",
       " 'true_one_hot_trigger_vector': [0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'true_one_hot_label_vector': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    true_trigger = []\n",
    "    seq_len = len(data_instance[\"piece_ids\"]) # because start_index of piece_ids is 1 instead of 0\n",
    "    \n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            true_trigger.append(data_instance[\"span\"][i])\n",
    "\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(num_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(seq_len)\n",
    "\n",
    "    set_label_in_one_sentence = set([label.item() for label in true_label])\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_label_vector += torch.eye(num_label)[i]\n",
    "\n",
    "\n",
    "    list_trigger = [trigger.tolist() for trigger in true_trigger]\n",
    "    trigger = []\n",
    "    for i in list_trigger:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig_in_one_sentence = set(trigger)\n",
    "\n",
    "    for i in set_trig_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(seq_len)[i]\n",
    "    \n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    true_trigger = []\n",
    "    seq_len = len(data_instance[\"piece_ids\"]) # because start_index of piece_ids is 1 instead of 0\n",
    "    matrix_word_is_label = torch.zeros(seq_len, num_label,dtype=int)\n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            true_trigger.append(data_instance[\"span\"][i])\n",
    "            for word_is_trigger in data_instance['span'][i]:\n",
    "                matrix_word_is_label[word_is_trigger,data_instance['label'][i]] = 1\n",
    "\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(num_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(seq_len)\n",
    "\n",
    "    set_label_in_one_sentence = set([label.item() for label in true_label])\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_label_vector += torch.eye(num_label)[i]\n",
    "\n",
    "\n",
    "    list_trigger = [trigger.tolist() for trigger in true_trigger]\n",
    "    trigger = []\n",
    "    for i in list_trigger:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig_in_one_sentence = set(trigger)\n",
    "\n",
    "    for i in set_trig_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(seq_len)[i]\n",
    "    true_one_hot_trigger_vector = true_one_hot_trigger_vector.to(device)\n",
    "    true_one_hot_label_vector = true_one_hot_label_vector.to(device)\n",
    "    matrix_word_is_label = matrix_word_is_label.to(device)\n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label\n",
    "\n",
    "def true_label_and_trigger(train_x,train_y,train_masks, train_span, class_num):\n",
    "    num_instance = len(train_x)\n",
    "    true_one_hot_label_vectors = []\n",
    "    true_one_hot_trigger_vectors = []\n",
    "    golden_matrix = []\n",
    "    for i in range(num_instance):\n",
    "        data_instace={\n",
    "            'piece_ids': train_x[i],\n",
    "            'label': train_y[i],\n",
    "            'span': train_span[i],\n",
    "            'mask': train_masks[i]\n",
    "        }\n",
    "\n",
    "        true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label= get_one_hot_true_label_and_true_trigger(data_instance=data_instace,num_label=class_num)\n",
    "        true_one_hot_trigger_vectors.append(true_one_hot_trigger_vector)\n",
    "        true_one_hot_label_vectors.append(true_one_hot_label_vector)\n",
    "        golden_matrix.append(matrix_word_is_label)\n",
    "    true_one_hot_trigger_vectors = torch.stack([x.to(device) for x in true_one_hot_trigger_vectors])\n",
    "    true_one_hot_label_vectors = torch.stack([x.to(device) for x in true_one_hot_label_vectors])\n",
    "    pi_golden_matrix = torch.stack([x.to(device) for x in golden_matrix])\n",
    "    return true_one_hot_trigger_vectors, true_one_hot_label_vectors, pi_golden_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_x = torch.tensor([[  101,  6398,  1024,  6175,  2003,  2025,  1996,  2069,  2510,  2564,\n",
    "          2040,  2363,  1037,  6302,  1998,  3661,  1012,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101, 21524,  1998,  2037,  2079, 24968,  5611,  6956, 19974,  2224,\n",
    "          1996,  2773,  1036,  1036,  6139,  1005,  1005,  2043,  9694,  2008,\n",
    "          3956,  2681,  1996,  2225,  2924,  1998, 14474,  1998,  4487, 11512,\n",
    "          9286,  3644,  7617,  1012,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  1045,  2079,  1050,  1005,  1056,  2228,  2008,  1005,  1055,\n",
    "          3243,  8321,  2004,  2000,  2054,  2002,  2626,  2033,  2055,  1012,\n",
    "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2358, 23111,  6582,  1010,  2040,  2001,  2809,  2706,  6875,\n",
    "          1010,  2018,  3041,  2042,  3331,  2007,  2014,  2388,  2006,  1996,\n",
    "          3042,  1010,  1998,  5112,  2039,  3038,  1037,  2450,  2016,  2018,\n",
    "         11834,  3064,  2007,  3784,  2018,  2074,  3369,  2012,  2014,  2341,\n",
    "          1010,  4614,  2056,  1012,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]])\n",
    "\n",
    "train_y = [torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), torch.tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0]), torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), torch.tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
    "train_masks = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0]])\n",
    "train_span = [torch.tensor([[15, 15],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [16, 16]]), torch.tensor([[21, 21],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [15, 15],\n",
    "        [16, 16],\n",
    "        [17, 17],\n",
    "        [18, 18],\n",
    "        [19, 19],\n",
    "        [20, 20],\n",
    "        [22, 22],\n",
    "        [23, 23],\n",
    "        [24, 24],\n",
    "        [25, 25],\n",
    "        [26, 26],\n",
    "        [27, 27],\n",
    "        [28, 28],\n",
    "        [29, 29],\n",
    "        [30, 30],\n",
    "        [31, 31],\n",
    "        [32, 32],\n",
    "        [33, 33]]), torch.tensor([[16, 16],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [15, 15],\n",
    "        [17, 17],\n",
    "        [18, 18],\n",
    "        [19, 19]]), torch.tensor([[20, 20],\n",
    "        [30, 31],\n",
    "        [ 1,  1],\n",
    "        [ 2,  2],\n",
    "        [ 3,  3],\n",
    "        [ 4,  4],\n",
    "        [ 5,  5],\n",
    "        [ 6,  6],\n",
    "        [ 7,  7],\n",
    "        [ 8,  8],\n",
    "        [ 9,  9],\n",
    "        [10, 10],\n",
    "        [11, 11],\n",
    "        [12, 12],\n",
    "        [13, 13],\n",
    "        [14, 14],\n",
    "        [15, 15],\n",
    "        [16, 16],\n",
    "        [17, 17],\n",
    "        [18, 18],\n",
    "        [19, 19],\n",
    "        [21, 21],\n",
    "        [22, 22],\n",
    "        [23, 23],\n",
    "        [24, 24],\n",
    "        [25, 25],\n",
    "        [26, 26],\n",
    "        [27, 27],\n",
    "        [28, 28],\n",
    "        [29, 29],\n",
    "        [32, 32],\n",
    "        [33, 33],\n",
    "        [34, 34],\n",
    "        [35, 35],\n",
    "        [36, 36],\n",
    "        [37, 37],\n",
    "        [38, 38],\n",
    "        [39, 39],\n",
    "        [40, 40],\n",
    "        [41, 41],\n",
    "        [42, 42],\n",
    "        [43, 43]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trigger, true_label, pi_golden = true_label_and_trigger(train_x=train_x,train_y=train_y,train_masks=train_masks,train_span=train_span,class_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_golden[3][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi_star weighted by golden matrix (sum along axis 1):\n",
      "[0.7 0.5 0.3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Giả sử bạn có các ma trận sau:\n",
    "# pi_star: Ma trận alignment (sau khi tính toán thông qua OT)\n",
    "pi_star = np.array([[0.3, 0.7, 0.1],\n",
    "                    [0.5, 0.4, 0.1],\n",
    "                    [0.2, 0.6, 0.3]])\n",
    "\n",
    "# pi_g: Ma trận golden (true trigger labels)\n",
    "pi_g = np.array([[0, 1, 0],  # w1 có nhãn đúng là t2\n",
    "                 [1, 0, 0],  # w2 có nhãn đúng là t1\n",
    "                 [0, 0, 1]]) # w3 có nhãn đúng là t3\n",
    "\n",
    "# Nhân ma trận pi_star với pi_g (theo từng phần tử)\n",
    "pi_star_weighted = pi_star * pi_g\n",
    "\n",
    "# Tính tổng theo chiều ngang (axis=1)\n",
    "pi_star_sum = pi_star_weighted.sum(axis=1)\n",
    "\n",
    "print(\"Pi_star weighted by golden matrix (sum along axis 1):\")\n",
    "print(pi_star_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_star_1 = torch.tensor([\n",
    "    [[0.3, 0.7, 0.1,-1.0], [0.5, 0.4, 0.1,-1.0], [0.2, 0.6, 0.3,-1.0]],\n",
    "    [[0.5, 0.4, 0.1,-2.0], [0.3, 0.7, 0.1,-2.0], [0.2, 0.6, 0.3,-2.0]]]\n",
    ")\n",
    "pi_star_2 = torch.tensor([\n",
    "    [[0.3, 0.7, 0.1,-1.0], [0.5, 0.4, 0.1,-1.0], [0.2, 0.6, 0.3,-1.0]],\n",
    "    [[0.5, 0.4, 0.1,-2.0], [0.3, 0.7, 0.1,-2.0], [0.2, 0.6, 0.3,-2.0]]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6357)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def L_task(pi_star, y_true):\n",
    "    \"\"\"\n",
    "    Tính Loss Task (Negative Log-Likelihood Loss) cho mỗi batch dữ liệu.\n",
    "\n",
    "    Arguments:\n",
    "    - pi_star (Tensor): Tensor có kích thước (batch_size, seq_len), chứa xác suất dự đoán cho từng từ và nhãn.\n",
    "    - y_true (Tensor): Tensor có kích thước (batch_size, seq_len), chứa nhãn thực tế (labels) cho từng từ.\n",
    "\n",
    "    Return:\n",
    "    - loss (Tensor): giá trị loss trung bình cho cả batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Chỉ tính log của pi_star mà không có phần (1 - pi_star)\n",
    "    loss = -torch.log((torch.sum(pi_star*y_true,dim=-1))).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Ví dụ sử dụng:\n",
    "batch_size = 2  # Số câu trong batch\n",
    "seq_len = 4  # Số từ trong mỗi câu\n",
    "\n",
    "# Giả sử chúng ta có xác suất pi_star và nhãn thực tế y_true cho mỗi câu trong batch\n",
    "pi_star = torch.tensor(\n",
    "    [\n",
    "        [[0.3, 0.7, 0.1, -1.0], [0.5, 0.4, 0.1, -1.0], [0.2, 0.6, 0.3, -1.0]],\n",
    "        [[0.5, 0.4, 0.1, -2.0], [0.3, 0.7, 0.1, -2.0], [0.2, 0.6, 0.3, -2.0]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_true = torch.tensor(\n",
    "    [\n",
    "        [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0]],\n",
    "        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tính L_task\n",
    "loss_task = L_task(pi_star, y_true)\n",
    "print(loss_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6357)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = torch.sum(pi_star * y_true, dim=-1)\n",
    "log_probs = torch.log(log_probs + 1e-10)\n",
    "loss = -log_probs.mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6357)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log((torch.sum(pi_star*y_true,dim=-1))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.3567, -0.0000,     nan],\n",
       "         [-0.6931, -0.0000, -0.0000,     nan],\n",
       "         [-0.0000, -0.0000, -1.2040,     nan]],\n",
       "\n",
       "        [[-0.6931, -0.0000, -0.0000,     nan],\n",
       "         [-0.0000, -0.3567, -0.0000,     nan],\n",
       "         [-0.0000, -0.5108, -0.0000,     nan]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(pi_star)*y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul = -torch.log(pi_star_1*pi_star_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4079,  0.7133,  4.6052, -0.0000],\n",
       "         [ 1.3863,  1.8326,  4.6052, -0.0000],\n",
       "         [ 3.2189,  1.0217,  2.4079, -0.0000]],\n",
       "\n",
       "        [[ 1.3863,  1.8326,  4.6052, -1.3863],\n",
       "         [ 2.4079,  0.7133,  4.6052, -1.3863],\n",
       "         [ 3.2189,  1.0217,  2.4079, -1.3863]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8499, 1.5033])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul.sum(dim=[1,2])/(mul.size(1)*mul.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star.sum(dim=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instance = {\n",
    "    \"piece_ids\": train_x[3],\n",
    "    \"label\": train_y[3],\n",
    "    \"span\": train_span[3],\n",
    "    \"mask\": train_masks[3],\n",
    "}\n",
    "# a, b = get_one_hot_true_label_and_true_trigger(data_instance=data_instance,num_label=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_word_is_label = torch.zeros(len(data_instance['piece_ids']),10)\n",
    "for i in range(len(data_instance[\"label\"])):\n",
    "    if data_instance[\"label\"][i] != 0:\n",
    "        # print(data_instance[\"label\"][i])\n",
    "\n",
    "    #     true_label.append(data_instance[\"label\"][i])\n",
    "    #     true_trigger.append(data_instance[\"span\"][i])\n",
    "        for word_is_trigger in data_instance['span'][i]:\n",
    "            # print(word_is_trigger)\n",
    "            matrix_word_is_label[word_is_trigger,data_instance['label'][i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_word_is_label[20:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_span)):\n",
    "    print(train_span[i].size(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = [torch.tensor([[1,2,3],[4,5,6]]),torch.tensor([[7,8,9],[10,11,12]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Giả sử bạn có các tensor sau:\n",
    "# `last_hidden_state` có kích thước [batch_size, seqlen, hidden_dim]\n",
    "last_hidden_state = torch.randn(4, 122, 768)  # Kích thước giả định\n",
    "\n",
    "# `mask` có kích thước [batch_size, seqlen], giá trị 1 cho token thực sự và 0 cho padding\n",
    "mask = torch.randint(0, 2, (4, 122))  # Ví dụ: 1 cho token thực sự, 0 cho padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_hidden_state_without_padding(last_hidden_state, masks):\n",
    "    masks = masks.unsqueeze(-1)\n",
    "    mask_hidden_state = last_hidden_state * masks\n",
    "    true_hidden_state_without_padding = mask_hidden_state.view(-1, 768)[\n",
    "        masks.view(-1) == 1\n",
    "    ]\n",
    "    return true_hidden_state_without_padding  # [sum_true_token, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_embedding = get_true_hidden_state_without_padding(last_hidden_state,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_hidden_state = last_hidden_state * mask.unsqueeze(-1)\n",
    "masked_hidden_state.size()\n",
    "masked_hidden_state = masked_hidden_state.view(-1, 768)[mask.view(-1) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249, 768])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249, 768])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giả sử bạn có các tensor như sau:\n",
    "batch_size = 4\n",
    "seq_len = 6\n",
    "\n",
    "# Các xác suất trigger cho từng token trong câu (p_wi)\n",
    "p_wi = torch.sigmoid(torch.randn(batch_size, seq_len))  # [4, 6]\n",
    "\n",
    "# Các nhãn thực tế (true_trig)\n",
    "true_trig = torch.randint(0, 2, (batch_size, seq_len))  # [4, 6] với giá trị 0 hoặc 1\n",
    "\n",
    "# Attention mask (masks), 1 cho token thực, 0 cho token padding\n",
    "masks = torch.tensor([[1, 1, 1, 1, 0, 0],  # Câu 1 có 4 token thực\n",
    "                      [1, 1, 1, 0, 0, 0],  # Câu 2 có 3 token thực\n",
    "                      [1, 1, 1, 1, 1, 0],  # Câu 3 có 5 token thực\n",
    "                      [1, 1, 1, 1, 1, 1]]) # Câu 4 có 6 token thực (không có padding)\n",
    "\n",
    "# Tính loss TI\n",
    "# loss = compute_loss_TI(p_wi, true_trig, masks)\n",
    "# print(\"Loss TI:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8904, 0.4555, 0.2070])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wi[1][masks[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 5\n",
    "num_classes = 3\n",
    "\n",
    "# Tạo dữ liệu giả cho pi_star và pi_golden (giả sử xác suất phân bố đều cho pi_star, và pi_golden là các vector nhãn)\n",
    "pi_star = torch.rand(batch_size, seq_len, num_classes)  # Xác suất dự đoán ngẫu nhiên từ [0, 1]\n",
    "pi_star = pi_star / pi_star.sum(dim=-1, keepdim=True)  # Chuẩn hóa về tổng = 1 (xác suất)\n",
    "\n",
    "pi_golden = torch.randint(0, 2, (batch_size, seq_len, num_classes)).float()  # Nhãn thực (0 hoặc 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1151, 0.4943, 1.0000, 0.3761, 0.3852],\n",
       "        [0.4031, 0.7366, 1.0000, 0.8397, 0.1146],\n",
       "        [1.0000, 0.6542, 0.8769, 1.0000, 0.4036],\n",
       "        [0.8367, 0.6056, 0.7390, 0.0000, 0.3532]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pi_golden*pi_star,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.1151, 0.0000],\n",
       "         [0.1963, 0.2979, 0.0000],\n",
       "         [0.2573, 0.4445, 0.2983],\n",
       "         [0.0000, 0.3761, 0.0000],\n",
       "         [0.0000, 0.3852, 0.0000]],\n",
       "\n",
       "        [[0.4031, 0.0000, 0.0000],\n",
       "         [0.0000, 0.3005, 0.4361],\n",
       "         [0.4377, 0.2548, 0.3074],\n",
       "         [0.3200, 0.0000, 0.5196],\n",
       "         [0.0246, 0.0000, 0.0900]],\n",
       "\n",
       "        [[0.2333, 0.3558, 0.4110],\n",
       "         [0.1819, 0.0000, 0.4723],\n",
       "         [0.6229, 0.0000, 0.2540],\n",
       "         [0.2883, 0.2216, 0.4901],\n",
       "         [0.0000, 0.0000, 0.4036]],\n",
       "\n",
       "        [[0.0000, 0.3779, 0.4588],\n",
       "         [0.0000, 0.6056, 0.0000],\n",
       "         [0.2351, 0.5038, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.3532, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_golden*pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_pytorch(M, a, b, lambda_sh, numItermax=1000, stopThr=5e-3):\n",
    "    u = torch.ones_like(a) / a.size(0)\n",
    "    v = torch.zeros_like(b)\n",
    "    K = torch.exp(-M * lambda_sh)\n",
    "\n",
    "    cpt = 0\n",
    "    err = 1.0\n",
    "\n",
    "    def condition(cpt, u, v, err):\n",
    "        return cpt < numItermax and err > stopThr\n",
    "\n",
    "    def v_update(u, v):\n",
    "        v = b / torch.matmul(K.t(), u)\n",
    "        u = a / torch.matmul(K, v)\n",
    "        return u, v\n",
    "\n",
    "    def no_v_update(u, v):\n",
    "        return u, v\n",
    "\n",
    "    def err_f1(K, u, v, b):\n",
    "        bb = v * torch.matmul(K.t(), u)\n",
    "        err = torch.norm(torch.sum(torch.abs(bb - b), dim=0), p=float('inf'))\n",
    "        return err\n",
    "\n",
    "    def err_f2(err):\n",
    "        return err\n",
    "\n",
    "    def loop_func(cpt, u, v, err):\n",
    "        u = a / torch.matmul(K, b / torch.matmul(u.T, K).T)\n",
    "        cpt = cpt + 1\n",
    "        if cpt % 20 == 1 or cpt == numItermax:\n",
    "            u, v = v_update(u, v)\n",
    "            err = err_f1(K, u, v, b)\n",
    "        else:\n",
    "            u, v = no_v_update(u, v)\n",
    "            err = err_f2(err)\n",
    "        return cpt, u, v, err\n",
    "\n",
    "    while condition(cpt, u, v, err):\n",
    "        cpt, u, v, err = loop_func(cpt, u, v, err)\n",
    "\n",
    "    sinkhorn_divergences = torch.sum(u * torch.matmul(K * M, v), dim=0)\n",
    "    return sinkhorn_divergences\n",
    "\n",
    "def compute_pi_star(M, a, b, lambda_sh, numItermax=1000, stopThr=5e-3):\n",
    "    # Gọi hàm Sinkhorn để tính u, v và ma trận K\n",
    "    u, v, K = sinkhorn_pytorch(M, a, b, lambda_sh, numItermax, stopThr)\n",
    "    \n",
    "    # Tính ma trận đồng nhất tối ưu pi_star bằng công thức pi_star = diag(u) * K * diag(v)\n",
    "    pi_star = torch.diag(u) @ K @ torch.diag(v)\n",
    "    \n",
    "    return pi_star\n",
    "\n",
    "# batch_size = 4\n",
    "# num_class = 11\n",
    "# num_token = 122\n",
    "\n",
    "# word_preference = torch.randn(batch_size,num_token)\n",
    "# type_preference = torch.randn(batch_size,num_class)\n",
    "# cost_matrix = torch.randn(batch_size,num_class,num_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class, num_token = 11, 122\n",
    "\n",
    "# Tạo ma trận chi phí ngẫu nhiên kích thước (n, m) = (11, 122)\n",
    "M = torch.rand(num_class, num_token)\n",
    "\n",
    "# Tạo các vector a và b là phân phối xác suất (tổng = 1)\n",
    "a = torch.tensor([1.0 / num_class] * num_class)  # Phân phối xác suất cho a (có tổng = 1)\n",
    "b = torch.tensor([1.0 / num_token] * num_token)  # Phân phối xác suất cho b (có tổng = 1)\n",
    "\n",
    "# Tham số điều chỉnh regularization\n",
    "lambda_sh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 122, 11, 768])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([4,122,768])\n",
    "y = torch.randn([11,768])\n",
    "(x.unsqueeze(2)-y.unsqueeze(0).unsqueeze(0)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.unsqueeze(2).repeat(1,1,11,1)\n",
    "y1=y.unsqueeze(0).unsqueeze(0).repeat(4,122,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 122, 11])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-torch.nn.functional.cosine_similarity(x1=x1,x2=y1,dim=3)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.randint(1,20,(4,11,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 1, 12, 7, 5, 17, 16, 11, 14, 13, 15, 6]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict.fromkeys(test[0].flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 1, 12, 7, 1, 5, 17, 16, 11, 14, 1, 13, 5, 11, 5, 15, 2, 17, 6, 11, 1]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 8, 8, 1, 1, 12, 13, 7, 5, 17, 16, 11, 2, 14, 13, 15, 6]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2, 2, 8, 8, 1, 1, 12, 13, 7, 5, 17, 16, 11,2, 14, 13, 15, 6]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 1, 12, 13, 7, 5, 17, 16, 11, 14, 15, 6]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict.fromkeys(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([4,122,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 768])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][list(dict.fromkeys(a)),:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(768,1)\n",
    "input = torch.rand([17,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.sigmoid(linear(input))\n",
    "output1 = (linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3153, -0.2226, -0.1029, -0.2959, -0.4649, -0.2998, -0.5438, -0.1999,\n",
       "        -0.1981, -0.6199, -0.4724, -0.2184, -0.0811, -0.1304, -0.4779, -0.1843,\n",
       "        -0.3864], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0576, 0.0632, 0.0713, 0.0588, 0.0496, 0.0585, 0.0458, 0.0647, 0.0648,\n",
       "        0.0425, 0.0492, 0.0635, 0.0728, 0.0693, 0.0490, 0.0657, 0.0537],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output.squeeze(1),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_y(y):\n",
    "    true_y = []\n",
    "    for i in range(len(y)):\n",
    "        filter_y = (y[i] != 0).int()\n",
    "        true_y.append(filter_y)\n",
    "    return true_y\n",
    "\n",
    "test = torch.randint(0,10,[4,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 9, 6, 3, 6, 7, 5, 4, 6, 6],\n",
       "        [6, 4, 7, 1, 3, 0, 1, 7, 9, 3],\n",
       "        [0, 0, 2, 1, 9, 3, 3, 9, 4, 7],\n",
       "        [9, 1, 3, 8, 7, 9, 7, 3, 6, 7]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_true_y(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6034)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_loss_TI(p_wi, true_y):\n",
    "    loss_TI = 0.0\n",
    "    for i in range(len(true_y)):\n",
    "        # mask padding token\n",
    "        loss_TI += -torch.dot(true_y[i].float(), torch.log(p_wi[i])) - torch.dot(\n",
    "            (1 - true_y[i].float()), torch.log(1 - p_wi[i])\n",
    "        )\n",
    "\n",
    "    return loss_TI / len(true_y)\n",
    "\n",
    "\n",
    "p_wi = [torch.tensor([0.9,0.05,0.1]), torch.tensor([0.6,0.1,0.1,0.2])]\n",
    "true_y = [torch.tensor([1,0,0]),torch.tensor([1,0,0,0])]\n",
    "compute_loss_TI(p_wi,true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_y(y,num_class):\n",
    "    true_trig,true_label = [],[]\n",
    "    for i in range(len(y)):\n",
    "        true_label_loop = torch.zeros(num_class)\n",
    "        set_label = set(y[i].tolist())\n",
    "        for label in set_label:\n",
    "            if label != 0:\n",
    "                true_label_loop += torch.nn.functional.one_hot(torch.tensor(label),num_classes=num_class)\n",
    "        \n",
    "        filter_y = (y[i] != 0).int()\n",
    "        true_trig.append(filter_y)\n",
    "        true_label.append(true_label_loop)\n",
    "    return true_trig,true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [torch.tensor([2,3,5,0,0,0]),torch.tensor([7,8,0]),torch.tensor([1,1,2,0])]\n",
    "num_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trig, true_label =get_true_y(y,num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 1, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([1, 1, 0], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 0], dtype=torch.int32)]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_trig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 1., 1., 0., 1., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_one_hot_true_label_and_true_trigger(data_instance, num_label):\n",
    "    true_label = []\n",
    "    true_trigger = []\n",
    "    seq_len = len(\n",
    "        data_instance[\"piece_ids\"]\n",
    "    )  # because start_index of piece_ids is 1 instead of 0\n",
    "    matrix_word_is_label = torch.zeros(seq_len, num_label, dtype=int)\n",
    "    for i in range(len(data_instance[\"label\"])):\n",
    "        if data_instance[\"label\"][i] != 0:\n",
    "            true_label.append(data_instance[\"label\"][i])\n",
    "            true_trigger.append(data_instance[\"span\"][i])\n",
    "            for word_is_trigger in data_instance[\"span\"][i]:\n",
    "                matrix_word_is_label[word_is_trigger, data_instance[\"label\"][i]] = 1\n",
    "\n",
    "    true_one_hot_label_vector = torch.zeros(num_label)\n",
    "    true_one_hot_trigger_vector = torch.zeros(seq_len)\n",
    "\n",
    "    set_label_in_one_sentence = set([label.item() for label in true_label])\n",
    "    for i in set_label_in_one_sentence:\n",
    "        true_one_hot_label_vector += torch.eye(num_label)[i]\n",
    "\n",
    "    list_trigger = [trigger.tolist() for trigger in true_trigger]\n",
    "    trigger = []\n",
    "    for i in list_trigger:\n",
    "        trigger.extend(i)\n",
    "\n",
    "    set_trig_in_one_sentence = set(trigger)\n",
    "\n",
    "    for i in set_trig_in_one_sentence:\n",
    "        true_one_hot_trigger_vector += torch.eye(seq_len)[i]\n",
    "    true_one_hot_trigger_vector = true_one_hot_trigger_vector.to(device)\n",
    "    true_one_hot_label_vector = true_one_hot_label_vector.to(device)\n",
    "    return true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label\n",
    "\n",
    "\n",
    "def true_label_and_trigger(train_x, train_y, train_masks, train_span, class_num):\n",
    "    num_instance = len(train_x)\n",
    "    true_one_hot_label_vectors = []\n",
    "    true_one_hot_trigger_vectors = []\n",
    "    golden_matrix = []\n",
    "    for i in range(num_instance):\n",
    "        data_instace = {\n",
    "            \"piece_ids\": train_x[i],\n",
    "            \"label\": train_y[i],\n",
    "            \"span\": train_span[i],\n",
    "            \"mask\": train_masks[i],\n",
    "        }\n",
    "\n",
    "        true_one_hot_trigger_vector, true_one_hot_label_vector, matrix_word_is_label = (\n",
    "            get_one_hot_true_label_and_true_trigger(\n",
    "                data_instance=data_instace, num_label=class_num\n",
    "            )\n",
    "        )\n",
    "        true_one_hot_trigger_vectors.append(true_one_hot_trigger_vector)\n",
    "        true_one_hot_label_vectors.append(true_one_hot_label_vector)\n",
    "        golden_matrix.append(matrix_word_is_label)\n",
    "    true_one_hot_trigger_vectors = torch.stack(\n",
    "        [x.to(device) for x in true_one_hot_trigger_vectors]\n",
    "    )\n",
    "    true_one_hot_label_vectors = torch.stack(\n",
    "        [x.to(device) for x in true_one_hot_label_vectors]\n",
    "    )\n",
    "    pi_golden_matrix = torch.stack([x.to(device) for x in golden_matrix])\n",
    "    return true_one_hot_trigger_vectors, true_one_hot_label_vectors, pi_golden_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_order = [torch.rand([11,768]),torch.rand([5,768]),torch.rand([6,768]),torch.rand(20,768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embedding = torch.rand([11,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11, 768])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embedding.unsqueeze(0).repeat([5,1,1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11, 768])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state_order[1].unsqueeze(1).repeat([1,11,1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-torch.nn.functional.cosine_similarity(label_embedding.unsqueeze(0).repeat([5,1,1]),last_hidden_state_order[1].unsqueeze(1).repeat([1,11,1]),dim=-1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_transport(last_hidden_state_order, label_embedding, num_classes = 11):\n",
    "    # last_hidden_state_order: [batch_size, num_span, hidden_dim]\n",
    "    # label_embedding: [num_class, hidden_dim]\n",
    "    \n",
    "    batch_size = len(last_hidden_state_order)\n",
    "    cost_matrix = []\n",
    "    for i in range(batch_size):\n",
    "        num_span = last_hidden_state_order[i].size(0)\n",
    "        label_embedding_scale = label_embedding.unsqueeze(0).repeat([num_span,1,1])\n",
    "        last_hidden_state_order_scale = last_hidden_state_order[i].unsqueeze(1).repeat([1,num_classes,1])\n",
    "        cost = 1-torch.nn.functional.cosine_similarity(last_hidden_state_order_scale,label_embedding_scale,dim=-1)\n",
    "        cost_matrix.append(cost)\n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix = compute_cost_transport(last_hidden_state_order=last_hidden_state_order,label_embedding=label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of cost_matrix[0]: torch.Size([11, 11])\n",
      "size of cost_matrix[1]: torch.Size([5, 11])\n",
      "size of cost_matrix[2]: torch.Size([6, 11])\n",
      "size of cost_matrix[3]: torch.Size([20, 11])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cost_matrix)):\n",
    "    print(f'size of cost_matrix[{i}]: {cost_matrix[i].size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(768,1)\n",
    "D_W_P = []\n",
    "for sentence in range(len(last_hidden_state_order)):\n",
    "    D_W_P.append(torch.softmax(torch.sigmoid(linear(last_hidden_state_order[sentence])).flatten(),dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(D_W_P[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_cls = torch.rand([4,768])\n",
    "e_cls_scale = e_cls.unsqueeze(1).repeat([1,11,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11, 768])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_cls_scale.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embedding_scale = label_embedding.unsqueeze(0).repeat([4,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = torch.cat([e_cls_scale,label_embedding_scale],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffp = torch.nn.Linear(768*2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_T_P = torch.softmax(torch.sigmoid(ffp(concat).squeeze(-1)),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(D_T_P[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "def compute_optimal_transport(p, q, C, epsilon=0.05):\n",
    "    device = p.device\n",
    "\n",
    "    p_i = p.detach().cpu().numpy()\n",
    "    q_i = q.detach().cpu().numpy()\n",
    "    C_i = C.detach().cpu().numpy()\n",
    "\n",
    "    pi_i = ot.sinkhorn(p_i, q_i, C_i, reg=epsilon)\n",
    "\n",
    "    pi_i_tensor = torch.tensor(pi_i, device=device)\n",
    "\n",
    "    return pi_i_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0921, 0.0961, 0.0943, 0.0922, 0.0912, 0.0915, 0.0877, 0.0861, 0.0822,\n",
      "        0.0953, 0.0914], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.0918, 0.0882, 0.0929, 0.0894, 0.0941, 0.0892, 0.0920, 0.0894, 0.0907,\n",
      "        0.0916, 0.0907], grad_fn=<SelectBackward0>)\n",
      "tensor([[0.2582, 0.2447, 0.2369, 0.2448, 0.2562, 0.2636, 0.2619, 0.2515, 0.2402,\n",
      "         0.2487, 0.2451],\n",
      "        [0.2566, 0.2528, 0.2547, 0.2515, 0.2473, 0.2396, 0.2482, 0.2400, 0.2459,\n",
      "         0.2446, 0.2357],\n",
      "        [0.2433, 0.2444, 0.2473, 0.2426, 0.2496, 0.2442, 0.2377, 0.2474, 0.2523,\n",
      "         0.2609, 0.2431],\n",
      "        [0.2412, 0.2486, 0.2462, 0.2530, 0.2683, 0.2550, 0.2419, 0.2410, 0.2575,\n",
      "         0.2339, 0.2480],\n",
      "        [0.2692, 0.2484, 0.2487, 0.2553, 0.2570, 0.2561, 0.2587, 0.2569, 0.2745,\n",
      "         0.2466, 0.2502],\n",
      "        [0.2492, 0.2455, 0.2577, 0.2404, 0.2465, 0.2552, 0.2486, 0.2314, 0.2603,\n",
      "         0.2404, 0.2526],\n",
      "        [0.2706, 0.2537, 0.2363, 0.2622, 0.2615, 0.2597, 0.2703, 0.2486, 0.2498,\n",
      "         0.2531, 0.2637],\n",
      "        [0.2617, 0.2512, 0.2416, 0.2437, 0.2407, 0.2379, 0.2499, 0.2319, 0.2581,\n",
      "         0.2487, 0.2485],\n",
      "        [0.2428, 0.2718, 0.2489, 0.2628, 0.2479, 0.2384, 0.2459, 0.2548, 0.2578,\n",
      "         0.2620, 0.2306],\n",
      "        [0.2512, 0.2411, 0.2631, 0.2454, 0.2564, 0.2249, 0.2464, 0.2427, 0.2435,\n",
      "         0.2655, 0.2357],\n",
      "        [0.2515, 0.2465, 0.2444, 0.2596, 0.2401, 0.2355, 0.2340, 0.2434, 0.2350,\n",
      "         0.2432, 0.2404]])\n"
     ]
    }
   ],
   "source": [
    "print(D_W_P[0])\n",
    "print(D_T_P[0])\n",
    "print(cost_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhatminhnguyen/anaconda3/envs/natmin/lib/python3.11/site-packages/ot/bregman/_sinkhorn.py:531: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\"Sinkhorn did not converge. You might want to \"\n"
     ]
    }
   ],
   "source": [
    "pi_i = compute_optimal_transport(D_W_P[0],D_T_P[0],cost_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 11])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([6, 11])\n",
      "torch.Size([20, 11])\n"
     ]
    }
   ],
   "source": [
    "pi_star = []\n",
    "for sentence in range(len(D_W_P)):\n",
    "    pi_i = compute_optimal_transport(D_W_P[sentence],D_T_P[sentence],cost_matrix[sentence])\n",
    "    print(pi_i.size())\n",
    "    pi_star.append(pi_i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 11])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 0, 2, 1])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0156, 0.0163, 0.0175, 0.0116, 0.0145, 0.0117, 0.0144, 0.0162, 0.0179,\n",
       "         0.0147, 0.0167],\n",
       "        [0.0143, 0.0193, 0.0125, 0.0198, 0.0181, 0.0120, 0.0152, 0.0175, 0.0158,\n",
       "         0.0135, 0.0192],\n",
       "        [0.0169, 0.0132, 0.0133, 0.0175, 0.0171, 0.0154, 0.0146, 0.0175, 0.0179,\n",
       "         0.0150, 0.0152],\n",
       "        [0.0155, 0.0158, 0.0168, 0.0133, 0.0146, 0.0143, 0.0155, 0.0131, 0.0137,\n",
       "         0.0170, 0.0110],\n",
       "        [0.0150, 0.0121, 0.0126, 0.0138, 0.0134, 0.0193, 0.0117, 0.0091, 0.0139,\n",
       "         0.0174, 0.0168],\n",
       "        [0.0145, 0.0116, 0.0201, 0.0134, 0.0163, 0.0165, 0.0206, 0.0161, 0.0114,\n",
       "         0.0140, 0.0118]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y_3 = torch.randint(0,3,[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_y_3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 11])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.8244)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(-torch.log(pi_star[2].gather(1,true_y_3.unsqueeze(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0078, 0.0090, 0.0106, 0.0094, 0.0080, 0.0058, 0.0066, 0.0072, 0.0106,\n",
       "          0.0086, 0.0084],\n",
       "         [0.0080, 0.0076, 0.0074, 0.0081, 0.0095, 0.0093, 0.0086, 0.0090, 0.0094,\n",
       "          0.0092, 0.0100],\n",
       "         [0.0101, 0.0087, 0.0083, 0.0094, 0.0088, 0.0082, 0.0104, 0.0075, 0.0080,\n",
       "          0.0065, 0.0084],\n",
       "         [0.0107, 0.0081, 0.0086, 0.0077, 0.0061, 0.0067, 0.0096, 0.0086, 0.0073,\n",
       "          0.0112, 0.0077],\n",
       "         [0.0071, 0.0095, 0.0095, 0.0086, 0.0089, 0.0076, 0.0080, 0.0073, 0.0060,\n",
       "          0.0101, 0.0085],\n",
       "         [0.0089, 0.0085, 0.0067, 0.0097, 0.0093, 0.0065, 0.0083, 0.0103, 0.0068,\n",
       "          0.0097, 0.0069],\n",
       "         [0.0067, 0.0082, 0.0118, 0.0072, 0.0079, 0.0069, 0.0061, 0.0084, 0.0096,\n",
       "          0.0086, 0.0063],\n",
       "         [0.0064, 0.0070, 0.0085, 0.0084, 0.0096, 0.0085, 0.0074, 0.0094, 0.0065,\n",
       "          0.0075, 0.0069],\n",
       "         [0.0096, 0.0047, 0.0076, 0.0059, 0.0086, 0.0087, 0.0082, 0.0061, 0.0067,\n",
       "          0.0059, 0.0101],\n",
       "         [0.0087, 0.0094, 0.0061, 0.0090, 0.0077, 0.0122, 0.0087, 0.0083, 0.0096,\n",
       "          0.0059, 0.0098],\n",
       "         [0.0077, 0.0075, 0.0079, 0.0060, 0.0096, 0.0088, 0.0100, 0.0073, 0.0102,\n",
       "          0.0083, 0.0079]]),\n",
       " tensor([[0.0161, 0.0220, 0.0215, 0.0163, 0.0225, 0.0205, 0.0198, 0.0251, 0.0166,\n",
       "          0.0209, 0.0187],\n",
       "         [0.0221, 0.0140, 0.0167, 0.0190, 0.0155, 0.0212, 0.0189, 0.0179, 0.0139,\n",
       "          0.0156, 0.0219],\n",
       "         [0.0162, 0.0154, 0.0178, 0.0216, 0.0180, 0.0135, 0.0184, 0.0142, 0.0211,\n",
       "          0.0203, 0.0150],\n",
       "         [0.0181, 0.0163, 0.0126, 0.0171, 0.0218, 0.0195, 0.0155, 0.0163, 0.0199,\n",
       "          0.0173, 0.0142],\n",
       "         [0.0192, 0.0204, 0.0244, 0.0155, 0.0164, 0.0145, 0.0195, 0.0159, 0.0194,\n",
       "          0.0175, 0.0209]]),\n",
       " tensor([[0.0156, 0.0163, 0.0175, 0.0116, 0.0145, 0.0117, 0.0144, 0.0162, 0.0179,\n",
       "          0.0147, 0.0167],\n",
       "         [0.0143, 0.0193, 0.0125, 0.0198, 0.0181, 0.0120, 0.0152, 0.0175, 0.0158,\n",
       "          0.0135, 0.0192],\n",
       "         [0.0169, 0.0132, 0.0133, 0.0175, 0.0171, 0.0154, 0.0146, 0.0175, 0.0179,\n",
       "          0.0150, 0.0152],\n",
       "         [0.0155, 0.0158, 0.0168, 0.0133, 0.0146, 0.0143, 0.0155, 0.0131, 0.0137,\n",
       "          0.0170, 0.0110],\n",
       "         [0.0150, 0.0121, 0.0126, 0.0138, 0.0134, 0.0193, 0.0117, 0.0091, 0.0139,\n",
       "          0.0174, 0.0168],\n",
       "         [0.0145, 0.0116, 0.0201, 0.0134, 0.0163, 0.0165, 0.0206, 0.0161, 0.0114,\n",
       "          0.0140, 0.0118]]),\n",
       " tensor([[0.0032, 0.0037, 0.0040, 0.0030, 0.0054, 0.0055, 0.0049, 0.0036, 0.0056,\n",
       "          0.0054, 0.0045],\n",
       "         [0.0058, 0.0050, 0.0049, 0.0039, 0.0048, 0.0046, 0.0054, 0.0050, 0.0035,\n",
       "          0.0043, 0.0042],\n",
       "         [0.0050, 0.0051, 0.0040, 0.0046, 0.0045, 0.0058, 0.0048, 0.0041, 0.0051,\n",
       "          0.0051, 0.0059],\n",
       "         [0.0045, 0.0044, 0.0042, 0.0045, 0.0045, 0.0054, 0.0037, 0.0043, 0.0036,\n",
       "          0.0040, 0.0054],\n",
       "         [0.0049, 0.0057, 0.0052, 0.0049, 0.0038, 0.0044, 0.0042, 0.0050, 0.0042,\n",
       "          0.0057, 0.0037],\n",
       "         [0.0038, 0.0051, 0.0046, 0.0052, 0.0054, 0.0054, 0.0047, 0.0058, 0.0046,\n",
       "          0.0046, 0.0040],\n",
       "         [0.0037, 0.0043, 0.0064, 0.0042, 0.0055, 0.0029, 0.0065, 0.0057, 0.0038,\n",
       "          0.0032, 0.0030],\n",
       "         [0.0060, 0.0041, 0.0051, 0.0036, 0.0042, 0.0053, 0.0038, 0.0042, 0.0036,\n",
       "          0.0060, 0.0069],\n",
       "         [0.0045, 0.0046, 0.0060, 0.0034, 0.0038, 0.0046, 0.0046, 0.0046, 0.0049,\n",
       "          0.0049, 0.0030],\n",
       "         [0.0036, 0.0045, 0.0032, 0.0047, 0.0064, 0.0042, 0.0048, 0.0058, 0.0033,\n",
       "          0.0044, 0.0053],\n",
       "         [0.0033, 0.0047, 0.0051, 0.0053, 0.0032, 0.0050, 0.0042, 0.0043, 0.0054,\n",
       "          0.0034, 0.0048],\n",
       "         [0.0055, 0.0040, 0.0049, 0.0046, 0.0036, 0.0036, 0.0048, 0.0031, 0.0048,\n",
       "          0.0042, 0.0044],\n",
       "         [0.0065, 0.0043, 0.0055, 0.0043, 0.0051, 0.0035, 0.0034, 0.0038, 0.0051,\n",
       "          0.0037, 0.0040],\n",
       "         [0.0040, 0.0035, 0.0057, 0.0040, 0.0059, 0.0045, 0.0037, 0.0038, 0.0059,\n",
       "          0.0034, 0.0047],\n",
       "         [0.0035, 0.0050, 0.0044, 0.0050, 0.0031, 0.0030, 0.0055, 0.0041, 0.0047,\n",
       "          0.0051, 0.0041],\n",
       "         [0.0050, 0.0031, 0.0038, 0.0049, 0.0046, 0.0048, 0.0052, 0.0043, 0.0048,\n",
       "          0.0047, 0.0042],\n",
       "         [0.0051, 0.0053, 0.0043, 0.0059, 0.0042, 0.0036, 0.0048, 0.0047, 0.0039,\n",
       "          0.0058, 0.0049],\n",
       "         [0.0046, 0.0034, 0.0040, 0.0039, 0.0050, 0.0044, 0.0038, 0.0036, 0.0046,\n",
       "          0.0050, 0.0051],\n",
       "         [0.0047, 0.0039, 0.0039, 0.0057, 0.0049, 0.0037, 0.0044, 0.0058, 0.0049,\n",
       "          0.0051, 0.0041],\n",
       "         [0.0047, 0.0045, 0.0036, 0.0040, 0.0060, 0.0052, 0.0046, 0.0037, 0.0044,\n",
       "          0.0035, 0.0044]])]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9883)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(pi_star)\n",
    "Dist = 0.0\n",
    "for i in range(batch_size):\n",
    "    Dist+=torch.sum(pi_star[i]*cost_matrix[i])\n",
    "Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test = torch.randint(0,4, [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 0, 2, 2, 2, 0, 1, 0, 3])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "for i in true_test:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor(1),num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.nn.functional.one_hot(x, num_classes=11) for x in true_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi_g(y_true, num_classes=11):\n",
    "    batch_size = len(y_true)\n",
    "    pi_g = []\n",
    "    for i in range(batch_size):\n",
    "        pi_g_i = torch.stack(\n",
    "            [torch.nn.functional.one_hot(x, num_classes=num_classes) for x in y_true[i]]\n",
    "        )\n",
    "        pi_g.append(pi_g_i)\n",
    "\n",
    "    return pi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = [torch.randint(0,10,[5]), torch.randint(0,10,[11]), torch.randint(0,10,[6]),torch.randint(0,10,[20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8, 7, 2, 0, 8]),\n",
       " tensor([9, 1, 9, 8, 3, 5, 7, 0, 1, 8, 7]),\n",
       " tensor([7, 9, 5, 8, 5, 6]),\n",
       " tensor([5, 4, 1, 2, 9, 8, 0, 6, 3, 5, 5, 4, 7, 8, 1, 3, 6, 6, 0, 7])]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_g = get_pi_g(y_true=y_true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dist_pi_star = torch.rand([4])\n",
    "Dist_pi_g = torch.rand([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4224, 0.2023, 0.4167, 0.7400])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dist_pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0913, 0.7820, 0.7226, 0.8431])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dist_pi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3311, 0.5797, 0.3059, 0.1032])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(Dist_pi_star-Dist_pi_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3300)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(Dist_pi_star-Dist_pi_g).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_OT(Dist_pi_star, Dist_pi_g):\n",
    "    return torch.abs(Dist_pi_star-Dist_pi_g).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3300)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_OT(Dist_pi_star=Dist_pi_star,Dist_pi_g=Dist_pi_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 2, 0, 8])"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pi_g[0],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred(pi_star):\n",
    "    batch_size = len(pi_star)\n",
    "    y_pred = []\n",
    "    for i in range(batch_size):\n",
    "        y_pred_i = torch.argmax(pi_star[i],dim=-1).to(device)\n",
    "        y_pred.append(y_pred_i)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_y_pred(pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 10,  6,  9,  9,  7,  2,  4, 10,  5,  8,  7,  0,  3,  4,  2,  8,  3,\n",
       "         8,  9,  5,  6,  8,  0, 10, 10,  1,  7,  6, 10,  2,  4,  8,  0,  0,  4,\n",
       "         6,  6,  3, 10,  7,  4])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2, 10,  6,  9,  9,  7,  2,  4, 10,  5,  8]),\n",
       " tensor([7, 0, 3, 4, 2]),\n",
       " tensor([8, 3, 8, 9, 5, 6]),\n",
       " tensor([ 8,  0, 10, 10,  1,  7,  6, 10,  2,  4,  8,  0,  0,  4,  6,  6,  3, 10,\n",
       "          7,  4])]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_pytorch(a, b, M, lambda_sh=20, numItermax=1000, stopThr=5e-3):\n",
    "    \"\"\"\n",
    "    Compute the Sinkhorn optimal transport matrix using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        M (torch.Tensor): Cost matrix of shape (n, m).\n",
    "        a (torch.Tensor): Source distribution of shape (n,).\n",
    "        b (torch.Tensor): Target distribution of shape (m,).\n",
    "        lambda_sh (float): Regularization parameter (1 / epsilon).\n",
    "        numItermax (int): Maximum number of iterations.\n",
    "        stopThr (float): Stopping threshold for convergence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal transport matrix π of shape (n, m).\n",
    "    \"\"\"\n",
    "    K = torch.exp(-M * lambda_sh)  # Kernel matrix\n",
    "    u = torch.ones_like(a)  # Initialize u\n",
    "    v = torch.ones_like(b)  # Initialize v\n",
    "\n",
    "    for _ in range(numItermax):\n",
    "        u_prev = (\n",
    "            u.clone()\n",
    "        )  # Keep track of the previous value of u for convergence check\n",
    "        u = a / (K @ v)\n",
    "        v = b / (K.t() @ u)\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.norm(u - u_prev, p=1) < stopThr:\n",
    "            break\n",
    "\n",
    "    # Compute the optimal transport matrix π\n",
    "    pi = torch.diag(u) @ K @ torch.diag(v)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0156, 0.0163, 0.0175, 0.0116, 0.0145, 0.0117, 0.0144, 0.0162, 0.0179,\n",
       "         0.0147, 0.0167],\n",
       "        [0.0143, 0.0193, 0.0125, 0.0198, 0.0181, 0.0120, 0.0152, 0.0175, 0.0158,\n",
       "         0.0135, 0.0192],\n",
       "        [0.0169, 0.0132, 0.0133, 0.0175, 0.0171, 0.0154, 0.0146, 0.0175, 0.0179,\n",
       "         0.0150, 0.0152],\n",
       "        [0.0155, 0.0158, 0.0168, 0.0133, 0.0146, 0.0143, 0.0155, 0.0131, 0.0137,\n",
       "         0.0170, 0.0110],\n",
       "        [0.0150, 0.0121, 0.0126, 0.0138, 0.0134, 0.0193, 0.0117, 0.0091, 0.0139,\n",
       "         0.0174, 0.0168],\n",
       "        [0.0145, 0.0116, 0.0201, 0.0134, 0.0163, 0.0165, 0.0206, 0.0161, 0.0114,\n",
       "         0.0140, 0.0118]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinkhorn_pytorch(cost_matrix[2],D_W_P[2],D_T_P[2],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# OPTIMAL TRANSPORT NODE\n",
    "# Implementation of differentiable optimal transport using implicit differentiation. Makes use of Sinkhorn normalization\n",
    "# to solve the entropy regularized problem (Cuturi, NeurIPS 2013) in the forward pass. The problem can be written as\n",
    "# Let us write the entropy regularized optimal transport problem in the following form,\n",
    "#\n",
    "#    minimize (over P) <P, M> + 1/gamma KL(P || rc^T)\n",
    "#    subject to        P1 = r and P^T1 = c\n",
    "#\n",
    "# where r and c are m- and n-dimensional positive vectors, respectively, each summing to one. Here m-by-n matrix M is\n",
    "# the input and m-by-n dimensional positive matrix P is the output. The above problem leads to a solution of the form\n",
    "#\n",
    "#   P_{ij} = alpha_i beta_j e^{-gamma M_{ij}}\n",
    "#\n",
    "# where alpha and beta are found by iteratively applying row and column normalizations.\n",
    "#\n",
    "# We also provide an option to parametrize the input in log-space as M_{ij} = -log Q_{ij} where Q is a positive matrix.\n",
    "# The matrix Q becomes the input. This is more numerically stable for inputs M with large positive or negative values.\n",
    "#\n",
    "# See accompanying Jupyter Notebook at https://deepdeclarativenetworks.com.\n",
    "#\n",
    "# Stephen Gould <stephen.gould@anu.edu.au>\n",
    "# Dylan Campbell <dylan.campbell@anu.edu.au>\n",
    "# Fred Zhang <frederic.zhang@anu.edu.au>\n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "\n",
    "\n",
    "def sinkhorn(M, r=None, c=None, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False):\n",
    "    \"\"\"\n",
    "    PyTorch function for entropy regularized optimal transport. Assumes batched inputs as follows:\n",
    "        M:  (B,H,W) tensor\n",
    "        r:  (B,H) tensor, (1,H) tensor or None for constant uniform vector 1/H\n",
    "        c:  (B,W) tensor, (1,W) tensor or None for constant uniform vector 1/W\n",
    "\n",
    "    You can back propagate through this function in O(TBWH) time where T is the number of iterations taken to converge.\n",
    "    \"\"\"\n",
    "\n",
    "    B, H, W = M.shape\n",
    "    assert r is None or r.shape == (B, H) or r.shape == (1, H)\n",
    "    assert c is None or c.shape == (B, W) or c.shape == (1, W)\n",
    "    assert not logspace or torch.all(M > 0.0)\n",
    "\n",
    "    r = 1.0 / H if r is None else r.unsqueeze(dim=2)\n",
    "    c = 1.0 / W if c is None else c.unsqueeze(dim=1)\n",
    "\n",
    "    if logspace:\n",
    "        P = torch.pow(M, gamma)\n",
    "    else:\n",
    "        P = torch.exp(-1.0 * gamma * (M - torch.amin(M, 2, keepdim=True)))\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        alpha = torch.sum(P, 2)\n",
    "        # Perform division first for numerical stability\n",
    "        P = P / alpha.view(B, H, 1) * r\n",
    "\n",
    "        beta = torch.sum(P, 1)\n",
    "        if torch.max(torch.abs(beta - c)) <= eps:\n",
    "            break\n",
    "        P = P / beta.view(B, 1, W) * c\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "def _sinkhorn_inline(M, r=None, c=None, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False):\n",
    "    \"\"\"As above but with inline calculations for when autograd is not needed.\"\"\"\n",
    "\n",
    "    B, H, W = M.shape\n",
    "    assert r is None or r.shape == (B, H) or r.shape == (1, H)\n",
    "    assert c is None or c.shape == (B, W) or c.shape == (1, W)\n",
    "    assert not logspace or torch.all(M > 0.0)\n",
    "\n",
    "    r = 1.0 / H if r is None else r.unsqueeze(dim=2)\n",
    "    c = 1.0 / W if c is None else c.unsqueeze(dim=1)\n",
    "\n",
    "    if logspace:\n",
    "        P = torch.pow(M, gamma)\n",
    "    else:\n",
    "        P = torch.exp(-1.0 * gamma * (M - torch.amin(M, 2, keepdim=True)))\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        alpha = torch.sum(P, 2)\n",
    "        # Perform division first for numerical stability\n",
    "        P /= alpha.view(B, H, 1)\n",
    "        P *= r\n",
    "\n",
    "        beta = torch.sum(P, 1)\n",
    "        if torch.max(torch.abs(beta - c)) <= eps:\n",
    "            break\n",
    "        P /= beta.view(B, 1, W)\n",
    "        P *= c\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "class OptimalTransportFcn(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    PyTorch autograd function for entropy regularized optimal transport. Assumes batched inputs as follows:\n",
    "        M:  (B,H,W) tensor\n",
    "        r:  (B,H) tensor, (1,H) tensor or None for constant uniform vector\n",
    "        c:  (B,W) tensor, (1,W) tensor or None for constant uniform vector\n",
    "\n",
    "    Allows for approximate gradient calculations, which is faster and may be useful during early stages of learning,\n",
    "    when exp(-gamma M) is already nearly doubly stochastic, or when gradients are otherwise noisy.\n",
    "\n",
    "    Both r and c must be positive, if provided. They will be normalized to sum to one.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, M, r=None, c=None, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False, method='block'):\n",
    "        \"\"\"Solve optimal transport using skinhorn. Method can be 'block', 'full', 'fullchol' or 'approx'.\"\"\"\n",
    "        assert method in ('block', 'full', 'fullchol', 'approx')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # normalize r and c to ensure that they sum to one (and save normalization factor for backward pass)\n",
    "            if r is not None:\n",
    "                ctx.inv_r_sum = 1.0 / torch.sum(r, dim=1, keepdim=True)\n",
    "                r = ctx.inv_r_sum * r\n",
    "            if c is not None:\n",
    "                ctx.inv_c_sum = 1.0 / torch.sum(c, dim=1, keepdim=True)\n",
    "                c = ctx.inv_c_sum * c\n",
    "\n",
    "            # run sinkhorn\n",
    "            P = _sinkhorn_inline(M, r, c, gamma, eps, maxiters, logspace)\n",
    "\n",
    "        ctx.save_for_backward(M, r, c, P)\n",
    "        ctx.gamma = gamma\n",
    "        ctx.logspace = logspace\n",
    "        ctx.method = method\n",
    "\n",
    "        return P\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dJdP):\n",
    "        \"\"\"Implement backward pass using implicit differentiation.\"\"\"\n",
    "\n",
    "        M, r, c, P = ctx.saved_tensors\n",
    "        B, H, W = M.shape\n",
    "\n",
    "        # initialize backward gradients (-v^T H^{-1} B with v = dJdP and B = I or B = -1/r or B = -1/c)\n",
    "        dJdM = -1.0 * ctx.gamma * P * dJdP\n",
    "        dJdr = None if not ctx.needs_input_grad[1] else torch.zeros_like(r)\n",
    "        dJdc = None if not ctx.needs_input_grad[2] else torch.zeros_like(c)\n",
    "\n",
    "        # return approximate gradients\n",
    "        if ctx.method == 'approx':\n",
    "            return dJdM, dJdr, dJdc, None, None, None, None, None, None\n",
    "\n",
    "        # compute exact row and column sums (in case of small numerical errors or forward pass not converging)\n",
    "        alpha = torch.sum(P, dim=2)\n",
    "        beta = torch.sum(P, dim=1)\n",
    "\n",
    "        # compute [vHAt1, vHAt2] = v^T H^{-1} A^T as two blocks\n",
    "        vHAt1 = torch.sum(dJdM[:, 1:H, 0:W], dim=2).view(B, H - 1, 1)\n",
    "        vHAt2 = torch.sum(dJdM, dim=1).view(B, W, 1)\n",
    "\n",
    "        # compute [v1, v2] = -v^T H^{-1} A^T (A H^{-1] A^T)^{-1}\n",
    "        if ctx.method == 'block':\n",
    "            # by block inverse of (A H^{-1] A^T)\n",
    "            PdivC = P[:, 1:H, 0:W] / beta.view(B, 1, W)\n",
    "            RminusPPdivC = torch.diag_embed(alpha[:, 1:H]) - torch.bmm(P[:, 1:H, 0:W], PdivC.transpose(1, 2))\n",
    "            try:\n",
    "                block_11 = torch.linalg.cholesky(RminusPPdivC)\n",
    "            except:\n",
    "                # block_11 = torch.ones((B, H-1, H-1), device=M.device, dtype=M.dtype)\n",
    "                block_11 = torch.eye(H - 1, device=M.device, dtype=M.dtype).view(1, H - 1, H - 1).repeat(B, 1, 1)\n",
    "                for b in range(B):\n",
    "                    try:\n",
    "                        block_11[b, :, :] = torch.linalg.cholesky(RminusPPdivC[b, :, :])\n",
    "                    except:\n",
    "                        # keep initialized values (gradient will be close to zero)\n",
    "                        warnings.warn(\"backward pass encountered a singular matrix\")\n",
    "                        pass\n",
    "\n",
    "            block_12 = torch.cholesky_solve(PdivC, block_11)\n",
    "            #block_22 = torch.diag_embed(1.0 / beta) + torch.bmm(block_12.transpose(1, 2), PdivC)\n",
    "            block_22 = torch.bmm(block_12.transpose(1, 2), PdivC)\n",
    "\n",
    "            v1 = torch.cholesky_solve(vHAt1, block_11) - torch.bmm(block_12, vHAt2)\n",
    "            #v2 = torch.bmm(block_22, vHAt2) - torch.bmm(block_12.transpose(1, 2), vHAt1)\n",
    "            v2 = vHAt2 / beta.view(B, W, 1) + torch.bmm(block_22, vHAt2) - torch.bmm(block_12.transpose(1, 2), vHAt1)\n",
    "\n",
    "        else:\n",
    "            # by full inverse of (A H^{-1] A^T)\n",
    "            AinvHAt = torch.empty((B, H + W - 1, H + W - 1), device=M.device, dtype=M.dtype)\n",
    "            AinvHAt[:, 0:H - 1, 0:H - 1] = torch.diag_embed(alpha[:, 1:H])\n",
    "            AinvHAt[:, H - 1:H + W - 1, H - 1:H + W - 1] = torch.diag_embed(beta)\n",
    "            AinvHAt[:, 0:H - 1, H - 1:H + W - 1] = P[:, 1:H, 0:W]\n",
    "            AinvHAt[:, H - 1:H + W - 1, 0:H - 1] = P[:, 1:H, 0:W].transpose(1, 2)\n",
    "\n",
    "            if ctx.method == 'fullchol':\n",
    "                v = torch.cholesky_solve(torch.cat((vHAt1, vHAt2), dim=1), torch.linalg.cholesky(AinvHAt))\n",
    "            else:\n",
    "                v = torch.bmm(torch.inverse(AinvHAt), torch.cat((vHAt1, vHAt2), dim=1))\n",
    "                #v = torch.linalg.solve(AinvHAt, torch.cat((vHAt1, vHAt2), dim=1))\n",
    "\n",
    "            v1 = v[:, 0:H - 1].view(B, H - 1, 1)\n",
    "            v2 = v[:, H - 1:H + W - 1].view(B, W, 1)\n",
    "\n",
    "        # compute v^T H^{-1} A^T (A H^{-1] A^T)^{-1} A H^{-1} B - v^T H^{-1} B\n",
    "        dJdM[:, 1:H, 0:W] -= v1 * P[:, 1:H, 0:W]\n",
    "        dJdM -= v2.view(B, 1, W) * P\n",
    "\n",
    "        # multiply by derivative of log(M) if in log-space\n",
    "        if ctx.logspace:\n",
    "            dJdM /= -1.0 * M\n",
    "\n",
    "        # compute v^T H^{-1} A^T (A H^{-1] A^T)^{-1} (A H^{-1} B - C) - v^T H^{-1} B\n",
    "        if dJdr is not None:\n",
    "            dJdr = ctx.inv_r_sum.view(r.shape[0], 1) / ctx.gamma * \\\n",
    "                   (torch.sum(r[:, 1:H] * v1.view(B, H - 1), dim=1, keepdim=True) - torch.cat((torch.zeros(B, 1, device=r.device), v1.view(B, H - 1)), dim=1))\n",
    "\n",
    "        # compute v^T H^{-1} A^T (A H^{-1] A^T)^{-1} (A H^{-1} B - C) - v^T H^{-1} B\n",
    "        if dJdc is not None:\n",
    "            dJdc = ctx.inv_c_sum.view(c.shape[0], 1) / ctx.gamma * (torch.sum(c * v2.view(B, W), dim=1, keepdim=True) - v2.view(B, W))\n",
    "\n",
    "        # return gradients (None for gamma, eps, maxiters and logspace)\n",
    "        return dJdM, dJdr, dJdc, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class OptimalTransportLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network layer to implement optimal transport.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gamma: float, default: 1.0\n",
    "        Inverse of the coefficient on the entropy regularisation term.\n",
    "    eps: float, default: 1.0e-6\n",
    "        Tolerance used to determine the stop condition.\n",
    "    maxiters: int, default: 1000\n",
    "        The maximum number of iterations.\n",
    "    logspace: bool, default: False\n",
    "        If `True`, assumes that the input is provided as log M\n",
    "        If `False`, assumes that the input is provided as M (standard optimal transport)\n",
    "    method: str, default: 'block'\n",
    "        If `approx`, approximate the gradient by assuming exp(-gamma M) is already nearly doubly stochastic.\n",
    "        It is faster and could potentially be useful during early stages of training.\n",
    "        If `block`, exploit the block structure of matrix A H^{-1] A^T.\n",
    "        If `full`, invert the full A H^{-1} A^T matrix without exploiting the block structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, eps=1.0e-6, maxiters=1000, logspace=False, method='block'):\n",
    "        super(OptimalTransportLayer, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.maxiters = maxiters\n",
    "        self.logspace = logspace\n",
    "        self.method = method\n",
    "\n",
    "    def forward(self, M, r=None, c=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        M: torch.Tensor\n",
    "            Input matrix/matrices of size (H, W) or (B, H, W)\n",
    "        r: torch.Tensor, optional\n",
    "            Row sum constraint in the form of a 1xH or BxH matrix. Are assigned uniformly as 1/H by default.\n",
    "        c: torch.Tensor, optional\n",
    "            Column sum constraint in the form of a 1xW or BxW matrix. Are assigned uniformly as 1/W by default.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Normalised matrix/matrices, with the same shape as the inputs\n",
    "        \"\"\"\n",
    "        M_shape = M.shape\n",
    "        # Check the number of dimensions\n",
    "        ndim = len(M_shape)\n",
    "        if ndim == 2:\n",
    "            M = M.unsqueeze(dim=0)\n",
    "        elif ndim != 3:\n",
    "            raise ValueError(f\"The shape of the input tensor {M_shape} does not match that of an matrix\")\n",
    "\n",
    "        # Handle special case of 1x1 matrices\n",
    "        nr, nc = M_shape[-2:]\n",
    "        if nr == 1 and nc == 1:\n",
    "            P = torch.ones_like(M)\n",
    "        else:\n",
    "            P = OptimalTransportFcn.apply(M, r, c, self.gamma, self.eps, self.maxiters, self.logspace, self.method)\n",
    "\n",
    "        return P.view(*M_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT_layer = OptimalTransportLayer()\n",
    "output_pi_star = OT_layer(cost_matrix[2],D_W_P[2].unsqueeze(0),D_T_P[2].unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(cost_matrix)\n",
    "pi_star_tessss = []\n",
    "for i in range(batch_size):\n",
    "    pi_star_i = OT_layer(cost_matrix[i],D_W_P[i].unsqueeze(0), D_T_P[i].unsqueeze(0))\n",
    "    pi_star_tessss.append(pi_star_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0084, 0.0082, 0.0087, 0.0083, 0.0086, 0.0081, 0.0084, 0.0082, 0.0085,\n",
       "          0.0085, 0.0084],\n",
       "         [0.0088, 0.0084, 0.0088, 0.0086, 0.0091, 0.0086, 0.0088, 0.0086, 0.0088,\n",
       "          0.0088, 0.0088],\n",
       "         [0.0087, 0.0083, 0.0087, 0.0085, 0.0089, 0.0084, 0.0088, 0.0084, 0.0085,\n",
       "          0.0085, 0.0085],\n",
       "         [0.0086, 0.0081, 0.0086, 0.0082, 0.0085, 0.0081, 0.0085, 0.0083, 0.0083,\n",
       "          0.0086, 0.0083],\n",
       "         [0.0083, 0.0081, 0.0085, 0.0082, 0.0086, 0.0081, 0.0084, 0.0081, 0.0081,\n",
       "          0.0084, 0.0083],\n",
       "         [0.0084, 0.0081, 0.0084, 0.0083, 0.0086, 0.0081, 0.0084, 0.0083, 0.0082,\n",
       "          0.0084, 0.0082],\n",
       "         [0.0080, 0.0078, 0.0083, 0.0078, 0.0082, 0.0078, 0.0080, 0.0079, 0.0080,\n",
       "          0.0081, 0.0079],\n",
       "         [0.0078, 0.0076, 0.0080, 0.0077, 0.0082, 0.0077, 0.0079, 0.0078, 0.0077,\n",
       "          0.0079, 0.0078],\n",
       "         [0.0076, 0.0071, 0.0076, 0.0073, 0.0078, 0.0074, 0.0076, 0.0073, 0.0074,\n",
       "          0.0074, 0.0076],\n",
       "         [0.0088, 0.0085, 0.0087, 0.0086, 0.0089, 0.0087, 0.0088, 0.0085, 0.0087,\n",
       "          0.0086, 0.0087],\n",
       "         [0.0084, 0.0080, 0.0085, 0.0081, 0.0086, 0.0082, 0.0085, 0.0081, 0.0084,\n",
       "          0.0084, 0.0083]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0200, 0.0195, 0.0205, 0.0195, 0.0208, 0.0197, 0.0202, 0.0199, 0.0198,\n",
       "          0.0202, 0.0199],\n",
       "         [0.0182, 0.0172, 0.0182, 0.0177, 0.0184, 0.0177, 0.0181, 0.0176, 0.0176,\n",
       "          0.0179, 0.0180],\n",
       "         [0.0175, 0.0168, 0.0178, 0.0173, 0.0180, 0.0169, 0.0177, 0.0170, 0.0175,\n",
       "          0.0177, 0.0172],\n",
       "         [0.0173, 0.0166, 0.0172, 0.0169, 0.0179, 0.0169, 0.0172, 0.0168, 0.0172,\n",
       "          0.0173, 0.0169],\n",
       "         [0.0187, 0.0181, 0.0192, 0.0181, 0.0190, 0.0180, 0.0188, 0.0181, 0.0185,\n",
       "          0.0186, 0.0186]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0154, 0.0149, 0.0156, 0.0148, 0.0157, 0.0148, 0.0154, 0.0150, 0.0153,\n",
       "          0.0153, 0.0153],\n",
       "         [0.0161, 0.0158, 0.0162, 0.0160, 0.0167, 0.0156, 0.0162, 0.0159, 0.0161,\n",
       "          0.0161, 0.0162],\n",
       "         [0.0160, 0.0152, 0.0160, 0.0156, 0.0164, 0.0155, 0.0159, 0.0156, 0.0159,\n",
       "          0.0159, 0.0157],\n",
       "         [0.0148, 0.0143, 0.0150, 0.0143, 0.0151, 0.0143, 0.0148, 0.0143, 0.0145,\n",
       "          0.0148, 0.0144],\n",
       "         [0.0143, 0.0136, 0.0143, 0.0139, 0.0145, 0.0141, 0.0141, 0.0136, 0.0141,\n",
       "          0.0144, 0.0142],\n",
       "         [0.0152, 0.0145, 0.0157, 0.0148, 0.0157, 0.0149, 0.0155, 0.0150, 0.0149,\n",
       "          0.0152, 0.0149]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0044, 0.0043, 0.0045, 0.0043, 0.0046, 0.0044, 0.0045, 0.0043, 0.0045,\n",
       "          0.0045, 0.0044],\n",
       "         [0.0048, 0.0046, 0.0048, 0.0046, 0.0049, 0.0046, 0.0048, 0.0046, 0.0046,\n",
       "          0.0047, 0.0047],\n",
       "         [0.0050, 0.0048, 0.0050, 0.0048, 0.0051, 0.0049, 0.0050, 0.0048, 0.0049,\n",
       "          0.0050, 0.0049],\n",
       "         [0.0044, 0.0043, 0.0045, 0.0043, 0.0045, 0.0044, 0.0044, 0.0043, 0.0043,\n",
       "          0.0044, 0.0044],\n",
       "         [0.0048, 0.0046, 0.0048, 0.0046, 0.0048, 0.0046, 0.0047, 0.0046, 0.0047,\n",
       "          0.0048, 0.0046],\n",
       "         [0.0048, 0.0047, 0.0049, 0.0048, 0.0050, 0.0048, 0.0049, 0.0048, 0.0048,\n",
       "          0.0049, 0.0048],\n",
       "         [0.0045, 0.0044, 0.0047, 0.0044, 0.0047, 0.0043, 0.0046, 0.0045, 0.0044,\n",
       "          0.0044, 0.0044],\n",
       "         [0.0049, 0.0046, 0.0049, 0.0047, 0.0049, 0.0047, 0.0048, 0.0047, 0.0047,\n",
       "          0.0049, 0.0049],\n",
       "         [0.0045, 0.0043, 0.0046, 0.0043, 0.0046, 0.0044, 0.0045, 0.0044, 0.0045,\n",
       "          0.0045, 0.0044],\n",
       "         [0.0045, 0.0044, 0.0046, 0.0045, 0.0048, 0.0045, 0.0046, 0.0045, 0.0045,\n",
       "          0.0046, 0.0046],\n",
       "         [0.0044, 0.0043, 0.0046, 0.0044, 0.0045, 0.0044, 0.0045, 0.0044, 0.0045,\n",
       "          0.0044, 0.0045],\n",
       "         [0.0044, 0.0042, 0.0044, 0.0043, 0.0044, 0.0042, 0.0044, 0.0042, 0.0043,\n",
       "          0.0043, 0.0043],\n",
       "         [0.0046, 0.0043, 0.0046, 0.0044, 0.0047, 0.0043, 0.0045, 0.0044, 0.0045,\n",
       "          0.0045, 0.0044],\n",
       "         [0.0045, 0.0043, 0.0046, 0.0044, 0.0047, 0.0044, 0.0045, 0.0044, 0.0045,\n",
       "          0.0044, 0.0045],\n",
       "         [0.0043, 0.0042, 0.0044, 0.0043, 0.0044, 0.0042, 0.0044, 0.0043, 0.0043,\n",
       "          0.0044, 0.0043],\n",
       "         [0.0046, 0.0043, 0.0046, 0.0045, 0.0047, 0.0044, 0.0046, 0.0044, 0.0045,\n",
       "          0.0045, 0.0045],\n",
       "         [0.0048, 0.0047, 0.0048, 0.0047, 0.0049, 0.0046, 0.0048, 0.0047, 0.0047,\n",
       "          0.0048, 0.0048],\n",
       "         [0.0044, 0.0041, 0.0044, 0.0042, 0.0045, 0.0042, 0.0043, 0.0042, 0.0043,\n",
       "          0.0044, 0.0043],\n",
       "         [0.0047, 0.0045, 0.0047, 0.0046, 0.0048, 0.0045, 0.0047, 0.0046, 0.0047,\n",
       "          0.0047, 0.0046],\n",
       "         [0.0045, 0.0043, 0.0045, 0.0043, 0.0046, 0.0044, 0.0045, 0.0043, 0.0044,\n",
       "          0.0044, 0.0044]], grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star_tessss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/kkv19kbs5yb5yfn9jbkbfnl40000gn/T/ipykernel_17273/4276768131.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.rand([11,768]),requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(torch.rand([11,768]),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0228,  0.0673,  0.0134,  ...,  0.0033,  0.0552,  0.0141],\n",
       "        [ 0.0050,  0.0851,  0.0199,  ...,  0.0821, -0.0139,  0.0142],\n",
       "        [ 0.0420,  0.0297, -0.0061,  ..., -0.0962,  0.0189,  0.0341],\n",
       "        ...,\n",
       "        [ 0.0117,  0.0637, -0.0327,  ...,  0.0016,  0.0727, -0.0523],\n",
       "        [-0.0041,  0.0765,  0.0267,  ..., -0.0005,  0.0262,  0.0568],\n",
       "        [ 0.0381, -0.0104, -0.0840,  ..., -0.0699, -0.0283,  0.0368]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_normal_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0142,  0.0402, -0.0456,  ...,  0.0238, -0.0007, -0.0039],\n",
       "        [-0.0116,  0.0412,  0.0567,  ..., -0.0134, -0.0130, -0.0147],\n",
       "        [ 0.0558,  0.0363,  0.0165,  ...,  0.0330,  0.0237,  0.0063],\n",
       "        ...,\n",
       "        [-0.0449,  0.0437, -0.0218,  ..., -0.1183,  0.1327, -0.1092],\n",
       "        [-0.0793,  0.0837,  0.0344,  ..., -0.0134, -0.0063, -0.1125],\n",
       "        [ 0.0041, -0.1012,  0.0080,  ..., -0.0110, -0.0322,  0.0109]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.empty([11,767],requires_grad=True)\n",
    "torch.nn.init.xavier_normal_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000], Loss: 6.9158\n",
      "Epoch [100/100000], Loss: 6.8985\n",
      "Epoch [200/100000], Loss: 6.8813\n",
      "Epoch [300/100000], Loss: 6.8641\n",
      "Epoch [400/100000], Loss: 6.8469\n",
      "Epoch [500/100000], Loss: 6.8297\n",
      "Epoch [600/100000], Loss: 6.8125\n",
      "Epoch [700/100000], Loss: 6.7953\n",
      "Epoch [800/100000], Loss: 6.7782\n",
      "Epoch [900/100000], Loss: 6.7611\n",
      "Epoch [1000/100000], Loss: 6.7440\n",
      "Epoch [1100/100000], Loss: 6.7269\n",
      "Epoch [1200/100000], Loss: 6.7098\n",
      "Epoch [1300/100000], Loss: 6.6927\n",
      "Epoch [1400/100000], Loss: 6.6756\n",
      "Epoch [1500/100000], Loss: 6.6585\n",
      "Epoch [1600/100000], Loss: 6.6415\n",
      "Epoch [1700/100000], Loss: 6.6244\n",
      "Epoch [1800/100000], Loss: 6.6074\n",
      "Epoch [1900/100000], Loss: 6.5903\n",
      "Epoch [2000/100000], Loss: 6.5733\n",
      "Epoch [2100/100000], Loss: 6.5563\n",
      "Epoch [2200/100000], Loss: 6.5392\n",
      "Epoch [2300/100000], Loss: 6.5222\n",
      "Epoch [2400/100000], Loss: 6.5051\n",
      "Epoch [2500/100000], Loss: 6.4881\n",
      "Epoch [2600/100000], Loss: 6.4711\n",
      "Epoch [2700/100000], Loss: 6.4541\n",
      "Epoch [2800/100000], Loss: 6.4370\n",
      "Epoch [2900/100000], Loss: 6.4200\n",
      "Epoch [3000/100000], Loss: 6.4030\n",
      "Epoch [3100/100000], Loss: 6.3860\n",
      "Epoch [3200/100000], Loss: 6.3690\n",
      "Epoch [3300/100000], Loss: 6.3519\n",
      "Epoch [3400/100000], Loss: 6.3349\n",
      "Epoch [3500/100000], Loss: 6.3179\n",
      "Epoch [3600/100000], Loss: 6.3009\n",
      "Epoch [3700/100000], Loss: 6.2839\n",
      "Epoch [3800/100000], Loss: 6.2669\n",
      "Epoch [3900/100000], Loss: 6.2499\n",
      "Epoch [4000/100000], Loss: 6.2329\n",
      "Epoch [4100/100000], Loss: 6.2159\n",
      "Epoch [4200/100000], Loss: 6.1989\n",
      "Epoch [4300/100000], Loss: 6.1819\n",
      "Epoch [4400/100000], Loss: 6.1649\n",
      "Epoch [4500/100000], Loss: 6.1479\n",
      "Epoch [4600/100000], Loss: 6.1309\n",
      "Epoch [4700/100000], Loss: 6.1139\n",
      "Epoch [4800/100000], Loss: 6.0969\n",
      "Epoch [4900/100000], Loss: 6.0799\n",
      "Epoch [5000/100000], Loss: 6.0629\n",
      "Epoch [5100/100000], Loss: 6.0460\n",
      "Epoch [5200/100000], Loss: 6.0290\n",
      "Epoch [5300/100000], Loss: 6.0120\n",
      "Epoch [5400/100000], Loss: 5.9950\n",
      "Epoch [5500/100000], Loss: 5.9780\n",
      "Epoch [5600/100000], Loss: 5.9611\n",
      "Epoch [5700/100000], Loss: 5.9441\n",
      "Epoch [5800/100000], Loss: 5.9271\n",
      "Epoch [5900/100000], Loss: 5.9102\n",
      "Epoch [6000/100000], Loss: 5.8932\n",
      "Epoch [6100/100000], Loss: 5.8762\n",
      "Epoch [6200/100000], Loss: 5.8593\n",
      "Epoch [6300/100000], Loss: 5.8423\n",
      "Epoch [6400/100000], Loss: 5.8254\n",
      "Epoch [6500/100000], Loss: 5.8084\n",
      "Epoch [6600/100000], Loss: 5.7915\n",
      "Epoch [6700/100000], Loss: 5.7745\n",
      "Epoch [6800/100000], Loss: 5.7576\n",
      "Epoch [6900/100000], Loss: 5.7406\n",
      "Epoch [7000/100000], Loss: 5.7237\n",
      "Epoch [7100/100000], Loss: 5.7067\n",
      "Epoch [7200/100000], Loss: 5.6898\n",
      "Epoch [7300/100000], Loss: 5.6729\n",
      "Epoch [7400/100000], Loss: 5.6559\n",
      "Epoch [7500/100000], Loss: 5.6390\n",
      "Epoch [7600/100000], Loss: 5.6221\n",
      "Epoch [7700/100000], Loss: 5.6052\n",
      "Epoch [7800/100000], Loss: 5.5883\n",
      "Epoch [7900/100000], Loss: 5.5713\n",
      "Epoch [8000/100000], Loss: 5.5544\n",
      "Epoch [8100/100000], Loss: 5.5375\n",
      "Epoch [8200/100000], Loss: 5.5206\n",
      "Epoch [8300/100000], Loss: 5.5037\n",
      "Epoch [8400/100000], Loss: 5.4868\n",
      "Epoch [8500/100000], Loss: 5.4699\n",
      "Epoch [8600/100000], Loss: 5.4530\n",
      "Epoch [8700/100000], Loss: 5.4361\n",
      "Epoch [8800/100000], Loss: 5.4192\n",
      "Epoch [8900/100000], Loss: 5.4023\n",
      "Epoch [9000/100000], Loss: 5.3854\n",
      "Epoch [9100/100000], Loss: 5.3685\n",
      "Epoch [9200/100000], Loss: 5.3516\n",
      "Epoch [9300/100000], Loss: 5.3347\n",
      "Epoch [9400/100000], Loss: 5.3178\n",
      "Epoch [9500/100000], Loss: 5.3010\n",
      "Epoch [9600/100000], Loss: 5.2841\n",
      "Epoch [9700/100000], Loss: 5.2672\n",
      "Epoch [9800/100000], Loss: 5.2503\n",
      "Epoch [9900/100000], Loss: 5.2335\n",
      "Epoch [10000/100000], Loss: 5.2166\n",
      "Epoch [10100/100000], Loss: 5.1997\n",
      "Epoch [10200/100000], Loss: 5.1829\n",
      "Epoch [10300/100000], Loss: 5.1660\n",
      "Epoch [10400/100000], Loss: 5.1492\n",
      "Epoch [10500/100000], Loss: 5.1323\n",
      "Epoch [10600/100000], Loss: 5.1155\n",
      "Epoch [10700/100000], Loss: 5.0986\n",
      "Epoch [10800/100000], Loss: 5.0818\n",
      "Epoch [10900/100000], Loss: 5.0650\n",
      "Epoch [11000/100000], Loss: 5.0481\n",
      "Epoch [11100/100000], Loss: 5.0313\n",
      "Epoch [11200/100000], Loss: 5.0145\n",
      "Epoch [11300/100000], Loss: 4.9976\n",
      "Epoch [11400/100000], Loss: 4.9808\n",
      "Epoch [11500/100000], Loss: 4.9640\n",
      "Epoch [11600/100000], Loss: 4.9472\n",
      "Epoch [11700/100000], Loss: 4.9304\n",
      "Epoch [11800/100000], Loss: 4.9136\n",
      "Epoch [11900/100000], Loss: 4.8968\n",
      "Epoch [12000/100000], Loss: 4.8800\n",
      "Epoch [12100/100000], Loss: 4.8632\n",
      "Epoch [12200/100000], Loss: 4.8464\n",
      "Epoch [12300/100000], Loss: 4.8296\n",
      "Epoch [12400/100000], Loss: 4.8128\n",
      "Epoch [12500/100000], Loss: 4.7960\n",
      "Epoch [12600/100000], Loss: 4.7792\n",
      "Epoch [12700/100000], Loss: 4.7625\n",
      "Epoch [12800/100000], Loss: 4.7457\n",
      "Epoch [12900/100000], Loss: 4.7289\n",
      "Epoch [13000/100000], Loss: 4.7122\n",
      "Epoch [13100/100000], Loss: 4.6954\n",
      "Epoch [13200/100000], Loss: 4.6787\n",
      "Epoch [13300/100000], Loss: 4.6619\n",
      "Epoch [13400/100000], Loss: 4.6452\n",
      "Epoch [13500/100000], Loss: 4.6284\n",
      "Epoch [13600/100000], Loss: 4.6117\n",
      "Epoch [13700/100000], Loss: 4.5950\n",
      "Epoch [13800/100000], Loss: 4.5782\n",
      "Epoch [13900/100000], Loss: 4.5615\n",
      "Epoch [14000/100000], Loss: 4.5448\n",
      "Epoch [14100/100000], Loss: 4.5281\n",
      "Epoch [14200/100000], Loss: 4.5114\n",
      "Epoch [14300/100000], Loss: 4.4947\n",
      "Epoch [14400/100000], Loss: 4.4780\n",
      "Epoch [14500/100000], Loss: 4.4613\n",
      "Epoch [14600/100000], Loss: 4.4446\n",
      "Epoch [14700/100000], Loss: 4.4279\n",
      "Epoch [14800/100000], Loss: 4.4112\n",
      "Epoch [14900/100000], Loss: 4.3945\n",
      "Epoch [15000/100000], Loss: 4.3779\n",
      "Epoch [15100/100000], Loss: 4.3612\n",
      "Epoch [15200/100000], Loss: 4.3445\n",
      "Epoch [15300/100000], Loss: 4.3279\n",
      "Epoch [15400/100000], Loss: 4.3112\n",
      "Epoch [15500/100000], Loss: 4.2946\n",
      "Epoch [15600/100000], Loss: 4.2779\n",
      "Epoch [15700/100000], Loss: 4.2613\n",
      "Epoch [15800/100000], Loss: 4.2447\n",
      "Epoch [15900/100000], Loss: 4.2281\n",
      "Epoch [16000/100000], Loss: 4.2114\n",
      "Epoch [16100/100000], Loss: 4.1948\n",
      "Epoch [16200/100000], Loss: 4.1782\n",
      "Epoch [16300/100000], Loss: 4.1616\n",
      "Epoch [16400/100000], Loss: 4.1450\n",
      "Epoch [16500/100000], Loss: 4.1284\n",
      "Epoch [16600/100000], Loss: 4.1118\n",
      "Epoch [16700/100000], Loss: 4.0953\n",
      "Epoch [16800/100000], Loss: 4.0787\n",
      "Epoch [16900/100000], Loss: 4.0621\n",
      "Epoch [17000/100000], Loss: 4.0455\n",
      "Epoch [17100/100000], Loss: 4.0290\n",
      "Epoch [17200/100000], Loss: 4.0124\n",
      "Epoch [17300/100000], Loss: 3.9959\n",
      "Epoch [17400/100000], Loss: 3.9794\n",
      "Epoch [17500/100000], Loss: 3.9628\n",
      "Epoch [17600/100000], Loss: 3.9463\n",
      "Epoch [17700/100000], Loss: 3.9298\n",
      "Epoch [17800/100000], Loss: 3.9133\n",
      "Epoch [17900/100000], Loss: 3.8968\n",
      "Epoch [18000/100000], Loss: 3.8803\n",
      "Epoch [18100/100000], Loss: 3.8638\n",
      "Epoch [18200/100000], Loss: 3.8473\n",
      "Epoch [18300/100000], Loss: 3.8309\n",
      "Epoch [18400/100000], Loss: 3.8144\n",
      "Epoch [18500/100000], Loss: 3.7979\n",
      "Epoch [18600/100000], Loss: 3.7815\n",
      "Epoch [18700/100000], Loss: 3.7651\n",
      "Epoch [18800/100000], Loss: 3.7486\n",
      "Epoch [18900/100000], Loss: 3.7322\n",
      "Epoch [19000/100000], Loss: 3.7158\n",
      "Epoch [19100/100000], Loss: 3.6993\n",
      "Epoch [19200/100000], Loss: 3.6830\n",
      "Epoch [19300/100000], Loss: 3.6666\n",
      "Epoch [19400/100000], Loss: 3.6502\n",
      "Epoch [19500/100000], Loss: 3.6338\n",
      "Epoch [19600/100000], Loss: 3.6174\n",
      "Epoch [19700/100000], Loss: 3.6011\n",
      "Epoch [19800/100000], Loss: 3.5847\n",
      "Epoch [19900/100000], Loss: 3.5684\n",
      "Epoch [20000/100000], Loss: 3.5521\n",
      "Epoch [20100/100000], Loss: 3.5357\n",
      "Epoch [20200/100000], Loss: 3.5194\n",
      "Epoch [20300/100000], Loss: 3.5031\n",
      "Epoch [20400/100000], Loss: 3.4868\n",
      "Epoch [20500/100000], Loss: 3.4705\n",
      "Epoch [20600/100000], Loss: 3.4542\n",
      "Epoch [20700/100000], Loss: 3.4380\n",
      "Epoch [20800/100000], Loss: 3.4217\n",
      "Epoch [20900/100000], Loss: 3.4055\n",
      "Epoch [21000/100000], Loss: 3.3892\n",
      "Epoch [21100/100000], Loss: 3.3730\n",
      "Epoch [21200/100000], Loss: 3.3568\n",
      "Epoch [21300/100000], Loss: 3.3406\n",
      "Epoch [21400/100000], Loss: 3.3244\n",
      "Epoch [21500/100000], Loss: 3.3082\n",
      "Epoch [21600/100000], Loss: 3.2920\n",
      "Epoch [21700/100000], Loss: 3.2759\n",
      "Epoch [21800/100000], Loss: 3.2597\n",
      "Epoch [21900/100000], Loss: 3.2436\n",
      "Epoch [22000/100000], Loss: 3.2275\n",
      "Epoch [22100/100000], Loss: 3.2114\n",
      "Epoch [22200/100000], Loss: 3.1953\n",
      "Epoch [22300/100000], Loss: 3.1792\n",
      "Epoch [22400/100000], Loss: 3.1631\n",
      "Epoch [22500/100000], Loss: 3.1471\n",
      "Epoch [22600/100000], Loss: 3.1310\n",
      "Epoch [22700/100000], Loss: 3.1150\n",
      "Epoch [22800/100000], Loss: 3.0990\n",
      "Epoch [22900/100000], Loss: 3.0830\n",
      "Epoch [23000/100000], Loss: 3.0670\n",
      "Epoch [23100/100000], Loss: 3.0510\n",
      "Epoch [23200/100000], Loss: 3.0350\n",
      "Epoch [23300/100000], Loss: 3.0191\n",
      "Epoch [23400/100000], Loss: 3.0031\n",
      "Epoch [23500/100000], Loss: 2.9872\n",
      "Epoch [23600/100000], Loss: 2.9713\n",
      "Epoch [23700/100000], Loss: 2.9554\n",
      "Epoch [23800/100000], Loss: 2.9395\n",
      "Epoch [23900/100000], Loss: 2.9237\n",
      "Epoch [24000/100000], Loss: 2.9078\n",
      "Epoch [24100/100000], Loss: 2.8920\n",
      "Epoch [24200/100000], Loss: 2.8762\n",
      "Epoch [24300/100000], Loss: 2.8604\n",
      "Epoch [24400/100000], Loss: 2.8446\n",
      "Epoch [24500/100000], Loss: 2.8289\n",
      "Epoch [24600/100000], Loss: 2.8131\n",
      "Epoch [24700/100000], Loss: 2.7974\n",
      "Epoch [24800/100000], Loss: 2.7817\n",
      "Epoch [24900/100000], Loss: 2.7660\n",
      "Epoch [25000/100000], Loss: 2.7503\n",
      "Epoch [25100/100000], Loss: 2.7347\n",
      "Epoch [25200/100000], Loss: 2.7191\n",
      "Epoch [25300/100000], Loss: 2.7035\n",
      "Epoch [25400/100000], Loss: 2.6879\n",
      "Epoch [25500/100000], Loss: 2.6723\n",
      "Epoch [25600/100000], Loss: 2.6567\n",
      "Epoch [25700/100000], Loss: 2.6412\n",
      "Epoch [25800/100000], Loss: 2.6257\n",
      "Epoch [25900/100000], Loss: 2.6102\n",
      "Epoch [26000/100000], Loss: 2.5948\n",
      "Epoch [26100/100000], Loss: 2.5793\n",
      "Epoch [26200/100000], Loss: 2.5639\n",
      "Epoch [26300/100000], Loss: 2.5485\n",
      "Epoch [26400/100000], Loss: 2.5331\n",
      "Epoch [26500/100000], Loss: 2.5178\n",
      "Epoch [26600/100000], Loss: 2.5025\n",
      "Epoch [26700/100000], Loss: 2.4872\n",
      "Epoch [26800/100000], Loss: 2.4719\n",
      "Epoch [26900/100000], Loss: 2.4566\n",
      "Epoch [27000/100000], Loss: 2.4414\n",
      "Epoch [27100/100000], Loss: 2.4262\n",
      "Epoch [27200/100000], Loss: 2.4111\n",
      "Epoch [27300/100000], Loss: 2.3959\n",
      "Epoch [27400/100000], Loss: 2.3808\n",
      "Epoch [27500/100000], Loss: 2.3657\n",
      "Epoch [27600/100000], Loss: 2.3507\n",
      "Epoch [27700/100000], Loss: 2.3356\n",
      "Epoch [27800/100000], Loss: 2.3206\n",
      "Epoch [27900/100000], Loss: 2.3057\n",
      "Epoch [28000/100000], Loss: 2.2907\n",
      "Epoch [28100/100000], Loss: 2.2758\n",
      "Epoch [28200/100000], Loss: 2.2610\n",
      "Epoch [28300/100000], Loss: 2.2461\n",
      "Epoch [28400/100000], Loss: 2.2313\n",
      "Epoch [28500/100000], Loss: 2.2166\n",
      "Epoch [28600/100000], Loss: 2.2018\n",
      "Epoch [28700/100000], Loss: 2.1871\n",
      "Epoch [28800/100000], Loss: 2.1724\n",
      "Epoch [28900/100000], Loss: 2.1578\n",
      "Epoch [29000/100000], Loss: 2.1432\n",
      "Epoch [29100/100000], Loss: 2.1287\n",
      "Epoch [29200/100000], Loss: 2.1141\n",
      "Epoch [29300/100000], Loss: 2.0996\n",
      "Epoch [29400/100000], Loss: 2.0852\n",
      "Epoch [29500/100000], Loss: 2.0708\n",
      "Epoch [29600/100000], Loss: 2.0564\n",
      "Epoch [29700/100000], Loss: 2.0421\n",
      "Epoch [29800/100000], Loss: 2.0278\n",
      "Epoch [29900/100000], Loss: 2.0136\n",
      "Epoch [30000/100000], Loss: 1.9994\n",
      "Epoch [30100/100000], Loss: 1.9853\n",
      "Epoch [30200/100000], Loss: 1.9711\n",
      "Epoch [30300/100000], Loss: 1.9571\n",
      "Epoch [30400/100000], Loss: 1.9431\n",
      "Epoch [30500/100000], Loss: 1.9291\n",
      "Epoch [30600/100000], Loss: 1.9152\n",
      "Epoch [30700/100000], Loss: 1.9013\n",
      "Epoch [30800/100000], Loss: 1.8875\n",
      "Epoch [30900/100000], Loss: 1.8737\n",
      "Epoch [31000/100000], Loss: 1.8600\n",
      "Epoch [31100/100000], Loss: 1.8463\n",
      "Epoch [31200/100000], Loss: 1.8327\n",
      "Epoch [31300/100000], Loss: 1.8192\n",
      "Epoch [31400/100000], Loss: 1.8057\n",
      "Epoch [31500/100000], Loss: 1.7922\n",
      "Epoch [31600/100000], Loss: 1.7788\n",
      "Epoch [31700/100000], Loss: 1.7655\n",
      "Epoch [31800/100000], Loss: 1.7522\n",
      "Epoch [31900/100000], Loss: 1.7390\n",
      "Epoch [32000/100000], Loss: 1.7258\n",
      "Epoch [32100/100000], Loss: 1.7127\n",
      "Epoch [32200/100000], Loss: 1.6996\n",
      "Epoch [32300/100000], Loss: 1.6867\n",
      "Epoch [32400/100000], Loss: 1.6737\n",
      "Epoch [32500/100000], Loss: 1.6609\n",
      "Epoch [32600/100000], Loss: 1.6481\n",
      "Epoch [32700/100000], Loss: 1.6354\n",
      "Epoch [32800/100000], Loss: 1.6227\n",
      "Epoch [32900/100000], Loss: 1.6101\n",
      "Epoch [33000/100000], Loss: 1.5976\n",
      "Epoch [33100/100000], Loss: 1.5851\n",
      "Epoch [33200/100000], Loss: 1.5727\n",
      "Epoch [33300/100000], Loss: 1.5604\n",
      "Epoch [33400/100000], Loss: 1.5482\n",
      "Epoch [33500/100000], Loss: 1.5360\n",
      "Epoch [33600/100000], Loss: 1.5238\n",
      "Epoch [33700/100000], Loss: 1.5118\n",
      "Epoch [33800/100000], Loss: 1.4998\n",
      "Epoch [33900/100000], Loss: 1.4879\n",
      "Epoch [34000/100000], Loss: 1.4761\n",
      "Epoch [34100/100000], Loss: 1.4643\n",
      "Epoch [34200/100000], Loss: 1.4526\n",
      "Epoch [34300/100000], Loss: 1.4410\n",
      "Epoch [34400/100000], Loss: 1.4295\n",
      "Epoch [34500/100000], Loss: 1.4180\n",
      "Epoch [34600/100000], Loss: 1.4066\n",
      "Epoch [34700/100000], Loss: 1.3952\n",
      "Epoch [34800/100000], Loss: 1.3839\n",
      "Epoch [34900/100000], Loss: 1.3727\n",
      "Epoch [35000/100000], Loss: 1.3616\n",
      "Epoch [35100/100000], Loss: 1.3505\n",
      "Epoch [35200/100000], Loss: 1.3395\n",
      "Epoch [35300/100000], Loss: 1.3286\n",
      "Epoch [35400/100000], Loss: 1.3177\n",
      "Epoch [35500/100000], Loss: 1.3069\n",
      "Epoch [35600/100000], Loss: 1.2961\n",
      "Epoch [35700/100000], Loss: 1.2854\n",
      "Epoch [35800/100000], Loss: 1.2747\n",
      "Epoch [35900/100000], Loss: 1.2641\n",
      "Epoch [36000/100000], Loss: 1.2536\n",
      "Epoch [36100/100000], Loss: 1.2431\n",
      "Epoch [36200/100000], Loss: 1.2326\n",
      "Epoch [36300/100000], Loss: 1.2222\n",
      "Epoch [36400/100000], Loss: 1.2118\n",
      "Epoch [36500/100000], Loss: 1.2014\n",
      "Epoch [36600/100000], Loss: 1.1911\n",
      "Epoch [36700/100000], Loss: 1.1809\n",
      "Epoch [36800/100000], Loss: 1.1706\n",
      "Epoch [36900/100000], Loss: 1.1604\n",
      "Epoch [37000/100000], Loss: 1.1502\n",
      "Epoch [37100/100000], Loss: 1.1400\n",
      "Epoch [37200/100000], Loss: 1.1299\n",
      "Epoch [37300/100000], Loss: 1.1198\n",
      "Epoch [37400/100000], Loss: 1.1097\n",
      "Epoch [37500/100000], Loss: 1.0996\n",
      "Epoch [37600/100000], Loss: 1.0895\n",
      "Epoch [37700/100000], Loss: 1.0794\n",
      "Epoch [37800/100000], Loss: 1.0694\n",
      "Epoch [37900/100000], Loss: 1.0593\n",
      "Epoch [38000/100000], Loss: 1.0493\n",
      "Epoch [38100/100000], Loss: 1.0393\n",
      "Epoch [38200/100000], Loss: 1.0293\n",
      "Epoch [38300/100000], Loss: 1.0192\n",
      "Epoch [38400/100000], Loss: 1.0092\n",
      "Epoch [38500/100000], Loss: 0.9992\n",
      "Epoch [38600/100000], Loss: 0.9892\n",
      "Epoch [38700/100000], Loss: 0.9792\n",
      "Epoch [38800/100000], Loss: 0.9692\n",
      "Epoch [38900/100000], Loss: 0.9591\n",
      "Epoch [39000/100000], Loss: 0.9491\n",
      "Epoch [39100/100000], Loss: 0.9391\n",
      "Epoch [39200/100000], Loss: 0.9291\n",
      "Epoch [39300/100000], Loss: 0.9191\n",
      "Epoch [39400/100000], Loss: 0.9091\n",
      "Epoch [39500/100000], Loss: 0.8991\n",
      "Epoch [39600/100000], Loss: 0.8890\n",
      "Epoch [39700/100000], Loss: 0.8790\n",
      "Epoch [39800/100000], Loss: 0.8690\n",
      "Epoch [39900/100000], Loss: 0.8590\n",
      "Epoch [40000/100000], Loss: 0.8490\n",
      "Epoch [40100/100000], Loss: 0.8390\n",
      "Epoch [40200/100000], Loss: 0.8290\n",
      "Epoch [40300/100000], Loss: 0.8189\n",
      "Epoch [40400/100000], Loss: 0.8090\n",
      "Epoch [40500/100000], Loss: 0.7990\n",
      "Epoch [40600/100000], Loss: 0.7890\n",
      "Epoch [40700/100000], Loss: 0.7790\n",
      "Epoch [40800/100000], Loss: 0.7690\n",
      "Epoch [40900/100000], Loss: 0.7590\n",
      "Epoch [41000/100000], Loss: 0.7490\n",
      "Epoch [41100/100000], Loss: 0.7390\n",
      "Epoch [41200/100000], Loss: 0.7290\n",
      "Epoch [41300/100000], Loss: 0.7190\n",
      "Epoch [41400/100000], Loss: 0.7091\n",
      "Epoch [41500/100000], Loss: 0.6991\n",
      "Epoch [41600/100000], Loss: 0.6891\n",
      "Epoch [41700/100000], Loss: 0.6791\n",
      "Epoch [41800/100000], Loss: 0.6691\n",
      "Epoch [41900/100000], Loss: 0.6591\n",
      "Epoch [42000/100000], Loss: 0.6491\n",
      "Epoch [42100/100000], Loss: 0.6391\n",
      "Epoch [42200/100000], Loss: 0.6291\n",
      "Epoch [42300/100000], Loss: 0.6191\n",
      "Epoch [42400/100000], Loss: 0.6092\n",
      "Epoch [42500/100000], Loss: 0.5992\n",
      "Epoch [42600/100000], Loss: 0.5892\n",
      "Epoch [42700/100000], Loss: 0.5792\n",
      "Epoch [42800/100000], Loss: 0.5692\n",
      "Epoch [42900/100000], Loss: 0.5592\n",
      "Epoch [43000/100000], Loss: 0.5492\n",
      "Epoch [43100/100000], Loss: 0.5392\n",
      "Epoch [43200/100000], Loss: 0.5292\n",
      "Epoch [43300/100000], Loss: 0.5192\n",
      "Epoch [43400/100000], Loss: 0.5093\n",
      "Epoch [43500/100000], Loss: 0.4993\n",
      "Epoch [43600/100000], Loss: 0.4893\n",
      "Epoch [43700/100000], Loss: 0.4793\n",
      "Epoch [43800/100000], Loss: 0.4693\n",
      "Epoch [43900/100000], Loss: 0.4593\n",
      "Epoch [44000/100000], Loss: 0.4493\n",
      "Epoch [44100/100000], Loss: 0.4393\n",
      "Epoch [44200/100000], Loss: 0.4293\n",
      "Epoch [44300/100000], Loss: 0.4194\n",
      "Epoch [44400/100000], Loss: 0.4094\n",
      "Epoch [44500/100000], Loss: 0.3994\n",
      "Epoch [44600/100000], Loss: 0.3894\n",
      "Epoch [44700/100000], Loss: 0.3794\n",
      "Epoch [44800/100000], Loss: 0.3694\n",
      "Epoch [44900/100000], Loss: 0.3594\n",
      "Epoch [45000/100000], Loss: 0.3494\n",
      "Epoch [45100/100000], Loss: 0.3394\n",
      "Epoch [45200/100000], Loss: 0.3294\n",
      "Epoch [45300/100000], Loss: 0.3195\n",
      "Epoch [45400/100000], Loss: 0.3095\n",
      "Epoch [45500/100000], Loss: 0.2995\n",
      "Epoch [45600/100000], Loss: 0.2895\n",
      "Epoch [45700/100000], Loss: 0.2795\n",
      "Epoch [45800/100000], Loss: 0.2695\n",
      "Epoch [45900/100000], Loss: 0.2595\n",
      "Epoch [46000/100000], Loss: 0.2495\n",
      "Epoch [46100/100000], Loss: 0.2395\n",
      "Epoch [46200/100000], Loss: 0.2295\n",
      "Epoch [46300/100000], Loss: 0.2196\n",
      "Epoch [46400/100000], Loss: 0.2096\n",
      "Epoch [46500/100000], Loss: 0.1996\n",
      "Epoch [46600/100000], Loss: 0.1896\n",
      "Epoch [46700/100000], Loss: 0.1796\n",
      "Epoch [46800/100000], Loss: 0.1696\n",
      "Epoch [46900/100000], Loss: 0.1596\n",
      "Epoch [47000/100000], Loss: 0.1496\n",
      "Epoch [47100/100000], Loss: 0.1396\n",
      "Epoch [47200/100000], Loss: 0.1296\n",
      "Epoch [47300/100000], Loss: 0.1197\n",
      "Epoch [47400/100000], Loss: 0.1097\n",
      "Epoch [47500/100000], Loss: 0.0997\n",
      "Epoch [47600/100000], Loss: 0.0897\n",
      "Epoch [47700/100000], Loss: 0.0797\n",
      "Epoch [47800/100000], Loss: 0.0697\n",
      "Epoch [47900/100000], Loss: 0.0597\n",
      "Epoch [48000/100000], Loss: 0.0497\n",
      "Epoch [48100/100000], Loss: 0.0397\n",
      "Epoch [48200/100000], Loss: 0.0298\n",
      "Epoch [48300/100000], Loss: 0.0198\n",
      "Epoch [48400/100000], Loss: 0.0098\n",
      "Epoch [48500/100000], Loss: 0.0002\n",
      "Epoch [48600/100000], Loss: 0.0000\n",
      "Epoch [48700/100000], Loss: 0.0000\n",
      "Epoch [48800/100000], Loss: 0.0000\n",
      "Epoch [48900/100000], Loss: 0.0000\n",
      "Epoch [49000/100000], Loss: 0.0000\n",
      "Epoch [49100/100000], Loss: 0.0000\n",
      "Epoch [49200/100000], Loss: 0.0000\n",
      "Epoch [49300/100000], Loss: 0.0000\n",
      "Epoch [49400/100000], Loss: 0.0000\n",
      "Epoch [49500/100000], Loss: 0.0000\n",
      "Epoch [49600/100000], Loss: 0.0000\n",
      "Epoch [49700/100000], Loss: 0.0000\n",
      "Epoch [49800/100000], Loss: 0.0000\n",
      "Epoch [49900/100000], Loss: 0.0000\n",
      "Epoch [50000/100000], Loss: 0.0000\n",
      "Epoch [50100/100000], Loss: 0.0000\n",
      "Epoch [50200/100000], Loss: 0.0000\n",
      "Epoch [50300/100000], Loss: 0.0000\n",
      "Epoch [50400/100000], Loss: 0.0000\n",
      "Epoch [50500/100000], Loss: 0.0000\n",
      "Epoch [50600/100000], Loss: 0.0000\n",
      "Epoch [50700/100000], Loss: 0.0000\n",
      "Epoch [50800/100000], Loss: 0.0000\n",
      "Epoch [50900/100000], Loss: 0.0000\n",
      "Epoch [51000/100000], Loss: 0.0000\n",
      "Epoch [51100/100000], Loss: 0.0000\n",
      "Epoch [51200/100000], Loss: 0.0000\n",
      "Epoch [51300/100000], Loss: 0.0000\n",
      "Epoch [51400/100000], Loss: 0.0000\n",
      "Epoch [51500/100000], Loss: 0.0000\n",
      "Epoch [51600/100000], Loss: 0.0000\n",
      "Epoch [51700/100000], Loss: 0.0000\n",
      "Epoch [51800/100000], Loss: 0.0000\n",
      "Epoch [51900/100000], Loss: 0.0000\n",
      "Epoch [52000/100000], Loss: 0.0000\n",
      "Epoch [52100/100000], Loss: 0.0000\n",
      "Epoch [52200/100000], Loss: 0.0000\n",
      "Epoch [52300/100000], Loss: 0.0000\n",
      "Epoch [52400/100000], Loss: 0.0000\n",
      "Epoch [52500/100000], Loss: 0.0000\n",
      "Epoch [52600/100000], Loss: 0.0000\n",
      "Epoch [52700/100000], Loss: 0.0000\n",
      "Epoch [52800/100000], Loss: 0.0000\n",
      "Epoch [52900/100000], Loss: 0.0000\n",
      "Epoch [53000/100000], Loss: 0.0000\n",
      "Epoch [53100/100000], Loss: 0.0000\n",
      "Epoch [53200/100000], Loss: 0.0000\n",
      "Epoch [53300/100000], Loss: 0.0000\n",
      "Epoch [53400/100000], Loss: 0.0000\n",
      "Epoch [53500/100000], Loss: 0.0000\n",
      "Epoch [53600/100000], Loss: 0.0000\n",
      "Epoch [53700/100000], Loss: 0.0000\n",
      "Epoch [53800/100000], Loss: 0.0000\n",
      "Epoch [53900/100000], Loss: 0.0000\n",
      "Epoch [54000/100000], Loss: 0.0000\n",
      "Epoch [54100/100000], Loss: 0.0000\n",
      "Epoch [54200/100000], Loss: 0.0000\n",
      "Epoch [54300/100000], Loss: 0.0000\n",
      "Epoch [54400/100000], Loss: 0.0000\n",
      "Epoch [54500/100000], Loss: 0.0000\n",
      "Epoch [54600/100000], Loss: 0.0000\n",
      "Epoch [54700/100000], Loss: 0.0000\n",
      "Epoch [54800/100000], Loss: 0.0000\n",
      "Epoch [54900/100000], Loss: 0.0000\n",
      "Epoch [55000/100000], Loss: 0.0000\n",
      "Epoch [55100/100000], Loss: 0.0000\n",
      "Epoch [55200/100000], Loss: 0.0000\n",
      "Epoch [55300/100000], Loss: 0.0000\n",
      "Epoch [55400/100000], Loss: 0.0000\n",
      "Epoch [55500/100000], Loss: 0.0000\n",
      "Epoch [55600/100000], Loss: 0.0000\n",
      "Epoch [55700/100000], Loss: 0.0000\n",
      "Epoch [55800/100000], Loss: 0.0000\n",
      "Epoch [55900/100000], Loss: 0.0000\n",
      "Epoch [56000/100000], Loss: 0.0000\n",
      "Epoch [56100/100000], Loss: 0.0000\n",
      "Epoch [56200/100000], Loss: 0.0000\n",
      "Epoch [56300/100000], Loss: 0.0000\n",
      "Epoch [56400/100000], Loss: 0.0000\n",
      "Epoch [56500/100000], Loss: 0.0000\n",
      "Epoch [56600/100000], Loss: 0.0000\n",
      "Epoch [56700/100000], Loss: 0.0000\n",
      "Epoch [56800/100000], Loss: 0.0000\n",
      "Epoch [56900/100000], Loss: 0.0000\n",
      "Epoch [57000/100000], Loss: 0.0000\n",
      "Epoch [57100/100000], Loss: 0.0000\n",
      "Epoch [57200/100000], Loss: 0.0000\n",
      "Epoch [57300/100000], Loss: 0.0000\n",
      "Epoch [57400/100000], Loss: 0.0000\n",
      "Epoch [57500/100000], Loss: 0.0000\n",
      "Epoch [57600/100000], Loss: 0.0000\n",
      "Epoch [57700/100000], Loss: 0.0000\n",
      "Epoch [57800/100000], Loss: 0.0000\n",
      "Epoch [57900/100000], Loss: 0.0000\n",
      "Epoch [58000/100000], Loss: 0.0000\n",
      "Epoch [58100/100000], Loss: 0.0000\n",
      "Epoch [58200/100000], Loss: 0.0000\n",
      "Epoch [58300/100000], Loss: 0.0000\n",
      "Epoch [58400/100000], Loss: 0.0000\n",
      "Epoch [58500/100000], Loss: 0.0000\n",
      "Epoch [58600/100000], Loss: 0.0000\n",
      "Epoch [58700/100000], Loss: 0.0000\n",
      "Epoch [58800/100000], Loss: 0.0000\n",
      "Epoch [58900/100000], Loss: 0.0000\n",
      "Epoch [59000/100000], Loss: 0.0000\n",
      "Epoch [59100/100000], Loss: 0.0000\n",
      "Epoch [59200/100000], Loss: 0.0000\n",
      "Epoch [59300/100000], Loss: 0.0000\n",
      "Epoch [59400/100000], Loss: 0.0000\n",
      "Epoch [59500/100000], Loss: 0.0000\n",
      "Epoch [59600/100000], Loss: 0.0000\n",
      "Epoch [59700/100000], Loss: 0.0000\n",
      "Epoch [59800/100000], Loss: 0.0000\n",
      "Epoch [59900/100000], Loss: 0.0000\n",
      "Epoch [60000/100000], Loss: 0.0000\n",
      "Epoch [60100/100000], Loss: 0.0000\n",
      "Epoch [60200/100000], Loss: 0.0000\n",
      "Epoch [60300/100000], Loss: 0.0000\n",
      "Epoch [60400/100000], Loss: 0.0000\n",
      "Epoch [60500/100000], Loss: 0.0000\n",
      "Epoch [60600/100000], Loss: 0.0000\n",
      "Epoch [60700/100000], Loss: 0.0000\n",
      "Epoch [60800/100000], Loss: 0.0000\n",
      "Epoch [60900/100000], Loss: 0.0000\n",
      "Epoch [61000/100000], Loss: 0.0000\n",
      "Epoch [61100/100000], Loss: 0.0000\n",
      "Epoch [61200/100000], Loss: 0.0000\n",
      "Epoch [61300/100000], Loss: 0.0000\n",
      "Epoch [61400/100000], Loss: 0.0000\n",
      "Epoch [61500/100000], Loss: 0.0000\n",
      "Epoch [61600/100000], Loss: 0.0000\n",
      "Epoch [61700/100000], Loss: 0.0000\n",
      "Epoch [61800/100000], Loss: 0.0000\n",
      "Epoch [61900/100000], Loss: 0.0000\n",
      "Epoch [62000/100000], Loss: 0.0000\n",
      "Epoch [62100/100000], Loss: 0.0000\n",
      "Epoch [62200/100000], Loss: 0.0000\n",
      "Epoch [62300/100000], Loss: 0.0000\n",
      "Epoch [62400/100000], Loss: 0.0000\n",
      "Epoch [62500/100000], Loss: 0.0000\n",
      "Epoch [62600/100000], Loss: 0.0000\n",
      "Epoch [62700/100000], Loss: 0.0000\n",
      "Epoch [62800/100000], Loss: 0.0000\n",
      "Epoch [62900/100000], Loss: 0.0000\n",
      "Epoch [63000/100000], Loss: 0.0000\n",
      "Epoch [63100/100000], Loss: 0.0000\n",
      "Epoch [63200/100000], Loss: 0.0000\n",
      "Epoch [63300/100000], Loss: 0.0000\n",
      "Epoch [63400/100000], Loss: 0.0000\n",
      "Epoch [63500/100000], Loss: 0.0000\n",
      "Epoch [63600/100000], Loss: 0.0000\n",
      "Epoch [63700/100000], Loss: 0.0000\n",
      "Epoch [63800/100000], Loss: 0.0000\n",
      "Epoch [63900/100000], Loss: 0.0000\n",
      "Epoch [64000/100000], Loss: 0.0000\n",
      "Epoch [64100/100000], Loss: 0.0000\n",
      "Epoch [64200/100000], Loss: 0.0000\n",
      "Epoch [64300/100000], Loss: 0.0000\n",
      "Epoch [64400/100000], Loss: 0.0000\n",
      "Epoch [64500/100000], Loss: 0.0000\n",
      "Epoch [64600/100000], Loss: 0.0000\n",
      "Epoch [64700/100000], Loss: 0.0000\n",
      "Epoch [64800/100000], Loss: 0.0000\n",
      "Epoch [64900/100000], Loss: 0.0000\n",
      "Epoch [65000/100000], Loss: 0.0000\n",
      "Epoch [65100/100000], Loss: 0.0000\n",
      "Epoch [65200/100000], Loss: 0.0000\n",
      "Epoch [65300/100000], Loss: 0.0000\n",
      "Epoch [65400/100000], Loss: 0.0000\n",
      "Epoch [65500/100000], Loss: 0.0000\n",
      "Epoch [65600/100000], Loss: 0.0000\n",
      "Epoch [65700/100000], Loss: 0.0000\n",
      "Epoch [65800/100000], Loss: 0.0000\n",
      "Epoch [65900/100000], Loss: 0.0000\n",
      "Epoch [66000/100000], Loss: 0.0000\n",
      "Epoch [66100/100000], Loss: 0.0000\n",
      "Epoch [66200/100000], Loss: 0.0000\n",
      "Epoch [66300/100000], Loss: 0.0000\n",
      "Epoch [66400/100000], Loss: 0.0000\n",
      "Epoch [66500/100000], Loss: 0.0000\n",
      "Epoch [66600/100000], Loss: 0.0000\n",
      "Epoch [66700/100000], Loss: 0.0000\n",
      "Epoch [66800/100000], Loss: 0.0000\n",
      "Epoch [66900/100000], Loss: 0.0000\n",
      "Epoch [67000/100000], Loss: 0.0000\n",
      "Epoch [67100/100000], Loss: 0.0000\n",
      "Epoch [67200/100000], Loss: 0.0000\n",
      "Epoch [67300/100000], Loss: 0.0000\n",
      "Epoch [67400/100000], Loss: 0.0000\n",
      "Epoch [67500/100000], Loss: 0.0000\n",
      "Epoch [67600/100000], Loss: 0.0000\n",
      "Epoch [67700/100000], Loss: 0.0000\n",
      "Epoch [67800/100000], Loss: 0.0000\n",
      "Epoch [67900/100000], Loss: 0.0000\n",
      "Epoch [68000/100000], Loss: 0.0000\n",
      "Epoch [68100/100000], Loss: 0.0000\n",
      "Epoch [68200/100000], Loss: 0.0000\n",
      "Epoch [68300/100000], Loss: 0.0000\n",
      "Epoch [68400/100000], Loss: 0.0000\n",
      "Epoch [68500/100000], Loss: 0.0000\n",
      "Epoch [68600/100000], Loss: 0.0000\n",
      "Epoch [68700/100000], Loss: 0.0000\n",
      "Epoch [68800/100000], Loss: 0.0000\n",
      "Epoch [68900/100000], Loss: 0.0000\n",
      "Epoch [69000/100000], Loss: 0.0000\n",
      "Epoch [69100/100000], Loss: 0.0000\n",
      "Epoch [69200/100000], Loss: 0.0000\n",
      "Epoch [69300/100000], Loss: 0.0000\n",
      "Epoch [69400/100000], Loss: 0.0000\n",
      "Epoch [69500/100000], Loss: 0.0000\n",
      "Epoch [69600/100000], Loss: 0.0000\n",
      "Epoch [69700/100000], Loss: 0.0000\n",
      "Epoch [69800/100000], Loss: 0.0000\n",
      "Epoch [69900/100000], Loss: 0.0000\n",
      "Epoch [70000/100000], Loss: 0.0000\n",
      "Epoch [70100/100000], Loss: 0.0000\n",
      "Epoch [70200/100000], Loss: 0.0000\n",
      "Epoch [70300/100000], Loss: 0.0000\n",
      "Epoch [70400/100000], Loss: 0.0000\n",
      "Epoch [70500/100000], Loss: 0.0000\n",
      "Epoch [70600/100000], Loss: 0.0000\n",
      "Epoch [70700/100000], Loss: 0.0000\n",
      "Epoch [70800/100000], Loss: 0.0000\n",
      "Epoch [70900/100000], Loss: 0.0000\n",
      "Epoch [71000/100000], Loss: 0.0000\n",
      "Epoch [71100/100000], Loss: 0.0000\n",
      "Epoch [71200/100000], Loss: 0.0000\n",
      "Epoch [71300/100000], Loss: 0.0000\n",
      "Epoch [71400/100000], Loss: 0.0000\n",
      "Epoch [71500/100000], Loss: 0.0000\n",
      "Epoch [71600/100000], Loss: 0.0000\n",
      "Epoch [71700/100000], Loss: 0.0000\n",
      "Epoch [71800/100000], Loss: 0.0000\n",
      "Epoch [71900/100000], Loss: 0.0000\n",
      "Epoch [72000/100000], Loss: 0.0000\n",
      "Epoch [72100/100000], Loss: 0.0000\n",
      "Epoch [72200/100000], Loss: 0.0000\n",
      "Epoch [72300/100000], Loss: 0.0000\n",
      "Epoch [72400/100000], Loss: 0.0000\n",
      "Epoch [72500/100000], Loss: 0.0000\n",
      "Epoch [72600/100000], Loss: 0.0000\n",
      "Epoch [72700/100000], Loss: 0.0000\n",
      "Epoch [72800/100000], Loss: 0.0000\n",
      "Epoch [72900/100000], Loss: 0.0000\n",
      "Epoch [73000/100000], Loss: 0.0000\n",
      "Epoch [73100/100000], Loss: 0.0000\n",
      "Epoch [73200/100000], Loss: 0.0000\n",
      "Epoch [73300/100000], Loss: 0.0000\n",
      "Epoch [73400/100000], Loss: 0.0000\n",
      "Epoch [73500/100000], Loss: 0.0000\n",
      "Epoch [73600/100000], Loss: 0.0000\n",
      "Epoch [73700/100000], Loss: 0.0000\n",
      "Epoch [73800/100000], Loss: 0.0000\n",
      "Epoch [73900/100000], Loss: 0.0000\n",
      "Epoch [74000/100000], Loss: 0.0000\n",
      "Epoch [74100/100000], Loss: 0.0000\n",
      "Epoch [74200/100000], Loss: 0.0000\n",
      "Epoch [74300/100000], Loss: 0.0000\n",
      "Epoch [74400/100000], Loss: 0.0000\n",
      "Epoch [74500/100000], Loss: 0.0000\n",
      "Epoch [74600/100000], Loss: 0.0000\n",
      "Epoch [74700/100000], Loss: 0.0000\n",
      "Epoch [74800/100000], Loss: 0.0000\n",
      "Epoch [74900/100000], Loss: 0.0000\n",
      "Epoch [75000/100000], Loss: 0.0000\n",
      "Epoch [75100/100000], Loss: 0.0000\n",
      "Epoch [75200/100000], Loss: 0.0000\n",
      "Epoch [75300/100000], Loss: 0.0000\n",
      "Epoch [75400/100000], Loss: 0.0000\n",
      "Epoch [75500/100000], Loss: 0.0000\n",
      "Epoch [75600/100000], Loss: 0.0000\n",
      "Epoch [75700/100000], Loss: 0.0000\n",
      "Epoch [75800/100000], Loss: 0.0000\n",
      "Epoch [75900/100000], Loss: 0.0000\n",
      "Epoch [76000/100000], Loss: 0.0000\n",
      "Epoch [76100/100000], Loss: 0.0000\n",
      "Epoch [76200/100000], Loss: 0.0000\n",
      "Epoch [76300/100000], Loss: 0.0000\n",
      "Epoch [76400/100000], Loss: 0.0000\n",
      "Epoch [76500/100000], Loss: 0.0000\n",
      "Epoch [76600/100000], Loss: 0.0000\n",
      "Epoch [76700/100000], Loss: 0.0000\n",
      "Epoch [76800/100000], Loss: 0.0000\n",
      "Epoch [76900/100000], Loss: 0.0000\n",
      "Epoch [77000/100000], Loss: 0.0000\n",
      "Epoch [77100/100000], Loss: 0.0000\n",
      "Epoch [77200/100000], Loss: 0.0000\n",
      "Epoch [77300/100000], Loss: 0.0000\n",
      "Epoch [77400/100000], Loss: 0.0000\n",
      "Epoch [77500/100000], Loss: 0.0000\n",
      "Epoch [77600/100000], Loss: 0.0000\n",
      "Epoch [77700/100000], Loss: 0.0000\n",
      "Epoch [77800/100000], Loss: 0.0000\n",
      "Epoch [77900/100000], Loss: 0.0000\n",
      "Epoch [78000/100000], Loss: 0.0000\n",
      "Epoch [78100/100000], Loss: 0.0000\n",
      "Epoch [78200/100000], Loss: 0.0000\n",
      "Epoch [78300/100000], Loss: 0.0000\n",
      "Epoch [78400/100000], Loss: 0.0000\n",
      "Epoch [78500/100000], Loss: 0.0000\n",
      "Epoch [78600/100000], Loss: 0.0000\n",
      "Epoch [78700/100000], Loss: 0.0000\n",
      "Epoch [78800/100000], Loss: 0.0000\n",
      "Epoch [78900/100000], Loss: 0.0000\n",
      "Epoch [79000/100000], Loss: 0.0000\n",
      "Epoch [79100/100000], Loss: 0.0000\n",
      "Epoch [79200/100000], Loss: 0.0000\n",
      "Epoch [79300/100000], Loss: 0.0000\n",
      "Epoch [79400/100000], Loss: 0.0000\n",
      "Epoch [79500/100000], Loss: 0.0000\n",
      "Epoch [79600/100000], Loss: 0.0000\n",
      "Epoch [79700/100000], Loss: 0.0000\n",
      "Epoch [79800/100000], Loss: 0.0000\n",
      "Epoch [79900/100000], Loss: 0.0000\n",
      "Epoch [80000/100000], Loss: 0.0000\n",
      "Epoch [80100/100000], Loss: 0.0000\n",
      "Epoch [80200/100000], Loss: 0.0000\n",
      "Epoch [80300/100000], Loss: 0.0000\n",
      "Epoch [80400/100000], Loss: 0.0000\n",
      "Epoch [80500/100000], Loss: 0.0000\n",
      "Epoch [80600/100000], Loss: 0.0000\n",
      "Epoch [80700/100000], Loss: 0.0000\n",
      "Epoch [80800/100000], Loss: 0.0000\n",
      "Epoch [80900/100000], Loss: 0.0000\n",
      "Epoch [81000/100000], Loss: 0.0000\n",
      "Epoch [81100/100000], Loss: 0.0000\n",
      "Epoch [81200/100000], Loss: 0.0000\n",
      "Epoch [81300/100000], Loss: 0.0000\n",
      "Epoch [81400/100000], Loss: 0.0000\n",
      "Epoch [81500/100000], Loss: 0.0000\n",
      "Epoch [81600/100000], Loss: 0.0000\n",
      "Epoch [81700/100000], Loss: 0.0000\n",
      "Epoch [81800/100000], Loss: 0.0000\n",
      "Epoch [81900/100000], Loss: 0.0000\n",
      "Epoch [82000/100000], Loss: 0.0000\n",
      "Epoch [82100/100000], Loss: 0.0000\n",
      "Epoch [82200/100000], Loss: 0.0000\n",
      "Epoch [82300/100000], Loss: 0.0000\n",
      "Epoch [82400/100000], Loss: 0.0000\n",
      "Epoch [82500/100000], Loss: 0.0000\n",
      "Epoch [82600/100000], Loss: 0.0000\n",
      "Epoch [82700/100000], Loss: 0.0000\n",
      "Epoch [82800/100000], Loss: 0.0000\n",
      "Epoch [82900/100000], Loss: 0.0000\n",
      "Epoch [83000/100000], Loss: 0.0000\n",
      "Epoch [83100/100000], Loss: 0.0000\n",
      "Epoch [83200/100000], Loss: 0.0000\n",
      "Epoch [83300/100000], Loss: 0.0000\n",
      "Epoch [83400/100000], Loss: 0.0000\n",
      "Epoch [83500/100000], Loss: 0.0000\n",
      "Epoch [83600/100000], Loss: 0.0000\n",
      "Epoch [83700/100000], Loss: 0.0000\n",
      "Epoch [83800/100000], Loss: 0.0000\n",
      "Epoch [83900/100000], Loss: 0.0000\n",
      "Epoch [84000/100000], Loss: 0.0000\n",
      "Epoch [84100/100000], Loss: 0.0000\n",
      "Epoch [84200/100000], Loss: 0.0000\n",
      "Epoch [84300/100000], Loss: 0.0000\n",
      "Epoch [84400/100000], Loss: 0.0000\n",
      "Epoch [84500/100000], Loss: 0.0000\n",
      "Epoch [84600/100000], Loss: 0.0000\n",
      "Epoch [84700/100000], Loss: 0.0000\n",
      "Epoch [84800/100000], Loss: 0.0000\n",
      "Epoch [84900/100000], Loss: 0.0000\n",
      "Epoch [85000/100000], Loss: 0.0000\n",
      "Epoch [85100/100000], Loss: 0.0000\n",
      "Epoch [85200/100000], Loss: 0.0000\n",
      "Epoch [85300/100000], Loss: 0.0000\n",
      "Epoch [85400/100000], Loss: 0.0000\n",
      "Epoch [85500/100000], Loss: 0.0000\n",
      "Epoch [85600/100000], Loss: 0.0000\n",
      "Epoch [85700/100000], Loss: 0.0000\n",
      "Epoch [85800/100000], Loss: 0.0000\n",
      "Epoch [85900/100000], Loss: 0.0000\n",
      "Epoch [86000/100000], Loss: 0.0000\n",
      "Epoch [86100/100000], Loss: 0.0000\n",
      "Epoch [86200/100000], Loss: 0.0000\n",
      "Epoch [86300/100000], Loss: 0.0000\n",
      "Epoch [86400/100000], Loss: 0.0000\n",
      "Epoch [86500/100000], Loss: 0.0000\n",
      "Epoch [86600/100000], Loss: 0.0000\n",
      "Epoch [86700/100000], Loss: 0.0000\n",
      "Epoch [86800/100000], Loss: 0.0000\n",
      "Epoch [86900/100000], Loss: 0.0000\n",
      "Epoch [87000/100000], Loss: 0.0000\n",
      "Epoch [87100/100000], Loss: 0.0000\n",
      "Epoch [87200/100000], Loss: 0.0000\n",
      "Epoch [87300/100000], Loss: 0.0000\n",
      "Epoch [87400/100000], Loss: 0.0000\n",
      "Epoch [87500/100000], Loss: 0.0000\n",
      "Epoch [87600/100000], Loss: 0.0000\n",
      "Epoch [87700/100000], Loss: 0.0000\n",
      "Epoch [87800/100000], Loss: 0.0000\n",
      "Epoch [87900/100000], Loss: 0.0000\n",
      "Epoch [88000/100000], Loss: 0.0000\n",
      "Epoch [88100/100000], Loss: 0.0000\n",
      "Epoch [88200/100000], Loss: 0.0000\n",
      "Epoch [88300/100000], Loss: 0.0000\n",
      "Epoch [88400/100000], Loss: 0.0000\n",
      "Epoch [88500/100000], Loss: 0.0000\n",
      "Epoch [88600/100000], Loss: 0.0000\n",
      "Epoch [88700/100000], Loss: 0.0000\n",
      "Epoch [88800/100000], Loss: 0.0000\n",
      "Epoch [88900/100000], Loss: 0.0000\n",
      "Epoch [89000/100000], Loss: 0.0000\n",
      "Epoch [89100/100000], Loss: 0.0000\n",
      "Epoch [89200/100000], Loss: 0.0000\n",
      "Epoch [89300/100000], Loss: 0.0000\n",
      "Epoch [89400/100000], Loss: 0.0000\n",
      "Epoch [89500/100000], Loss: 0.0000\n",
      "Epoch [89600/100000], Loss: 0.0000\n",
      "Epoch [89700/100000], Loss: 0.0000\n",
      "Epoch [89800/100000], Loss: 0.0000\n",
      "Epoch [89900/100000], Loss: 0.0000\n",
      "Epoch [90000/100000], Loss: 0.0000\n",
      "Epoch [90100/100000], Loss: 0.0000\n",
      "Epoch [90200/100000], Loss: 0.0000\n",
      "Epoch [90300/100000], Loss: 0.0000\n",
      "Epoch [90400/100000], Loss: 0.0000\n",
      "Epoch [90500/100000], Loss: 0.0000\n",
      "Epoch [90600/100000], Loss: 0.0000\n",
      "Epoch [90700/100000], Loss: 0.0000\n",
      "Epoch [90800/100000], Loss: 0.0000\n",
      "Epoch [90900/100000], Loss: 0.0000\n",
      "Epoch [91000/100000], Loss: 0.0000\n",
      "Epoch [91100/100000], Loss: 0.0000\n",
      "Epoch [91200/100000], Loss: 0.0000\n",
      "Epoch [91300/100000], Loss: 0.0000\n",
      "Epoch [91400/100000], Loss: 0.0000\n",
      "Epoch [91500/100000], Loss: 0.0000\n",
      "Epoch [91600/100000], Loss: 0.0000\n",
      "Epoch [91700/100000], Loss: 0.0000\n",
      "Epoch [91800/100000], Loss: 0.0000\n",
      "Epoch [91900/100000], Loss: 0.0000\n",
      "Epoch [92000/100000], Loss: 0.0000\n",
      "Epoch [92100/100000], Loss: 0.0000\n",
      "Epoch [92200/100000], Loss: 0.0000\n",
      "Epoch [92300/100000], Loss: 0.0000\n",
      "Epoch [92400/100000], Loss: 0.0000\n",
      "Epoch [92500/100000], Loss: 0.0000\n",
      "Epoch [92600/100000], Loss: 0.0000\n",
      "Epoch [92700/100000], Loss: 0.0000\n",
      "Epoch [92800/100000], Loss: 0.0000\n",
      "Epoch [92900/100000], Loss: 0.0000\n",
      "Epoch [93000/100000], Loss: 0.0000\n",
      "Epoch [93100/100000], Loss: 0.0000\n",
      "Epoch [93200/100000], Loss: 0.0000\n",
      "Epoch [93300/100000], Loss: 0.0000\n",
      "Epoch [93400/100000], Loss: 0.0000\n",
      "Epoch [93500/100000], Loss: 0.0000\n",
      "Epoch [93600/100000], Loss: 0.0000\n",
      "Epoch [93700/100000], Loss: 0.0000\n",
      "Epoch [93800/100000], Loss: 0.0000\n",
      "Epoch [93900/100000], Loss: 0.0000\n",
      "Epoch [94000/100000], Loss: 0.0000\n",
      "Epoch [94100/100000], Loss: 0.0000\n",
      "Epoch [94200/100000], Loss: 0.0000\n",
      "Epoch [94300/100000], Loss: 0.0000\n",
      "Epoch [94400/100000], Loss: 0.0000\n",
      "Epoch [94500/100000], Loss: 0.0000\n",
      "Epoch [94600/100000], Loss: 0.0000\n",
      "Epoch [94700/100000], Loss: 0.0000\n",
      "Epoch [94800/100000], Loss: 0.0000\n",
      "Epoch [94900/100000], Loss: 0.0000\n",
      "Epoch [95000/100000], Loss: 0.0000\n",
      "Epoch [95100/100000], Loss: 0.0000\n",
      "Epoch [95200/100000], Loss: 0.0000\n",
      "Epoch [95300/100000], Loss: 0.0000\n",
      "Epoch [95400/100000], Loss: 0.0000\n",
      "Epoch [95500/100000], Loss: 0.0000\n",
      "Epoch [95600/100000], Loss: 0.0000\n",
      "Epoch [95700/100000], Loss: 0.0000\n",
      "Epoch [95800/100000], Loss: 0.0000\n",
      "Epoch [95900/100000], Loss: 0.0000\n",
      "Epoch [96000/100000], Loss: 0.0000\n",
      "Epoch [96100/100000], Loss: 0.0000\n",
      "Epoch [96200/100000], Loss: 0.0000\n",
      "Epoch [96300/100000], Loss: 0.0000\n",
      "Epoch [96400/100000], Loss: 0.0000\n",
      "Epoch [96500/100000], Loss: 0.0000\n",
      "Epoch [96600/100000], Loss: 0.0000\n",
      "Epoch [96700/100000], Loss: 0.0000\n",
      "Epoch [96800/100000], Loss: 0.0000\n",
      "Epoch [96900/100000], Loss: 0.0000\n",
      "Epoch [97000/100000], Loss: 0.0000\n",
      "Epoch [97100/100000], Loss: 0.0000\n",
      "Epoch [97200/100000], Loss: 0.0000\n",
      "Epoch [97300/100000], Loss: 0.0000\n",
      "Epoch [97400/100000], Loss: 0.0000\n",
      "Epoch [97500/100000], Loss: 0.0000\n",
      "Epoch [97600/100000], Loss: 0.0000\n",
      "Epoch [97700/100000], Loss: 0.0000\n",
      "Epoch [97800/100000], Loss: 0.0000\n",
      "Epoch [97900/100000], Loss: 0.0000\n",
      "Epoch [98000/100000], Loss: 0.0000\n",
      "Epoch [98100/100000], Loss: 0.0000\n",
      "Epoch [98200/100000], Loss: 0.0000\n",
      "Epoch [98300/100000], Loss: 0.0000\n",
      "Epoch [98400/100000], Loss: 0.0000\n",
      "Epoch [98500/100000], Loss: 0.0000\n",
      "Epoch [98600/100000], Loss: 0.0000\n",
      "Epoch [98700/100000], Loss: 0.0000\n",
      "Epoch [98800/100000], Loss: 0.0000\n",
      "Epoch [98900/100000], Loss: 0.0000\n",
      "Epoch [99000/100000], Loss: 0.0000\n",
      "Epoch [99100/100000], Loss: 0.0000\n",
      "Epoch [99200/100000], Loss: 0.0000\n",
      "Epoch [99300/100000], Loss: 0.0000\n",
      "Epoch [99400/100000], Loss: 0.0000\n",
      "Epoch [99500/100000], Loss: 0.0000\n",
      "Epoch [99600/100000], Loss: 0.0000\n",
      "Epoch [99700/100000], Loss: 0.0000\n",
      "Epoch [99800/100000], Loss: 0.0000\n",
      "Epoch [99900/100000], Loss: 0.0000\n",
      "Ma trận B đã học sau khi huấn luyện:\n",
      "[[1.       2.      ]\n",
      " [2.999992 4.      ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Định nghĩa ma trận A (không thay đổi)\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n",
    "\n",
    "# Định nghĩa ma trận B, khởi tạo bằng phương pháp Xavier\n",
    "class XavierMatrix(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(XavierMatrix, self).__init__()\n",
    "        self.B = nn.Parameter(torch.empty(size))  # Ma trận B là tham số học\n",
    "        nn.init.normal_(self.B, mean=0, std=1)  # Khởi tạo với phân phối chuẩn\n",
    "\n",
    "    def forward(self):\n",
    "        return self.B\n",
    "\n",
    "# Khoảng cách Frobenius giữa 2 ma trận\n",
    "def frobenius_distance(A, B):\n",
    "    return torch.norm(A - B, p='fro')  # Frobenius norm\n",
    "\n",
    "# Định nghĩa mô hình\n",
    "class WassersteinModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WassersteinModel, self).__init__()\n",
    "        self.xavier_matrix = XavierMatrix(size=(2, 2))  # Khởi tạo ma trận B (cùng kích thước với A)\n",
    "\n",
    "    def forward(self):\n",
    "        B = self.xavier_matrix()\n",
    "        return frobenius_distance(A, B)\n",
    "\n",
    "# Tạo mô hình\n",
    "model = WassersteinModel()\n",
    "\n",
    "# Định nghĩa hàm tối ưu (Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Giảm learning rate\n",
    "\n",
    "# Tiến hành tối ưu\n",
    "num_epochs = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    # Tiến hành cập nhật các tham số của ma trận B\n",
    "    optimizer.zero_grad()\n",
    "    loss = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # In thông tin loss sau mỗi 100 epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Sau khi huấn luyện, lấy ma trận B đã học\n",
    "B_learned = model.xavier_matrix.B.detach().numpy()  # Dùng .detach() để tránh tính toán gradient\n",
    "print(\"Ma trận B đã học sau khi huấn luyện:\")\n",
    "print(B_learned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_learned = model.xavier_matrix.B.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.35883 ,  9.771715],\n",
       "       [11.24564 ,  9.942004]], dtype=float32)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def\n",
    "def loss_test(test_y_pred):\n",
    "    return -torch.log(test_y_pred).sum()/len(test_y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
