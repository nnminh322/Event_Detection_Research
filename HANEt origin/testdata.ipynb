{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "perm_id = '0'\n",
    "task_num = 5\n",
    "class_num = 10\n",
    "shot_num = 5\n",
    "\n",
    "def collect_from_json(dataset, root, split):\n",
    "    default = ['train', 'dev', 'test']\n",
    "    if split == \"train\":\n",
    "        pth = os.path.join(root, dataset, \"perm\"+perm_id, f\"{dataset}_{task_num}task_{class_num // task_num}way_{shot_num}shot.{split}.jsonl\")\n",
    "    elif split in ['dev', 'test']:\n",
    "        pth = os.path.join(root, dataset, f\"{dataset}.{split}.jsonl\")\n",
    "    elif split == \"stream\":\n",
    "        pth = os.path.join(root, dataset, f\"stream_label_{task_num}task_{class_num // task_num}way.json\")\n",
    "    else:\n",
    "        raise ValueError(f\"Split \\\"{split}\\\" value wrong!\")\n",
    "    if not os.path.exists(pth):\n",
    "        raise FileNotFoundError(f\"Path {pth} do not exist!\")\n",
    "    else:\n",
    "        with open(pth) as f:\n",
    "            if pth.endswith('.jsonl'):\n",
    "                data = [json.loads(line) for line in f]\n",
    "                if split == \"train\":\n",
    "                    data = [list(i.values()) for i in data]\n",
    "            else:\n",
    "                data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MAVEN_Dataset(Dataset):\n",
    "    def __init__(self, tokens, labels, masks, spans) -> None:\n",
    "        super(Dataset).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "        self.spans = spans\n",
    "        # self.requires_cl = [0 for _ in range(len(spans))]\n",
    "        # self.labels = []\n",
    "        # for stream in streams:\n",
    "        #     for lb in stream:\n",
    "        #         if not lb in self.label2idx:\n",
    "        #             self.label2idx[lb] = len(self.label2idx)\n",
    "        # for label_ls in labels:\n",
    "        #     self.labels.append([self.label2idx[label]  for label in label_ls])\n",
    "    def __getitem__(self, index):\n",
    "        return [self.tokens[index], self.labels[index], self.masks[index], self.spans[index]]\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def extend(self, tokens, labels, masks, spans):\n",
    "        self.tokens.extend(tokens)\n",
    "        self.labels.extend(labels)\n",
    "        self.masks.extend(masks)\n",
    "        self.spans.extend(spans)\n",
    "        # self.requires_cl.extend(requires_cl)\n",
    "    # def collate_fn(self, batch):\n",
    "    #     batch = pad_sequence([torch.LongTensor(item) for item in batch[2]])\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "def collect_dataset(dataset, root, split, label2idx, stage_id, labels):\n",
    "    if split == 'train':\n",
    "        data = [instance for t in collect_from_json(dataset, root, split)[stage_id] for instance in t]\n",
    "    else:\n",
    "        data = collect_from_json(dataset, root, split)\n",
    "    data_tokens, data_labels, data_masks, data_spans = [], [], [], []\n",
    "    for dt in tqdm(data):\n",
    "        # pop useless properties\n",
    "        if 'mention_id' in dt.keys():\n",
    "            dt.pop('mention_id')\n",
    "        if 'sentence_id' in dt.keys():    \n",
    "            dt.pop('sentence_id')\n",
    "        # if split == 'train':\n",
    "        add_label = []\n",
    "        add_span = []\n",
    "        new_t = {}\n",
    "        for i in range(len(dt['label'])):\n",
    "            if dt['label'][i] in labels or dt['label'][i] == 0: # if the label of instance is in the query\n",
    "                add_label.append(dt['label'][i]) # append the instance and the label\n",
    "                add_span.append(dt['span'][i]) # the same as label\n",
    "        if len(add_label) != 0:\n",
    "            token = dt['piece_ids']\n",
    "            new_t['label'] = add_label\n",
    "            valid_span = add_span\n",
    "            valid_label = [label2idx[item] if item in label2idx else 0 for item in add_label]\n",
    "        # else:\n",
    "        #     token = dt['piece_ids']\n",
    "        #     valid_span = dt['span'].copy()\n",
    "        #     valid_label = [label2idx[item] if item in label2idx else 0 for item in dt['label']]\n",
    "            # max_seqlen = 90\n",
    "        max_seqlen = 120 # 344, 249, 230, 186, 167\n",
    "        if len(token) >= max_seqlen + 2:\n",
    "            token_sep = token[-1]\n",
    "            token = token[:max_seqlen + 1] + [token_sep]\n",
    "            invalid_span = np.unique(np.nonzero(np.asarray(valid_span) > max_seqlen)[0])\n",
    "            invalid_span = invalid_span[::-1]\n",
    "            for invalid_idx in invalid_span:\n",
    "                valid_span.pop(invalid_idx)\n",
    "                valid_label.pop(invalid_idx)\n",
    "        if len(token) < max_seqlen + 2:\n",
    "            token = token + [0] * (max_seqlen + 2 - len(token))\n",
    "        token_mask = [1 if tkn != 0 else 0 for tkn in token]\n",
    "            # span_mask = []\n",
    "            # for i in range(len(token)):\n",
    "            #     span_mask.append([0, 0])\n",
    "            # for item in valid_span:\n",
    "            #     for i in range(len(item)):\n",
    "            #         span_mask[item[i]][i] = 1\n",
    "        data_tokens.append(token)\n",
    "        data_labels.append(valid_label)\n",
    "        data_masks.append(token_mask)\n",
    "        data_spans.append(valid_span)\n",
    "            # data_spans.append(valid_span)\n",
    "    # if args.my_test:\n",
    "        return MAVEN_Dataset(data_tokens[:100], data_labels[:100], data_masks[:100], data_spans[:100]) # TODO: deprecated, used for debugging, not for test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data_incremental'\n",
    "dataset = 'MAVEN'\n",
    "split = 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collect_from_json(dataset=dataset,root=root,split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 21, 72, 14, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0][0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
